JAVA基础
JVM
类加载过程
类加载过程
当程序要使用某个类时，如果该类还未被加载到内存中，则系统会通过加载，连接，初始化三步来实现这个类进行初始化。

加载
加载，是指Java虚拟机查找字节流（查找.class文件），并且根据字节流创建java.lang.Class对象的过程。这个过程，将类的.class文件中的二进制数据读入内存，放在运行时区域的方法区内。然后在堆中创建java.lang.Class对象，用来封装类在方法区的数据结构。

类加载阶段：

（1）Java虚拟机将.class文件读入内存，并为之创建一个Class对象。

（2）任何类被使用时系统都会为其创建一个且仅有一个Class对象。

（3）这个Class对象描述了这个类创建出来的对象的所有信息，比如有哪些构造方法，都有哪些成员方法，都有哪些成员变量等。

Student类加载过程图示：



链接
链接包括验证、准备以及解析三个阶段。

（1）验证阶段。主要的目的是确保被加载的类（.class文件的字节流）满足Java虚拟机规范，不会造成安全错误。

（2）准备阶段。负责为类的静态成员分配内存，并设置默认初始值。

（3）解析阶段。将类的二进制数据中的符号引用替换为直接引用。

说明：

符号引用。即一个字符串，但是这个字符串给出了一些能够唯一性识别一个方法，一个变量，一个类的相关信息。

直接引用。可以理解为一个内存地址，或者一个偏移量。比如类方法，类变量的直接引用是指向方法区的指针；而实例方法，实例变量的直接引用则是从实例的头指针开始算起到这个实例变量位置的偏移量。

举个例子来说，现在调用方法hello()，这个方法的地址是0xaabbccdd，那么hello就是符号引用，0xaabbccdd就是直接引用。

在解析阶段，虚拟机会把所有的类名，方法名，字段名这些符号引用替换为具体的内存地址或偏移量，也就是直接引用。

初始化
初始化，则是为标记为常量值的字段赋值的过程。换句话说，只对static修饰的变量或语句块进行初始化。

如果初始化一个类的时候，其父类尚未初始化，则优先初始化其父类。

如果同时包含多个静态变量和静态代码块，则按照自上而下的顺序依次执行。

小结
类加载过程只是一个类生命周期的一部分，在其前，有编译的过程，只有对源代码编译之后，才能获得能够被虚拟机加载的字节码文件；在其后还有具体的类使用过程，当使用完成之后，还会在方法区垃圾回收的过程中进行卸载（垃圾回收）。

附录
常见问题：在自己的项目里新建一个java.lang包，里面新建了一个String类，能代替系统String吗？

不能，因为根据类加载的双亲委派机制，会将请求转发给父类加载器，父类加载器发现冲突了String就不会加载了。
类加载机制
类加载器简单来说是用来加载 Java 类到 Java 虚拟机中的。Java 虚拟机使用 Java 类的方式如下：Java 源程序（.java 文件）在经过 Java 编译器编译之后就被转换成 Java 字节代码（.class 文件）。类加载器负责读取 Java 字节代码，并转换成 java.lang.Class类的一个实例。每个这样的实例用来表示一个 Java 类。通过此实例的 newInstance()方法就可以创建出该类的一个对象。

想要真正深入理解Java类加载机制，就要弄懂三个问题：类什么时候加载、类加载的过程是什么、用什么加载。所以本文分为三部分分别介绍Java类加载的时机、类加载的过程、加载器。

一、Java类加载的时机

1.1 类加载的生命周期

类加载的生命周期是从类被加载到内存开始，直到卸载出内存为止的。整个生命周期分为7个阶段：加载、验证、准备、解析、初始化、使用、卸载。其中，验证、准备、解析三部分统称为连接。具体步骤如下图所示：




下面简单介绍下类加载器所执行的生命周期的过程。

(1) 装载：查找和导入Class文件；

(2) 链接：把类的二进制数据合并到JRE中；

(a)校验：检查载入Class文件数据的正确性；

(b)准备：给类的静态变量分配存储空间；

(c)解析：将符号引用转成直接引用；

(3) 初始化：对类的静态变量，静态代码块执行初始化操作。

1.2 类加载的时机

类加载的时机Java虚拟机规范中并没有强制规定，但是对于初始化阶段，有5种场景必须立即执行初始化，也被称为主动引用。

(1) 遇到new、getstatic、putstatic或invokestatic这4条字节码指令时，如果类没有进行过初始化，则需要先触发其初始化。生成这4条指令的最常见的Java代码场景是：使用new关键字实例化对象的时候，读取或设置一个类的静态字段（被final修饰、已在编译期把结果放入常量池的静态字段除外）的时候，以及调用一个类的静态方法的时候。

(2) 使用java.lang.reflect包的方法对类进行反射调用的时候，如果类没有进行过初始化，则需要先触发其初始化。

(3) 当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。

(4)当虚拟机启动时，用户需要指定一个要执行的主类（包含main()方法的那个类），虚拟机会先初始化这个主类。

(5)当使用JDK 1.7动态语言支持时，如果一个java.lang.invoke.MethodHandle实例最后的解析结果REF_getStatic、REF_putStatic、REF_invokeStatic的方法句柄，并且该方法句柄所对应的类没有初始化过，则先触发初始化。

二、Java类加载的过程



类加载的全过程分为7个阶段，但是主要的过程是加载、验证、准备、解析、初始化这5个阶段。

2.1 加载

在加载阶段，虚拟机需要完成3件事情：

(1) 通过一个类的全限定名来获取定义此类的二进制字节流；

(2) 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构；

(3) 在Java堆中生成一个代表这个类的java.lang.Class对象，作为方法区这些数据的访问入口。

2.2 验证

验证阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。整体来看，验证阶段大致分为4个验证动作。

(1)文件格式验证

第一阶段是验证字节流是否符合Class文件格式的规范，并且能被当前版本的虚拟机处理。主要目的是保证输入的字节流能正确地解析并存储于方法区之内，格式上符合描述一个Java类型信息的要求。该阶段是基于二进制字节流验证的，只有通过了这个阶段的验证，字节流才会进入内存的方法去中存储，后面的3个验证都是基于方法区的存储结构进行的。

这一阶段可能的验证点：

a.是否以魔数开头；

b.主、次版本号是否在当前虚拟机处理范围内；

c.常量池的常量数据类型是否被支持；

。。。

(2)元数据验证

元数据验证是对字节码描述信息进行语义分析，以保证其描述的信息符合Java语言规范的要求。这个阶段可能的验证点：

a.是否有父类；

b.是否继承了不被允许继承的类；

c.如果该类不是抽象类，是否实现了其父类或接口要求实现的所有方法；

。。。

(3)字节码验证

字节码验证的主要目的是通过数据流和控制流分析，确定程序语义的合法性和逻辑性。该阶段将对类的方法体进行校验分析，保证被校验类的方法在运行时不会做出危害虚拟机安全的事情。这个阶段可能的验证点：

a.保证任何时候操作数栈的数据类型与指令代码序列的一致性；

b.跳转指令不会跳转到方法体以外的字节码指令上；

。。。

(4)符号引用验证

符号引用验证的主要目的是保证解析动作能正常执行，如果无法通过符号引用验证，则会抛出异常。这个阶段可能的验证点：

a.符号引用的类、字段、方法的访问性（public、private等）是否可被当前类访问；

b.指定类是否存在符合方法的字段描述符；

。。。

2.3 准备

准备阶段是正式为类变量分配并设置类变量初始值的阶段，这些内存都将在方法区中进行分配,需要说明的是：这时候进行内存分配的仅包括类变量(被static修饰的变量),而不包括实例变量,实例变量将会在对象实例化时随着对象一起分配在Java堆中;这里所说的初始值“通常情况”是数据类型的零值，例如:

public static int value = 1;

value在准备阶段过后的初始值为0而不是1,而把value赋值的putstatic指令将在初始化阶段才会被执行。

特殊情况：

public static final int value = 1;//此时准备value赋值为1

2.4 解析

解析阶段是虚拟机将常量池内的符号引用替换成直接引用的过程。直接引用是直接指向目标的指针，相对偏移量或是一个能间接定位到目标的句柄。直接引用和虚拟机实现的内存有关，同一个符号引用在不同虚拟机实例上翻译出来的直接引用不尽相同。

2.5 初始化

初始化阶段是类加载过程的最后一步，到了该阶段才真正开始执行类定义的Java程序代码，根据程序员通过代码定制的主观计划去初始化类变量和其他资源，是执行类构造器初始化方法的过程。

三、类加载器

类加载器大致可以分为以下3部分：

(1) 启动类加载器: 将存放于<JAVA_HOME>\lib目录中的，或者被-Xbootclasspath参数所指定的路径中的，并且是虚拟机识别的（仅按照文件名识别，如 rt.jar 名字不符合的类库即使放在lib目录中也不会被加载）类库加载到虚拟机内存中。启动类加载器无法被Java程序直接引用。

(2) 扩展类加载器 : 将<JAVA_HOME>\lib\ext目录下的，或者被java.ext.dirs系统变量所指定的路径中的所有类库加载。开发者可以直接使用扩展类加载器。

(3) 应用程序类加载器: 负责加载用户类路径(ClassPath)上所指定的类库,开发者可直接使用。

我们的应用程序都是由这三种类加载器相互配合加载的。它们的关系如下图所示，称之为双亲委派模型。


 

工作过程：如果一个类加载器接收到了类加载的请求，它首先把这个请求委托给他的父类加载器去完成，每个层次的类加载器都是如此，因此所有的加载请求都应该传送到顶层的启动类加载器中，只有当父加载器反馈自己无法完成这个加载请求（它在搜索范围中没有找到所需的类）时，子加载器才会尝试自己去加载。

好处：java类随着它的类加载器一起具备了一种带有优先级的层次关系。例如类java.lang.Object，它存放在rt.jar中，无论哪个类加载器要加载这个类，最终都会委派给启动类加载器进行加载，因此Object类在程序的各种类加载器环境中都是同一个类。相反，如果用户自己写了一个名为java.lang.Object的类，并放在程序的Classpath中，那系统中将会出现多个不同的Object类，java类型体系中最基础的行为也无法保证，应用程序也会变得一片混乱。

双亲委派模型实现起来其实很简单，以下是实现代码，通过以下代码，可以对JVM采用的双亲委派类加载机制有了更感性的认识



总结：

本文从Java类加载的时机、类加载的过程以及类加载的方式三方面对Java类加载机制进行了浅析，希望通过阅读本文可以对Java类加载机制有个大致的了解。
内存模型


由上图可以清楚的看到JVM的内存空间分为3大部分：

堆内存
方法区
栈内存
其中栈内存可以再细分为java虚拟机栈和本地方法栈,堆内存可以划分为新生代和老年代,新生代中还可以再次划分为Eden区、From Survivor区和To Survivor区。

其中一部分是线程共享的，包括 Java 堆和方法区；另一部分是线程私有的，包括虚拟机栈和本地方法栈，以及程序计数器这一小部分内存。

堆内存（Heap）

对于大多数应用来说，Java 堆（Java Heap）是Java 虚拟机所管理的内存中最大的一块。Java 堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。

此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。

堆内存是所有线程共有的，可以分为两个部分：年轻代和老年代。

下图中的Perm代表的是永久代，但是注意永久代并不属于堆内存中的一部分，同时jdk1.8之后永久代已经被移除。

新生代 ( Young ) 与老年代 ( Old ) 的比例的值为 1:2 ( 该值可以通过参数 –XX:NewRatio 来指定 )

默认的，Eden : from : to = 8 : 1 : 1 ( 可以通过参数 –XX:SurvivorRatio 来设定 )，即： Eden = 8/10 的新生代空间大小，from = to = 1/10 的新生代空间大小。

方法区（Method Area）

方法区也称"永久代"，它用于存储虚拟机加载的类信息、常量、静态变量、是各个线程共享的内存区域。

在JDK8之前的HotSpot JVM，存放这些”永久的”的区域叫做“永久代(permanent generation)”。永久代是一片连续的堆空间，在JVM启动之前通过在命令行设置参数-XX:MaxPermSize来设定永久代最大可分配的内存空间，默认大小是64M（64位JVM默认是85M）。

随着JDK8的到来，JVM不再有 永久代(PermGen)。但类的元数据信息（metadata）还在，只不过不再是存储在连续的堆空间上，而是移动到叫做“Metaspace”的本地内存（Native memory。

方法区或永生代相关设置

-XX:PermSize=64MB 最小尺寸，初始分配
-XX:MaxPermSize=256MB 最大允许分配尺寸，按需分配
XX:+CMSClassUnloadingEnabled -XX:+CMSPermGenSweepingEnabled 设置垃圾不回收
默认大小
-server选项下默认MaxPermSize为64m
-client选项下默认MaxPermSize为32m

虚拟机栈(JVM Stack)

描述的是java方法执行的内存模型：每个方法被执行的时候都会创建一个"栈帧",用于存储局部变量表(包括参数)、操作栈、方法出口等信息。每个方法被调用到执行完的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。

Java虚拟机栈(JVM Stack)
2.1. 定义
相对于基于寄存器的运行环境来说，JVM是基于栈结构的运行环境
栈结构移植性更好，可控性更强
JVM中的虚拟机栈是描述Java方法执行的内存区域，它是线程私有的
栈中的元素用于支持虚拟机进行方法调用，每个方法从开始调用到执行完成的过程，就是栈帧从入栈到出栈的过程
在活动线程中，只有位于栈顶的帧才是有效的，称为当前栈帧
正在执行的方法称为当前方法
栈帧是方法运行的基本结构

在执行引擎运行时，所有指令都只能针对当前栈帧进行操作
StackOverflowError表示请求的栈溢出，导致内存耗尽，通常出现在递归方法中
JVM能够横扫千军，虚拟机栈就是它的心腹大将，当前方法的栈帧，都是正在战斗的战场，其中的操作栈是参与战斗的士兵



虚拟机栈通过压/出栈的方式，对每个方法对应的活动栈帧进行运算处理，方法正常执行结束，肯定会跳转到另一个栈帧上
在执行的过程中，如果出现异常，会进行异常回溯，返回地址通过异常处理表确定
栈帧在整个JVM体系中的地位颇高,包括局部变量表、操作栈、动态连接、方法返回地址等

局部变量表
存放方法参数和局部变量
相对于类属性变量的准备阶段和初始化阶段来说，局部变量没有准备阶段，必须显式初始化
如果是非静态方法，则在index[0]位置上存储的是方法所属对象的实例引用，随后存储的是参数和局部变量
字节码指令中的STORE指令就是将操作栈中计算完成的局部变量写回局部变量表的存储空间内
操作栈
操作栈是一个初始状态为空的桶式结构栈
在方法执行过程中，会有各种指令往栈中写入和提取信息
JVM的执行引擎是基于栈的执行引擎，其中的栈指的就是操作栈
字节码指令集的定义都是基于栈类型的,栈的深度在方法元信息的stack属性中
下面用一段简单的代码说明操作栈与局部变量表的交互



详细的字节码操作顺序如下:



第1处说明:局部变量表就像个中药柜，里面有很多抽屉,依次编号为0, 1, 2,3，.,. n
字节码指令istore_ 1就是打开1号抽屉，把栈顶中的数13存进去
栈是一个很深的竖桶，任何时候只能对桶口元素进行操作，所以数据只能在栈顶进行存取
某些指令可以直接在抽屉里进行，比如inc指令，直接对抽屉里的数值进行+1操作
程序员面试过程中，常见的i++和++i的区别，可以从字节码上对比出来



iload_ 1从局部变量表的第1号抽屉里取出一个数,压入栈顶，下一步直接在抽屉里实现+1的操作，而这个操作对栈顶元素的值没有影响
所以istore_ 2只是把栈顶元素赋值给a
表格右列，先在第1号抽屉里执行+1操作，然后通过iload_ 1 把第1号抽屉里的数压入栈顶，所以istore_ 2存入的是+1之后的值
这里延伸一个信息，i++并非原子操作。即使通过volatile关键字进行修饰，多个线程同时写的话，也会产生数据互相覆盖的问题.

动态连接
每个栈帧中包含一个在常量池中对当前方法的引用，目的是支持方法调用过程的动态连接
方法返回地址
方法执行时有两种退出情况
正常退出
正常执行到任何方法的返回字节码指令，如RETURN、IRETURN、ARETURN等
异常退出
无论何种退出情况，都将返回至方法当前被调用的位置。方法退出的过程相当于弹出当前栈帧

退出可能有三种方式:

返回值压入，上层调用栈帧
异常信息抛给能够处理的栈帧
PC计数器指向方法调用后的下一条指令
Java虚拟机栈是描述Java方法运行过程的内存模型

Java虚拟机栈会为每一个即将运行的Java方法创建“栈帧”
用于存储该方法在运行过程中所需要的一些信息

局部变量表
存放基本数据类型变量、引用类型的变量、returnAddress类型的变量
操作数栈
动态链接
当前方法的常量池指针
当前方法的返回地址
方法出口等信息
每一个方法从被调用到执行完成的过程,都对应着一个个栈帧在JVM栈中的入栈和出栈过程

注意：人们常说，Java的内存空间分为“栈”和“堆”，栈中存放局部变量，堆中存放对象。
这句话不完全正确！这里的“堆”可以这么理解，但这里的“栈”就是现在讲的虚拟机栈,或者说Java虚拟机栈中的局部变量表部分.
真正的Java虚拟机栈是由一个个栈帧组成，而每个栈帧中都拥有：局部变量表、操作数栈、动态链接、方法出口信息.

2.2. 特点
局部变量表的创建是在方法被执行的时候,随着栈帧的创建而创建.
而且表的大小在编译期就确定,在创建的时候只需分配事先规定好的大小即可.
在方法运行过程中,表的大小不会改变

Java虚拟机栈会出现两种异常

StackOverFlowError
若Java虚拟机栈的内存大小不允许动态扩展,那么当线程请求的栈深度大于虚拟机允许的最大深度时(但内存空间可能还有很多),就抛出此异常
OutOfMemoryError
若Java虚拟机栈的内存大小允许动态扩展,且当线程请求栈时内存用完了,无法再动态扩展了,此时抛出OutOfMemoryError异常
Java虚拟机栈也是线程私有的,每个线程都有各自的Java虚拟机栈,而且随着线程的创建而创建,随着线程的死亡而死亡.

本地方法栈(Native Stack)

本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。

程序计数器（PC Register）

程序计数器是用于标识当前线程执行的字节码文件的行号指示器。多线程情况下，每个线程都具有各自独立的程序计数器，所以该区域是非线程共享的内存区域。

当执行java方法时候，计数器中保存的是字节码文件的行号；当执行Native方法时，计数器的值为空。

直接内存

直接内存并不是虚拟机内存的一部分，也不是Java虚拟机规范中定义的内存区域。jdk1.4中新加入的NIO，引入了通道与缓冲区的IO方式，它可以调用Native方法直接分配堆外内存，这个堆外内存就是本机内存，不会影响到堆内存的大小。

JVM内存参数设置

-Xms设置堆的最小空间大小。
-Xmx设置堆的最大空间大小。
-Xmn:设置年轻代大小
-XX:NewSize设置新生代最小空间大小。
-XX:MaxNewSize设置新生代最大空间大小。
-XX:PermSize设置永久代最小空间大小。
-XX:MaxPermSize设置永久代最大空间大小。
-Xss设置每个线程的堆栈大小
-XX:+UseParallelGC:选择垃圾收集器为并行收集器。此配置仅对年轻代有效。即上述配置下,年轻代使用并发收集,而年老代仍旧使用串行收集。
-XX:ParallelGCThreads=20:配置并行收集器的线程数,即:同时多少个线程一起进行垃圾回收。此值最好配置与处理器数目相等。
典型JVM参数配置参考:

java-Xmx3550m-Xms3550m-Xmn2g-Xss128k
-XX:ParallelGCThreads=20
-XX:+UseConcMarkSweepGC-XX:+UseParNewGC
-Xmx3550m:设置JVM最大可用内存为3550M。

-Xms3550m:设置JVM促使内存为3550m。此值可以设置与-Xmx相同,以避免每次垃圾回收完成后JVM重新分配内存。

-Xmn2g:设置年轻代大小为2G。整个堆大小=年轻代大小+年老代大小+持久代大小。持久代一般固定大小为64m,所以增大年轻代后,将会减小年老代大小。此值对系统性能影响较大,官方推荐配置为整个堆的3/8。

-Xss128k:设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M,以前每个线程堆栈大小为256K。更具应用的线程所需内存大 小进行调整。在相同物理内存下,减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的,不能无限生成,经验值在3000~5000 左右。



如何识别垃圾
引用计数法
最容易想到的一种方式是引用计数法，啥叫引用计数法，简单地说，就是对象被引用一次，在它的对象头上加一次引用次数，如果没有被引用（引用次数为 0），则此对象可回收
String ref = new String("Java");
以上代码 ref1 引用了右侧定义的对象，所以引用次数是 1

如果在上述代码后面添加一个 ref = null，则由于对象没被引用，引用次数置为 0，由于不被任何变量引用，此时即被回收，动图如下

看起来用引用计数确实没啥问题了，不过它无法解决一个主要的问题：循环引用！啥叫循环引用
public class TestRC {

    TestRC instance;
    public TestRC(String name) {
    }

    public static  void main(String[] args) {
        // 第一步
	A a = new TestRC("a");
	B b = new TestRC("b");

        // 第二步
	a.instance = b;
	b.instance = a;

        // 第三步
	a = null;
	b = null;
    }
}
按步骤一步步画图


到了第三步，虽然 a，b 都被置为 null 了，但是由于之前它们指向的对象互相指向了对方（引用计数都为 1），所以无法回收，也正是由于无法解决循环引用的问题，所以现代虚拟机都不用引用计数法来判断对象是否应该被回收。
可达性算法
现代虚拟机基本都是采用这种算法来判断对象是否存活，可达性算法的原理是以一系列叫做  GC Root  的对象为起点出发，引出它们指向的下一个节点，再以下个节点为起点，引出此节点指向的下一个结点。。。（这样通过 GC Root 串成的一条线就叫引用链），直到所有的结点都遍历完毕,如果相关对象不在任意一个以 GC Root 为起点的引用链中，则这些对象会被判断为「垃圾」,会被 GC 回收。


如图示，如果用可达性算法即可解决上述循环引用的问题，因为从GC Root 出发没有到达 a,b,所以 a，b 可回收
a, b 对象可回收，就一定会被回收吗?并不是，对象的 finalize 方法给了对象一次垂死挣扎的机会，当对象不可达（可回收）时，当发生GC时，会先判断对象是否执行了 finalize 方法，如果未执行，则会先执行 finalize 方法，我们可以在此方法里将当前对象与 GC Roots 关联，这样执行 finalize 方法之后，GC 会再次判断对象是否可达，如果不可达，则会被回收，如果可达，则不回收！
注意： finalize 方法只会被执行一次，如果第一次执行 finalize 方法此对象变成了可达确实不会回收，但如果对象再次被 GC，则会忽略 finalize 方法，对象会被回收！这一点切记!
那么这些 GC Roots 到底是什么东西呢，哪些对象可以作为 GC Root 呢，有以下几类
虚拟机栈（栈帧中的本地变量表）中引用的对象
方法区中类静态属性引用的对象
方法区中常量引用的对象
本地方法栈中 JNI（即一般说的 Native 方法）引用的对象
虚拟机栈中引用的对象
如下代码所示，a 是栈帧中的本地变量，当 a = null 时，由于此时 a 充当了 GC Root 的作用，a 与原来指向的实例 new Test() 断开了连接，所以对象会被回收。
publicclass Test {
    public static  void main(String[] args) {
	Test a = new Test();
	a = null;
    }
}
方法区中类静态属性引用的对象
如下代码所示，当栈帧中的本地变量 a = null 时，由于 a 原来指向的对象与 GC Root (变量 a) 断开了连接，所以 a 原来指向的对象会被回收，而由于我们给 s 赋值了变量的引用，s 在此时是类静态属性引用，充当了 GC Root 的作用，它指向的对象依然存活!
public class Test {
    public static Test s;
    public static  void main(String[] args) {
	Test a = new Test();
	a.s = new Test();
	a = null;
    }
}
方法区中常量引用的对象
如下代码所示，常量 s 指向的对象并不会因为 a 指向的对象被回收而回收
public class Test {
	public static final Test s = new Test();
        public static void main(String[] args) {
	    Test a = new Test();
	    a = null;
        }
}
本地方法栈中 JNI 引用的对象
这是简单给不清楚本地方法为何物的童鞋简单解释一下：所谓本地方法就是一个 java 调用非 java 代码的接口，该方法并非 Java 实现的，可能由 C 或 Python等其他语言实现的， Java 通过 JNI 来调用本地方法， 而本地方法是以库文件的形式存放的（在 WINDOWS 平台上是 DLL 文件形式，在 UNIX 机器上是 SO 文件形式）。通过调用本地的库文件的内部方法，使 JAVA 可以实现和本地机器的紧密联系，调用系统级的各接口方法。
当调用 Java 方法时，虚拟机会创建一个栈桢并压入 Java 栈，而当它调用的是本地方法时，虚拟机会保持 Java 栈不变，不会在 Java 栈祯中压入新的祯，虚拟机只是简单地动态连接并直接调用指定的本地方法。



JNIEXPORT void JNICALL Java_com_pecuyu_jnirefdemo_MainActivity_newStringNative(JNIEnv *env, jobject instance，jstring jmsg) {
...
   // 缓存String的class
   jclass jc = (*env)->FindClass(env, STRING_PATH);
}
如上代码所示，当 java 调用以上本地方法时，jc 会被本地方法栈压入栈中, jc 就是我们说的本地方法栈中 JNI 的对象引用，因此只会在此本地方法执行完成后才会被释放。

垃圾回收主要方法
上一节我们知道了可以通过可达性算法来识别哪些数据是垃圾，那该怎么对这些垃圾进行回收呢。主要有以下几种方式方式
标记清除算法
复制算法
标记整理法
标记清除算法
步骤很简单
1.先根据可达性算法标记出相应的可回收对象（图中黄色部分）
2.对可回收的对象进行回收

操作起来确实很简单，也不用做移动数据的操作，那有啥问题呢？仔细看上图，没错，内存碎片！假如我们想在上图中的堆中分配一块需要连续内存占用 4M 或 5M 的区域，显然是会失败，怎么解决呢，如果能把上面未使用的 2M， 2M，1M 内存能连起来就能连成一片可用空间为 5M 的区域即可，怎么做呢?
复制算法
把堆等分成两块区域, A 和 B，区域 A 负责分配对象，区域 B 不分配, 对区域 A 使用以上所说的标记法把存活的对象标记出来（下图有误无需清除），然后把区域 A 中存活的对象都复制到区域 B（存活对象都依次紧邻排列）最后把 A 区对象全部清理掉释放出空间，这样就解决了内存碎片的问题了。

不过复制算法的缺点很明显，比如给堆分配了 500M 内存，结果只有 250M 可用，空间平白无故减少了一半！这肯定是不能接受的！另外每次回收也要把存活对象移动到另一半，效率低下（我们可以想想删除数组元素再把非删除的元素往一端移，效率显然堪忧）
标记整理法
前面两步和标记清除法一样，不同的是它在标记清除法的基础上添加了一个整理的过程 ，即将所有的存活对象都往一端移动,紧邻排列（如图示），再清理掉另一端的所有区域，这样的话就解决了内存碎片的问题。

但是缺点也很明显：每进一次垃圾清除都要频繁地移动存活的对象，效率十分低下。
分代收集算法
分代收集算法整合了以上算法，综合了这些算法的优点，最大程度避免了它们的缺点，所以是现代虚拟机采用的首选算法,与其说它是算法，倒不是说它是一种策略，因为它是把上述几种算法整合在了一起，为啥需要分代收集呢，来看一下对象的分配有啥规律

由图可知，大部分的对象都很短命，都在很短的时间内都被回收了（IBM 专业研究表明，一般来说，98% 的对象都是朝生夕死的，经过一次 Minor GC 后就会被回收），所以分代收集算法根据对象存活周期的不同将堆分成新生代和老生代（Java8以前还有个永久代）,默认比例为 1 : 2，新生代又分为 Eden 区， from Survivor 区（简称S0），to Survivor 区(简称 S1),三者的比例为 8: 1 : 1，这样就可以根据新老生代的特点选择最合适的垃圾回收算法，我们把新生代发生的 GC 称为 Young GC（也叫 Minor GC）,老年代发生的 GC 称为 Old GC（也称为 Full GC）。
那么分代垃圾收集是怎么工作的呢，我们一起来看看
分代收集工作原理
1、对象在新生代的分配与回收
由以上的分析可知，大部分对象在很短的时间内都会被回收，对象一般分配在 Eden 区


当 Eden 区将满时，触发 Minor GC

我们之前怎么说来着，大部分对象在短时间内都会被回收, 所以经过 Minor GC 后只有少部分对象会存活，它们会被移到 S0 区（这就是为啥空间大小  Eden: S0: S1 = 8:1:1, Eden 区远大于 S0,S1 的原因，因为在 Eden 区触发的 Minor GC 把大部对象（接近98%）都回收了,只留下少量存活的对象，此时把它们移到 S0 或 S1 绰绰有余）同时对象年龄加一（对象的年龄即发生 Minor GC 的次数），最后把 Eden 区对象全部清理以释放出空间,动图如下


当触发下一次 Minor GC 时，会把 Eden 区的存活对象和 S0（或S1） 中的存活对象（S0 或 S1 中的存活对象经过每次 Minor GC 都可能被回收）一起移到 S1（Eden 和 S0 的存活对象年龄+1）, 同时清空 Eden 和 S0 的空间。
若再触发下一次 Minor GC，则重复上一步，只不过此时变成了 从 Eden，S1 区将存活对象复制到 S0 区,每次垃圾回收, S0, S1 角色互换，都是从 Eden ,S0(或S1) 将存活对象移动到 S1(或S0)。也就是说在 Eden 区的垃圾回收我们采用的是复制算法，因为在 Eden 区分配的对象大部分在 Minor GC 后都消亡了，只剩下极少部分存活对象（这也是为啥 Eden:S0:S1 默认为 8:1:1 的原因），S0,S1 区域也比较小，所以最大限度地降低了复制算法造成的对象频繁拷贝带来的开销。
2、对象何时晋升老年代

当对象的年龄达到了我们设定的阈值，则会从S0（或S1）晋升到老年代如图示：年龄阈值设置为 15， 当发生下一次 Minor GC 时，S0 中有个对象年龄达到 15，达到我们的设定阈值，晋升到老年代！


大对象 当某个对象分配需要大量的连续内存时，此时对象的创建不会分配在 Eden 区，会直接分配在老年代，因为如果把大对象分配在 Eden 区, Minor GC 后再移动到 S0,S1 会有很大的开销（对象比较大，复制会比较慢，也占空间），也很快会占满 S0,S1 区，所以干脆就直接移到老年代.


还有一种情况也会让对象晋升到老年代，即在 S0（或S1） 区相同年龄的对象大小之和大于 S0（或S1）空间一半以上时，则年龄大于等于该年龄的对象也会晋升到老年代。

3、空间分配担保
在发生 MinorGC 之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象的总空间，如果大于，那么Minor GC 可以确保是安全的,如果不大于，那么虚拟机会查看 HandlePromotionFailure 设置值是否允许担保失败。如果允许，那么会继续检查老年代最大可用连续空间是否大于历次晋升到老年代对象的平均大小，如果大于则进行 Minor GC，否则可能进行一次 Full GC。
4、Stop The World
如果老年代满了，会触发 Full GC, Full GC 会同时回收新生代和老年代（即对整个堆进行GC），它会导致 Stop The World（简称 STW）,造成挺大的性能开销。
什么是 STW ？所谓的 STW, 即在 GC（minor GC 或 Full GC）期间，只有垃圾回收器线程在工作，其他工作线程则被挂起。

一般 Full GC 会导致工作线程停顿时间过长（因为Full GC 会清理整个堆中的不可用对象，一般要花较长的时间），如果在此 server 收到了很多请求，则会被拒绝服务！所以我们要尽量减少 Full GC（Minor GC 也会造成 STW,但只会触发轻微的 STW,因为 Eden 区的对象大部分都被回收了，只有极少数存活对象会通过复制算法转移到 S0 或 S1 区，所以相对还好）。
现在我们应该明白把新生代设置成 Eden, S0，S1区或者给对象设置年龄阈值或者默认把新生代与老年代的空间大小设置成 1:2 都是为了尽可能地避免对象过早地进入老年代，尽可能晚地触发 Full GC。想想新生代如果只设置 Eden 会发生什么，后果就是每经过一次 Minor GC，存活对象会过早地进入老年代，那么老年代很快就会装满，很快会触发 Full GC，而对象其实在经过两三次的 Minor GC 后大部分都会消亡，所以有了 S0,S1的缓冲，只有少数的对象会进入老年代，老年代大小也就不会这么快地增长，也就避免了过早地触发 Full GC。
由于 Full GC（或Minor GC） 会影响性能，所以我们要在一个合适的时间点发起 GC，这个时间点被称为 Safe Point，这个时间点的选定既不能太少以让 GC 时间太长导致程序过长时间卡顿，也不能过于频繁以至于过分增大运行时的负荷。一般当线程在这个时间点上状态是可以确定的，如确定 GC Root 的信息等，可以使 JVM 开始安全地 GC。Safe Point 主要指的是以下特定位置：
循环的末尾
方法返回前
调用方法的 call 之后
抛出异常的位置 另外需要注意的是由于新生代的特点（大部分对象经过 Minor GC后会消亡）， Minor GC 用的是复制算法，而在老生代由于对象比较多，占用的空间较大，使用复制算法会有较大开销（复制算法在对象存活率较高时要进行多次复制操作，同时浪费一半空间）所以根据老生代特点，在老年代进行的 GC 一般采用的是标记整理法来进行回收。

垃圾收集器种类
如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。Java 虚拟机规范并没有规定垃圾收集器应该如何实现，因此一般来说不同厂商，不同版本的虚拟机提供的垃圾收集器实现可能会有差别，一般会给出参数来让用户根据应用的特点来组合各个年代使用的收集器，主要有以下垃圾收集器

在新生代工作的垃圾回收器：Serial, ParNew, ParallelScavenge
在老年代工作的垃圾回收器：CMS，Serial Old, Parallel Old
同时在新老生代工作的垃圾回收器：G1
图片中的垃圾收集器如果存在连线，则代表它们之间可以配合使用，接下来我们来看看各个垃圾收集器的具体功能。
新生代收集器
Serial 收集器
Serial 收集器是工作在新生代的，单线程的垃圾收集器，单线程意味着它只会使用一个 CPU 或一个收集线程来完成垃圾回收，不仅如此，还记得我们上文提到的 STW 了吗，它在进行垃圾收集时，其他用户线程会暂停，直到垃圾收集结束，也就是说在 GC 期间，此时的应用不可用。
看起来单线程垃圾收集器不太实用，不过我们需要知道的任何技术的使用都不能脱离场景，在 Client 模式下，它简单有效（与其他收集器的单线程比），对于限定单个 CPU 的环境来说，Serial 单线程模式无需与其他线程交互，减少了开销，专心做 GC 能将其单线程的优势发挥到极致，另外在用户的桌面应用场景，分配给虚拟机的内存一般不会很大，收集几十甚至一两百兆（仅是新生代的内存，桌面应用基本不会再大了），STW 时间可以控制在一百多毫秒内，只要不是频繁发生，这点停顿是可以接受的，所以对于运行在 Client 模式下的虚拟机，Serial 收集器是新生代的默认收集器
ParNew 收集器
ParNew 收集器是 Serial 收集器的多线程版本，除了使用多线程，其他像收集算法,STW,对象分配规则，回收策略与 Serial 收集器完成一样，在底层上，这两种收集器也共用了相当多的代码，它的垃圾收集过程如下

ParNew 主要工作在 Server 模式，我们知道服务端如果接收的请求多了，响应时间就很重要了，多线程可以让垃圾回收得更快，也就是减少了 STW 时间，能提升响应时间，所以是许多运行在 Server 模式下的虚拟机的首选新生代收集器，另一个与性能无关的原因是因为除了 Serial  收集器，只有它能与 CMS 收集器配合工作，CMS 是一个划时代的垃圾收集器，是真正意义上的并发收集器，它第一次实现了垃圾收集线程与用户线程（基本上）同时工作，它采用的是传统的 GC 收集器代码框架，与 Serial,ParNew 共用一套代码框架，所以能与这两者一起配合工作，而后文提到的 Parallel Scavenge 与 G1 收集器没有使用传统的 GC 收集器代码框架，而是另起炉灶独立实现的，另外一些收集器则只是共用了部分的框架代码,所以无法与 CMS 收集器一起配合工作。
在多 CPU 的情况下，由于 ParNew 的多线程回收特性，毫无疑问垃圾收集会更快，也能有效地减少 STW 的时间，提升应用的响应速度。
Parallel Scavenge 收集器
Parallel Scavenge 收集器也是一个使用复制算法，多线程，工作于新生代的垃圾收集器，看起来功能和 ParNew 收集器一样，它有啥特别之处吗
关注点不同，CMS 等垃圾收集器关注的是尽可能缩短垃圾收集时用户线程的停顿时间，而 Parallel Scavenge 目标是达到一个可控制的吞吐量（吞吐量 = 运行用户代码时间 / （运行用户代码时间+垃圾收集时间）），也就是说 CMS 等垃圾收集器更适合用到与用户交互的程序，因为停顿时间越短，用户体验越好，而 Parallel Scavenge 收集器关注的是吞吐量，所以更适合做后台运算等不需要太多用户交互的任务。
Parallel Scavenge 收集器提供了两个参数来精确控制吞吐量，分别是控制最大垃圾收集时间的 -XX:MaxGCPauseMillis 参数及直接设置吞吐量大小的 -XX:GCTimeRatio（默认99%）
除了以上两个参数，还可以用 Parallel Scavenge 收集器提供的第三个参数 -XX:UseAdaptiveSizePolicy，开启这个参数后，就不需要手工指定新生代大小,Eden 与 Survivor 比例（SurvivorRatio）等细节，只需要设置好基本的堆大小（-Xmx 设置最大堆）,以及最大垃圾收集时间与吞吐量大小，虚拟机就会根据当前系统运行情况收集监控信息，动态调整这些参数以尽可能地达到我们设定的最大垃圾收集时间或吞吐量大小这两个指标。自适应策略也是 Parallel Scavenge  与 ParNew 的重要区别！
老年代收集器
Serial Old 收集器
上文我们知道， Serial 收集器是工作于新生代的单线程收集器，与之相对地，Serial Old 是工作于老年代的单线程收集器，此收集器的主要意义在于给 Client 模式下的虚拟机使用，如果在 Server 模式下，则它还有两大用途：一种是在 JDK 1.5 及之前的版本中与 Parallel Scavenge 配合使用，另一种是作为 CMS 收集器的后备预案,在并发收集发生 Concurrent Mode Failure 时使用（后文讲述）,它与 Serial 收集器配合使用示意图如下

Parallel Old 收集器
Parallel Old 是相对于 Parallel Scavenge 收集器的老年代版本，使用多线程和标记整理法，两者组合示意图如下,这两者的组合由于都是多线程收集器，真正实现了「吞吐量优先」的目标

CMS 收集器
CMS 收集器是以实现最短 STW 时间为目标的收集器，如果应用很重视服务的响应速度，希望给用户最好的体验，则 CMS 收集器是个很不错的选择！
我们之前说老年代主要用标记整理法，而 CMS 虽然工作于老年代，但采用的是标记清除法，主要有以下四个步骤
①　初始标记
②　并发标记
③　重新标记
④　并发清除

从图中可以的看到初始标记和重新标记两个阶段会发生 STW，造成用户线程挂起，不过初始标记仅标记 GC Roots 能关联的对象，速度很快，并发标记是进行 GC Roots  Tracing 的过程，重新标记是为了修正并发标记期间因用户线程继续运行而导致标记产生变动的那一部分对象的标记记录，这一阶段停顿时间一般比初始标记阶段稍长，但远比并发标记时间短。
整个过程中耗时最长的是并发标记和标记清理，不过这两个阶段用户线程都可工作，所以不影响应用的正常使用，所以总体上看，可以认为 CMS 收集器的内存回收过程是与用户线程一起并发执行的。
但是 CMS 收集器远达不到完美的程度，主要有以下三个缺点
CMS 收集器对 CPU 资源非常敏感  原因也可以理解，比如本来我本来可以有 10 个用户线程处理请求，现在却要分出 3 个作为回收线程，吞吐量下降了30%，CMS 默认启动的回收线程数是 （CPU数量+3）/ 4, 如果 CPU 数量只有一两个，那吞吐量就直接下降 50%,显然是不可接受的
CMS 无法处理浮动垃圾（Floating Garbage）,可能出现 「Concurrent Mode Failure」而导致另一次 Full GC 的产生，由于在并发清理阶段用户线程还在运行，所以清理的同时新的垃圾也在不断出现，这部分垃圾只能在下一次 GC 时再清理掉（即浮云垃圾），同时在垃圾收集阶段用户线程也要继续运行，就需要预留足够多的空间要确保用户线程正常执行，这就意味着 CMS 收集器不能像其他收集器一样等老年代满了再使用，JDK 1.5 默认当老年代使用了68%空间后就会被激活，当然这个比例可以通过 -XX:CMSInitiatingOccupancyFraction 来设置，但是如果设置地太高很容易导致在 CMS 运行期间预留的内存无法满足程序要求，会导致 Concurrent Mode Failure 失败，这时会启用 Serial Old 收集器来重新进行老年代的收集，而我们知道 Serial Old 收集器是单线程收集器，这样就会导致 STW 更长了。
CMS 采用的是标记清除法，上文我们已经提到这种方法会产生大量的内存碎片，这样会给大内存分配带来很大的麻烦，如果无法找到足够大的连续空间来分配对象，将会触发 Full GC，这会影响应用的性能。当然我们可以开启 -XX:+UseCMSCompactAtFullCollection（默认是开启的），用于在 CMS 收集器顶不住要进行 FullGC 时开启内存碎片的合并整理过程，内存整理会导致 STW，停顿时间会变长，还可以用另一个参数 -XX:CMSFullGCsBeforeCompation 用来设置执行多少次不压缩的 Full GC 后跟着带来一次带压缩的。
G1（Garbage First） 收集器
G1 收集器是面向服务端的垃圾收集器，被称为驾驭一切的垃圾回收器，主要有以下几个特点
像 CMS 收集器一样，能与应用程序线程并发执行。
整理空闲空间更快。
需要 GC 停顿时间更好预测。
不会像 CMS 那样牺牲大量的吞吐性能。
不需要更大的 Java Heap
与 CMS 相比，它在以下两个方面表现更出色
1.运作期间不会产生内存碎片，G1 从整体上看采用的是标记-整理法，局部（两个 Region）上看是基于复制算法实现的，两个算法都不会产生内存碎片，收集后提供规整的可用内存，这样有利于程序的长时间运行。
2.在 STW 上建立了可预测的停顿时间模型，用户可以指定期望停顿时间，G1 会将停顿时间控制在用户设定的停顿时间以内。
为什么G1能建立可预测的停顿模型呢，主要原因在于 G1 对堆空间的分配与传统的垃圾收集器不一器，传统的内存分配就像我们前文所述，是连续的，分成新生代，老年代，新生代又分 Eden,S0,S1,如下

而 G1 各代的存储地址不是连续的，每一代都使用了 n 个不连续的大小相同的 Region，每个Region占有一块连续的虚拟内存地址，如图示

除了和传统的新老生代，幸存区的空间区别，Region还多了一个H，它代表Humongous，这表示这些Region存储的是巨大对象（humongous object，H-obj），即大小大于等于region一半的对象，这样超大对象就直接分配到了老年代，防止了反复拷贝移动。那么 G1 分配成这样有啥好处呢？
传统的收集器如果发生 Full GC 是对整个堆进行全区域的垃圾收集，而分配成各个 Region 的话，方便 G1 跟踪各个 Region 里垃圾堆积的价值大小（回收所获得的空间大小及回收所需经验值），这样根据价值大小维护一个优先列表，根据允许的收集时间，优先收集回收价值最大的 Region,也就避免了整个老年代的回收，也就减少了 STW 造成的停顿时间。同时由于只收集部分 Region,可就做到了 STW 时间的可控。
G1 收集器的工作步骤如下
①　初始标记
②　并发标记
③　最终标记
④　筛选回收

可以看到整体过程与 CMS 收集器非常类似，筛选阶段会根据各个 Region 的回收价值和成本进行排序，根据用户期望的 GC 停顿时间来制定回收计划。
总结
本文简述了垃圾回收的原理与垃圾收集器的种类，相信大家对开头提的一些问题应该有了更深刻的认识，在生产环境中我们要根据不同的场景来选择垃圾收集器组合，如果是运行在桌面环境处于 Client 模式的，则用 Serial + Serial Old 收集器绰绰有余，如果需要响应时间快，用户体验好的，则用 ParNew + CMS 的搭配模式，即使是号称是「驾驭一切」的 G1，也需要根据吞吐量等要求适当调整相应的 JVM 参数，没有最牛的技术，只有最合适的使用场景，切记！
内存分配与内存回收(GC)
java虚拟机的内存分配与回收机制
　　分为4个方面来介绍内存分配与回收，分别是内存是如何分配的、哪些内存需要回收、在什么情况下执行回收、如何监控和优化GC机制。

java GC（Garbage Collction）垃圾回收机制，是java与C/C++的主要区别之一。通过对jvm中内存进行标记，自主回收一些无用的内存。目前使用的最多的是sun公司jdk中的HotSpot，所以本文也以该jvm作为介绍的根本。

1.Java内存区域

在java运行时的数据取里，由jvm管理的内存区域分为多个部分：



程序计数器(program counter register)：程序计数器是用来表示当前程序运行哪里的一个指示器。由于每个线程都由自己的执行顺序，所以程序计数器是线程私有的，每个线程都要由一个自己的程序计数器来指示自己（线程）下一步要执行哪条指令。

如果程序执行的是一个java方法，那么计数器记录的是正在执行的虚拟机字节码指令地址；如果正在执行的是一个本地方法（native方法），那么计数器的值为Undefined。由于程序计数器记录的只是当前指令地址，所以不存在内存泄漏的情况，也是jvm内存区域中唯一一个没有OOME（out of memory error)定义的区域。

虚拟机栈(JVM stack)：当线程的每个方法在执行的时候都会创建一个栈帧（Stack Frame）用来存储方法中的局部变量、方法出口等，同时会将这个栈帧放入JVM栈中，方法调用完成时，这个栈帧出栈。每个线程都要一个自己的虚拟机栈来保存自己的方法调用时候的数据，因此虚拟机栈也是线程私有的。

虚拟机栈中定义了两种异常，如果线程调用的栈深度大于虚拟机允许的最大深度，抛出StackOverFlowError，不过虚拟机基本上都允许动态扩展虚拟机栈的大小。这样的话线程可以一直申请栈，直到内存不足的时候，会抛出OOME（out of memory error）内存溢出。

本地方法栈(Native Method Stack）：本地方法栈与虚拟机栈类似，只是本地方法栈存放的栈帧是在native方法调用的时候产生的。有的虚拟机中会将本地方法栈和虚拟栈放在一起，因此本地方法栈也是线程私有的。

堆（Heap）：堆是java GC机制中最重要的区域。堆是为了放置“对象的实例”，对象都是在堆区上分配内存的，堆在逻辑上连续，在物理上不一定连续。所有的线程共用一个堆，堆的大小是可扩展的，如果在执行GC之后，仍没有足够的内存可以分配且堆大小不可再扩展，将会抛出OOME。

方法区（Method Area）：又叫静态区，用于存储类的信息、常量池等，逻辑上是堆的一部分，是各个线程共享的区域，为了与堆区分，又叫非堆。在永久代还存在时，方法区被用作永久代。方法区可以选择是否开启垃圾回收。jvm内存不足时会抛出OOME。

直接内存（Direct Memory）：直接内存指的是非jvm管理的内存，是机器剩余的内存。用基于通道（Channel）和缓冲区(Buffer）的方式来进行内存分配，用存储在JVM中的DirectByteBuffer来引用，当机器本身内存不足时，也会抛出OOME。

举例说明：Object obj = new Object();

obj表示一个本地引用，存储在jvm栈的本地变量表中，new Object()作为一个对象放在堆中，Object类的类型信息（接口，方法，对象类型等）放在堆中，而这些类型信息的地址放在方法区中。

这里需要知道如何通过引用访问到具体对象，也就是通过obj引用如何找到new出来的这个Object()对象，主要有两种方法，通过句柄和通过直接指针访问。

　　
通过句柄：



在java堆中会专门有一块区域被划分为句柄池，一个引用的背后是一个对象实例数据（java堆中）的指针和对象类型信息（方法区中）的指针，而这两个指针都是在java堆上的。这种方法是优势是较为稳定，但是速度不是很快。

通过直接指针：



一个引用背后是一个对象的实例数据，这个实例数据里面包含了“到对象类型信息的指针”。这种方式的优势是速度快，在HotSpot中用的就是这种方式。

2.内存是如何分配和回收的

内存分配主要是在堆上的分配，如前面new出来的对象，放在堆上，但是现代技术也支持在栈上分配，较为少见，本文不考虑。分配内存与回收内存的标准是八个字：分代分配，分代回收。那么这个代是什么呢？

jvm中将对象根据存活的时间划分为三代：年轻代（Young Generation）、年老代（Old Generation）和永久代（Permannent Generation）。在jdk1.8中已经不再使用永久代，因此这里不再介绍。




年轻代：又叫新生代，所有新生成的对象都是先放在年轻代。年轻代分三个区，一个Eden区，两个Survivor区，一个叫From，一个叫To（这个名字是动态变化的）。当Eden中满时，执行Minor GC将消亡的对象清理掉，仍存活的对象将被复制到Survivor中的From区，清空Eden。当这个From区满的时候，仍存活的对象将被复制到To区，清空From区，并且原From区变为To区，原To区变为From区，这样的目的是保证To区一直为空。当From区满无对象可清理或者From-To区交换的次数超过设定（HotSpot默认为15，通过-XX：MaxTenuringThreashold控制）的时候，仍存活的对象进入老年代。年轻代中Eden和Servivor的比例通过-XX：SerivorRation参数来配置，默认为8，也就时说Eden：From：To=8：1：1。年轻代的回收方式叫做Minor GC，又叫停止-复制清理法。这种方法在回收的时候，需要暂停其他所有线程的执行，导致效率很低，现在虽然有优化，但是仅仅是将停止的时间变短，并没有彻底取消这个停止。

年老代：年老代的空间较大，当年老代内存不足时，将执行Major GC也叫Full GC。如果对象比较大，可能会直接分配到老年代上而不经过年轻代。用-XX：pertenureSizeThreashold来设定这个值，大于这个的对象会直接分配到老年代上。

3.垃圾收集器

在GC机制中，起作用的是垃圾收集器。HotSpot1.6中使用的垃圾收集器如下（有连线表示有联系）：



Serial收集器：新生代（年轻代）收集器，使用停止-复制算法，使用一个线程进行GC，其他工作线程暂停。

ParNew收集器：新生代收集器，使用停止-复制算法，Serial收集器的多线程版，用多个线程进行GC，其他工作线程暂停，关注缩短垃圾收集时间。

Parallel Scavenge收集器：新生代收集器，使用停止-复制算法，关注CPU吞吐量，即运行用户代码的时间/总时间。

Serial Old收集器：年老代收集器，单线程收集器，使用标记-整理算法（整理的方法包括sweep清理和compact压缩，标记-清理是先标记需要回收的对象，在标记完成后统一清楚标记的对象，这样清理之后空闲的内存是不连续的；标记-压缩是先标记需要回收的对象，把存活的对象都向一端移动，然后直接清理掉端边界以外的内存，这样清理之后空闲的内存是连续的）。

Parallel Old收集器：老年代收集器，多线程收集器，使用标记-整理算法（整理的方法包括summary汇总和compact压缩，标记-压缩与Serial Old一样，标记-汇总是将幸存的对象复制到预先准备好的区域，再清理之前的对象）。

CMS（Concurrent Mark Sweep）收集器：老年老代收集器，多线程收集器，关注最短回收时间停顿，使用标记-清除算法，用户线程可以和GC线程同时工作。

G1收集器：JDK1.7中发布，使用较少，不作介绍。

Java GC是一个非常复杂的机制，想要详细说清楚他需要很多时间，如有错误恳请指正。
JVM线上调优
Azkaban执行多次调度任务之后，就会进入preparation 等待状态，整个服务器就卡住。修改初始堆内存与最大堆内存为2G，
一般和Xmx配置成一样以避免每次gc后JVM重新分配内存。

之后还是不行。后面通过jstat定位到fullGC时间等待太长，通过jstack定位线程问题，最后发现是每次打包etl脚本，执行task之前，Azkaban会进行一个遍历包含etl脚本的压缩文件。直接读取文件到内存之中，文件过大，直接造成频繁fullGC。最后通过执行完task，清空压缩包的方式。解决了问题

一个性能较好的web服务器jvm参数配置：

-server//服务器模式
-Xmx2g //JVM最大允许分配的堆内存，按需分配
-Xms2g //JVM初始分配的堆内存，一般和Xmx配置成一样以避免每次gc后JVM重新分配内存。
-Xmn256m //年轻代内存大小，整个JVM内存=年轻代 + 年老代 + 持久代
-XX:PermSize=128m //持久代内存大小
-Xss256k //设置每个线程的堆栈大小
-XX:+DisableExplicitGC //忽略手动调用GC, System.gc()的调用就会变成一个空调用，完全不触发GC
-XX:+UseConcMarkSweepGC //并发标记清除（CMS）收集器
-XX:+CMSParallelRemarkEnabled //降低标记停顿
-XX:+UseCMSCompactAtFullCollection //在FULL GC的时候对年老代的压缩
-XX:LargePageSizeInBytes=128m //内存页的大小
-XX:+UseFastAccessorMethods //原始类型的快速优化
-XX:+UseCMSInitiatingOccupancyOnly //使用手动定义初始化定义开始CMS收集
-XX:CMSInitiatingOccupancyFraction=70 //使用cms作为垃圾回收使用70％后开始CMS收集
JVM调优参数（JVM调优总结（七）-典型配置举例1）
堆大小设置
年轻代的设置很关键
JVM中最大堆大小有三方面限制：相关操作系统的数据模型（32-bt还是64-bit）限制；系统的可用虚拟内存限制；系统的可用物理内存限制。32位系统下，一般限制在1.5G~2G；64为操作系统对内存无限制。在Windows Server 2003 系统，3.5G物理内存，JDK5.0下测试，最大可设置为1478m。
典型设置：
java -Xmx3550m -Xms3550m -Xmn2g –Xss128k
-Xmx3550m：设置JVM最大可用内存为3550M。
-Xms3550m：设置JVM促使内存为3550m。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。
-Xmn2g：设置年轻代大小为2G。整个堆大小=年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。
-Xss128k：设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。更具应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。
 
java -Xmx3550m -Xms3550m -Xss128k -XX:NewRatio=4 -XX:SurvivorRatio=4 -XX:MaxPermSize=16m -XX:MaxTenuringThreshold=0
-XX:NewRatio=4:设置年轻代（包括Eden和两个Survivor区）与年老代的比值（除去持久代）。设置为4，则年轻代与年老代所占比值为1：4，年轻代占整个堆栈的1/5
-XX:SurvivorRatio=4：设置年轻代中Eden区与Survivor区的大小比值。设置为4，则两个Survivor区与一个Eden区的比值为2:4，一个Survivor区占整个年轻代的1/6
-XX:MaxPermSize=16m:设置持久代大小为16m。
-XX:MaxTenuringThreshold=0：设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概论。
 
回收器选择
JVM给了三种选择：串行收集器、并行收集器、并发收集器，但是串行收集器只适用于小数据量的情况，所以这里的选择主要针对并行收集器和并发收集器。默认情况下，JDK5.0以前都是使用串行收集器，如果想使用其他收集器需要在启动时加入相应参数。JDK5.0以后，JVM会根据当前系统配置进行判断。
吞吐量优先的并行收集器
如上文所述，并行收集器主要以到达一定的吞吐量为目标，适用于科学技术和后台处理等。
典型配置：
java -Xmx3800m -Xms3800m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:ParallelGCThreads=20
-XX:+UseParallelGC：选择垃圾收集器为并行收集器。此配置仅对年轻代有效。即上述配置下，年轻代使用并发收集，而年老代仍旧使用串行收集。
-XX:ParallelGCThreads=20：配置并行收集器的线程数，即：同时多少个线程一起进行垃圾回收。此值最好配置与处理器数目相等。
java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC -XX:ParallelGCThreads=20 -XX:+UseParallelOldGC
-XX:+UseParallelOldGC：配置年老代垃圾收集方式为并行收集。JDK6.0支持对年老代并行收集。
java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC  -XX:MaxGCPauseMillis=100
-XX:MaxGCPauseMillis=100:设置每次年轻代垃圾回收的最长时间，如果无法满足此时间，JVM会自动调整年轻代大小，以满足此值。
n java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseParallelGC  -XX:MaxGCPauseMillis=100 -XX:+UseAdaptiveSizePolicy
-XX:+UseAdaptiveSizePolicy：设置此选项后，并行收集器会自动选择年轻代区大小和相应的Survivor区比例，以达到目标系统规定的最低相应时间或者收集频率等，此值建议使用并行收集器时，一直打开。
 
响应时间优先的并发收集器
如上文所述，并发收集器主要是保证系统的响应时间，减少垃圾收集时的停顿时间。适用于应用服务器、电信领域等。
典型配置：
java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:ParallelGCThreads=20 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC
-XX:+UseConcMarkSweepGC：设置年老代为并发收集。测试中配置这个以后，-XX:NewRatio=4的配置失效了，原因不明。所以，此时年轻代大小最好用-Xmn设置。
-XX:+UseParNewGC: 设置年轻代为并行收集。可与CMS收集同时使用。JDK5.0以上，JVM会根据系统配置自行设置，所以无需再设置此值。
java -Xmx3550m -Xms3550m -Xmn2g -Xss128k -XX:+UseConcMarkSweepGC -XX:CMSFullGCsBeforeCompaction=5 -XX:+UseCMSCompactAtFullCollection
-XX:CMSFullGCsBeforeCompaction：由于并发收集器不对内存空间进行压缩、整理，所以运行一段时间以后会产生“碎片”，使得运行效率降低。此值设置运行多少次GC以后对内存空间进行压缩、整理。
-XX:+UseCMSCompactAtFullCollection：打开对年老代的压缩。可能会影响性能，但是可以消除碎片
 
辅助信息
JVM提供了大量命令行参数，打印信息，供调试使用。主要有以下一些：
-XX:+PrintGC：输出形式：[GC 118250K->113543K(130112K), 0.0094143 secs] [Full GC 121376K->10414K(130112K), 0.0650971 secs]
-XX:+PrintGCDetails：输出形式：[GC [DefNew: 8614K->781K(9088K), 0.0123035 secs] 118250K->113543K(130112K), 0.0124633 secs] [GC [DefNew: 8614K->8614K(9088K), 0.0000665 secs][Tenured: 112761K->10414K(121024K), 0.0433488 secs] 121376K->10414K(130112K), 0.0436268 secs]
-XX:+PrintGCTimeStamps -XX:+PrintGC：PrintGCTimeStamps可与上面两个混合使用
输出形式：11.851: [GC 98328K->93620K(130112K), 0.0082960 secs]
-XX:+PrintGCApplicationConcurrentTime：打印每次垃圾回收前，程序未中断的执行时间。可与上面混合使用。输出形式：Application time: 0.5291524 seconds
-XX:+PrintGCApplicationStoppedTime：打印垃圾回收期间程序暂停的时间。可与上面混合使用。输出形式：Total time for which application threads were stopped: 0.0468229 seconds
-XX:PrintHeapAtGC: 打印GC前后的详细堆栈信息。输出形式：
34.702: [GC {Heap before gc invocations=7:
def new generation   total 55296K, used 52568K [0x1ebd0000, 0x227d0000, 0x227d0000)
eden space 49152K,  99% used [0x1ebd0000, 0x21bce430, 0x21bd0000)
from space 6144K,  55% used [0x221d0000, 0x22527e10, 0x227d0000)
to   space 6144K,   0% used [0x21bd0000, 0x21bd0000, 0x221d0000)
tenured generation   total 69632K, used 2696K [0x227d0000, 0x26bd0000, 0x26bd0000)
the space 69632K,   3% used [0x227d0000, 0x22a720f8, 0x22a72200, 0x26bd0000)
compacting perm gen  total 8192K, used 2898K [0x26bd0000, 0x273d0000, 0x2abd0000)
   the space 8192K,  35% used [0x26bd0000, 0x26ea4ba8, 0x26ea4c00, 0x273d0000)
ro space 8192K,  66% used [0x2abd0000, 0x2b12bcc0, 0x2b12be00, 0x2b3d0000)
rw space 12288K,  46% used [0x2b3d0000, 0x2b972060, 0x2b972200, 0x2bfd0000)
34.735: [DefNew: 52568K->3433K(55296K), 0.0072126 secs] 55264K->6615K(124928K)Heap after gc invocations=8:
def new generation   total 55296K, used 3433K [0x1ebd0000, 0x227d0000, 0x227d0000)
eden space 49152K,   0% used [0x1ebd0000, 0x1ebd0000, 0x21bd0000)
  from space 6144K,  55% used [0x21bd0000, 0x21f2a5e8, 0x221d0000)
  to   space 6144K,   0% used [0x221d0000, 0x221d0000, 0x227d0000)
tenured generation   total 69632K, used 3182K [0x227d0000, 0x26bd0000, 0x26bd0000)
the space 69632K,   4% used [0x227d0000, 0x22aeb958, 0x22aeba00, 0x26bd0000)
compacting perm gen  total 8192K, used 2898K [0x26bd0000, 0x273d0000, 0x2abd0000)
   the space 8192K,  35% used [0x26bd0000, 0x26ea4ba8, 0x26ea4c00, 0x273d0000)
   ro space 8192K,  66% used [0x2abd0000, 0x2b12bcc0, 0x2b12be00, 0x2b3d0000)
   rw space 12288K,  46% used [0x2b3d0000, 0x2b972060, 0x2b972200, 0x2bfd0000)
}
, 0.0757599 secs]
-Xloggc:filename:与上面几个配合使用，把相关日志信息记录到文件以便分析。

JVM调优总结（十）-调优方法
JVM调优工具
Jconsole，jProfile，VisualVM
Jconsole : jdk自带，功能简单，但是可以在系统有一定负荷的情况下使用。对垃圾回收算法有很详细的跟踪。详细说明参考这里
 
JProfiler：商业软件，需要付费。功能强大。详细说明参考这里
 
VisualVM：JDK自带，功能强大，与JProfiler类似。推荐。
 
如何调优
观察内存释放情况、集合类检查、对象树
上面这些调优工具都提供了强大的功能，但是总的来说一般分为以下几类功能
 
一、堆信息查看
 
可查看堆空间大小分配（年轻代、年老代、持久代分配）
提供即时的垃圾回收功能
垃圾监控（长时间监控回收情况）
 
 
查看堆内类、对象信息查看：数量、类型等
 
 
对象引用情况查看
 
有了堆信息查看方面的功能，我们一般可以顺利解决以下问题：
  --年老代年轻代大小划分是否合理
  --内存泄漏
  --垃圾回收算法设置是否合理
 
二、线程监控
 
线程信息监控：系统线程数量。
线程状态监控：各个线程都处在什么样的状态下
 
 
Dump线程详细信息：查看线程内部运行情况
死锁检查
 
三、热点分析
 
 
 
    CPU热点：检查系统哪些方法占用的大量CPU时间
    内存热点：检查哪些对象在系统中数量最大（一定时间内存活对象和销毁对象一起统计）
 
    这两个东西对于系统优化很有帮助。我们可以根据找到的热点，有针对性的进行系统的瓶颈查找和进行系统优化，而不是漫无目的的进行所有代码的优化。
 
 
四、快照
    快照是系统运行到某一时刻的一个定格。在我们进行调优的时候，不可能用眼睛去跟踪所有系统变化，依赖快照功能，我们就可以进行系统两个不同运行时刻，对象（或类、线程等）的不同，以便快速找到问题
    举例说，我要检查系统进行垃圾回收以后，是否还有该收回的对象被遗漏下来的了。那么，我可以在进行垃圾回收前后，分别进行一次堆情况的快照，然后对比两次快照的对象情况。
 
五、内存泄漏检查
    内存泄漏是比较常见的问题，而且解决方法也比较通用，这里可以重点说一下，而线程、热点方面的问题则是具体问题具体分析了。
    内存泄漏一般可以理解为系统资源（各方面的资源，堆、栈、线程等）在错误使用的情况下，导致使用完毕的资源无法回收（或没有回收），从而导致新的资源分配请求无法完成，引起系统错误。
    内存泄漏对系统危害比较大，因为他可以直接导致系统的崩溃。
    需要区别一下，内存泄漏和系统超负荷两者是有区别的，虽然可能导致的最终结果是一样的。内存泄漏是用完的资源没有回收引起错误，而系统超负荷则是系统确实没有那么多资源可以分配了（其他的资源都在使用）。
 
 
年老代堆空间被占满
异常： java.lang.OutOfMemoryError: Java heap space
说明：

 
    这是最典型的内存泄漏方式，简单说就是所有堆空间都被无法回收的垃圾对象占满，虚拟机无法再在分配新空间。
    如上图所示，这是非常典型的内存泄漏的垃圾回收情况图。所有峰值部分都是一次垃圾回收点，所有谷底部分表示是一次垃圾回收后剩余的内存。连接所有谷底的点，可以发现一条由底到高的线，这说明，随时间的推移，系统的堆空间被不断占满，最终会占满整个堆空间。因此可以初步认为系统内部可能有内存泄漏。（上面的图仅供示例，在实际情况下收集数据的时间需要更长，比如几个小时或者几天）
 
解决：
    这种方式解决起来也比较容易，一般就是根据垃圾回收前后情况对比，同时根据对象引用情况（常见的集合对象引用）分析，基本都可以找到泄漏点。
 
 
持久代被占满
异常：java.lang.OutOfMemoryError: PermGen space
说明：
    Perm空间被占满。无法为新的class分配存储空间而引发的异常。这个异常以前是没有的，但是在Java反射大量使用的今天这个异常比较常见了。主要原因就是大量动态反射生成的类不断被加载，最终导致Perm区被占满。
    更可怕的是，不同的classLoader即便使用了相同的类，但是都会对其进行加载，相当于同一个东西，如果有N个classLoader那么他将会被加载N次。因此，某些情况下，这个问题基本视为无解。当然，存在大量classLoader和大量反射类的情况其实也不多。
解决：
    1. -XX:MaxPermSize=16m
    2. 换用JDK。比如JRocket。
 
 
堆栈溢出
异常：java.lang.StackOverflowError
说明：这个就不多说了，一般就是递归没返回，或者循环调用造成
 
 
线程堆栈满
异常：Fatal: Stack size too small
说明：java中一个线程的空间大小是有限制的。JDK5.0以后这个值是1M。与这个线程相关的数据将会保存在其中。但是当线程空间满了以后，将会出现上面异常。
解决：增加线程栈大小。-Xss2m。但这个配置无法解决根本问题，还要看代码部分是否有造成泄漏的部分。
 
系统内存被占满
异常：java.lang.OutOfMemoryError: unable to create new native thread
说明：
    这个异常是由于操作系统没有足够的资源来产生这个线程造成的。系统创建线程时，除了要在Java堆中分配内存外，操作系统本身也需要分配资源来创建线程。因此，当线程数量大到一定程度以后，堆中或许还有空间，但是操作系统分配不出资源来了，就出现这个异常了。
分配给Java虚拟机的内存愈多，系统剩余的资源就越少，因此，当系统内存固定时，分配给Java虚拟机的内存越多，那么，系统总共能够产生的线程也就越少，两者成反比的关系。同时，可以通过修改-Xss来减少分配给单个线程的空间，也可以增加系统总共内生产的线程数。
解决：
    1. 重新设计系统减少线程数量。
    2. 线程数量不能减少的情况下，通过-Xss减小单个线程大小。以便能生产更多的线程。

什么时候需要jvm调优
1.heap 内存（老年代）持续上涨达到设置的最大内存值；
2.Full GC 次数频繁；
3.GC 停顿时间过长（超过1秒）；
4.应用出现OutOfMemory 等内存异常；
5.应用中有使用本地缓存且占用大量内存空间；
6.系统吞吐量与响应性能不高或下降
JVM调优原则

1.多数的Java应用不需要在服务器上进行JVM优化；

2.多数导致GC问题的Java应用，都不是因为我们参数设置错误，而是代码问题；

3.在应用上线之前，先考虑将机器的JVM参数设置到最优（最适合）；

4.减少创建对象的数量；

5.减少使用全局变量和大对象；

6.JVM优化是到最后不得已才采用的手段；

7.在实际使用中，分析GC情况优化代码比优化JVM参数更好；
jvm调优目标：
  1. Heap 内存使用率 <= 70%;

  2. Old generation内存使用率<= 70%;

  3. avgpause <= 1秒; 

  4. Full gc 次数0 或 avg pause interval >= 24小时 ;

  注意：不同应用，其JVM调优量化目标是不一样的。
JVM内存设置多大合适？Xmx和Xmn如何设置？
依据的原则是根据Java Performance里面的推荐公式来进行设置。




具体来讲：
Java整个堆大小设置，Xmx 和 Xms设置为老年代存活对象的3-4倍，即FullGC之后的老年代内存占用的3-4倍
永久代 PermSize和MaxPermSize设置为老年代存活对象的1.2-1.5倍。
年轻代Xmn的设置为老年代存活对象的1-1.5倍。
老年代的内存大小设置为老年代存活对象的2-3倍。

BTW：
1、Sun官方建议年轻代的大小为整个堆的3/8左右， 所以按照上述设置的方式，基本符合Sun的建议。
2、堆大小=年轻代大小+年老代大小， 即xmx=xmn+老年代大小 。 Permsize不影响堆大小。
3、为什么要按照上面的来进行设置呢？ 没有具体的说明，但应该是根据多种调优之后得出的一个结论。
调优经验，调优步骤、参数分析
1. JVM调优经验总结

JVM调优的一般步骤为：

第1步：分析GC日志及dump文件，判断是否需要优化，确定瓶颈问题点；

第2步：确定JVM调优量化目标；

第3步：确定JVM调优参数（根据历史JVM参数来调整）；

第4步：调优一台服务器，对比观察调优前后的差异；

第5步：不断的分析和调整，直到找到合适的JVM参数配置；

第6步：找到最合适的参数，将这些参数应用到所有服务器，并进行后续跟踪。

2. JVM调优重要参数解析

注意：不同应用，其JVM最佳稳定参数配置是不一样的。

配置： -server

-Xms12g -Xmx12g -XX:PermSize=500m -XX:MaxPermSize=1000m -Xmn2400m -XX:SurvivorRatio=1 -Xss512k -XX:MaxDirectMemorySize=1G

-XX:+DisableExplicitGC -XX:CompileThreshold=8000 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC

-XX:+UseCompressedOops -XX:CMSInitiatingOccupancyFraction=60 -XX:ConcGCThreads=4

-XX:MaxTenuringThreshold=10 -XX:ParallelGCThreads=8

-XX:+ParallelRefProcEnabled -XX:+CMSClassUnloadingEnabled -XX:+CMSParallelRemarkEnabled

-XX:CMSMaxAbortablePrecleanTime=500 -XX:CMSFullGCsBeforeCompaction=4

XX:+UseCMSInitiatingOccupancyOnly -XX:+UseCMSCompactAtFullCollection

-XX:+HeapDumpOnOutOfMemoryError -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:/weblogic/gc/gc_$$.log

重要参数（可调优）解析：

-Xms12g：初始化堆内存大小为12GB。

-Xmx12g：堆内存最大值为12GB 。

-Xmn2400m：新生代大小为2400MB，包括 Eden区与2个Survivor区。

-XX:SurvivorRatio=1：Eden区与一个Survivor区比值为1:1。

-XX:MaxDirectMemorySize=1G：直接内存。报java.lang.OutOfMemoryError: Direct buffer memory 异常可以上调这个值。

-XX:+DisableExplicitGC：禁止运行期显式地调用 System.gc() 来触发fulll GC。

注意: Java RMI的定时GC触发机制可通过配置-Dsun.rmi.dgc.server.gcInterval=86400来控制触发的时间。

-XX:CMSInitiatingOccupancyFraction=60：老年代内存回收阈值，默认值为68。

-XX:ConcGCThreads=4：CMS垃圾回收器并行线程线，推荐值为CPU核心数。

-XX:ParallelGCThreads=8：新生代并行收集器的线程数。

-XX:MaxTenuringThreshold=10：设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概论。

-XX:CMSFullGCsBeforeCompaction=4：指定进行多少次fullGC之后，进行tenured区 内存空间压缩。

-XX:CMSMaxAbortablePrecleanTime=500：当abortable-preclean预清理阶段执行达到这个时间时就会结束。

3. 触发Full GC的场景及应对策略

年轻代空间（包括 Eden 和 Survivor 区域）回收内存被称为 Minor GC，对老年代GC称为MajorGC，而Full GC是对整个堆来说的，在最近几个版本的JDK里默认包括了对永生带即方法区的回收（JDK8中无永生带了），出现Full GC的时候经常伴随至少一次的Minor GC,但非绝对的。MajorGC的速度一般会比Minor GC慢10倍以上。

触发Full GC的场景及应对策略：

1.System.gc()方法的调用，应对策略：通过-XX:+DisableExplicitGC来禁止调用System.gc ;
2.老年代代空间不足，应对策略：让对象在Minor GC阶段被回收，让对象在新生代多存活一段时间，不要创建过大的对象及数组;
3.永生区空间不足，应对策略：增大PermGen空间
4.GC时出现promotionfailed和concurrent mode failure，应对策略：增大survivor space
5.Minor GC后晋升到旧生代的对象大小大于老年代的剩余空间，应对策略：增大Tenured space 或下调CMSInitiatingOccupancyFraction=60
内存持续增涨达到上限导致Full GC ，应对策略：通过dumpheap 分析是否存在内存泄漏
4. Gc日志分析工具

借助GCViewer日志分析工具，可以非常直观地分析出待调优点。

可从以下几方面来分析：

1.Memory,分析Totalheap、Tenuredheap、Youngheap内存占用率及其他指标，理论上内存占用率越小越好；

2.Pause ,分析Gc pause、Fullgc pause、Total pause三个大项中各指标，理论上GC次数越少越好，GC时长越小越好；

5. MAT 堆内存分析工具

EclipseMemory Analysis Tools (MAT) 是一个分析Java堆数据的专业工具，用它可以定位内存泄漏的原因。
Full GC出现场景及解决方案
Full GC产生原因
下图为与产生Full GC相关的内存区域，初生代、老年代、以及Metaspace区域（JDK8用metaSpace区域来代替了以前的永久区，这个区域主要存放被加载的class信息）。




触发Full GC的场景及应对策略：

1.System.gc()方法的调用，应对策略：通过-XX:+DisableExplicitGC来禁止调用System.gc ;
2.老年代代空间不足，应对策略：让对象在Minor GC阶段被回收，让对象在新生代多存活一段时间，不要创建过大的对象及数组;
3.永生区空间不足，应对策略：增大PermGen空间
4.GC时出现promotionfailed和concurrent mode failure，应对策略：增大survivor space
5.Minor GC后晋升到旧生代的对象大小大于老年代的剩余空间，应对策略：增大Tenured space 或下调CMSInitiatingOccupancyFraction=60
内存持续增涨达到上限导致Full GC ，应对策略：通过dumpheap 分析是否存在内存泄漏
JVM 调优 —— GC 长时间停顿问题及解决方法
业务场景：垃圾收集器长时间停顿，表现在 Web 页面上可能是页面响应码 500 之类的服务器错误问题，如果是个支付过程可能会导致支付失败，将造成公司的直接经济损失，程序员要尽量避免或者说减少此类情况发生。
一. 并发模式失败（concurrent mode failure）
并发模式失败日志： 


两个原因：
在 CMS 启动过程中，新生代提升速度过快，老年代收集速度赶不上新生代提升速度
在 CMS 启动过程中，老年代碎片化严重，无法容纳新生代提升上来的大对象

发送这种情况，应用线程将会全部停止（相当于网站这段时间无法响应用户请求），进行压缩式垃圾收集（回退到 Serial Old 算法）

解决办法：
新生代提升过快问题：（1）如果频率太快的话，说明空间不足，首先可以尝试调大新生代空间和晋升阈值。（2）如果内存有限，可以设置 CMS 垃圾收集在老年代占比达到多少时启动来减少问题发生频率（越早启动问题发生频率越低，但是会降低吞吐量，具体得多调整几次找到平衡点），参数如下：如果没有第二个参数，会随着 JVM 动态调节 CMS 启动时间
-XX:CMSInitiatingOccupancyFraction=68 （默认是 68）
-XX:+UseCMSInitiatingOccupancyOnly

老年代碎片严重问题：（1）如果频率太快或者 Full GC 后空间释放不多的话，说明空间不足，首先可以尝试调大老年代空间（2）如果内存不足，可以设置进行 n 次 CMS 后进行一次压缩式 Full GC，参数如下：
-XX:+UseCMSCompactAtFullCollection：允许在 Full GC 时，启用压缩式 GC
-XX:CMSFullGCBeforeCompaction=n     在进行 n 次，CMS 后，进行一次压缩的 Full GC，用以减少 CMS 产生的碎片



二. 提升失败（promotion failed）
在 Minor GC 过程中，Survivor Unused 可能不足以容纳 Eden 和另一个 Survivor 中的存活对象， 那么多余的将被移到老年代， 称为过早提升（Premature Promotion）。 这会导致老年代中短期存活对象的增长， 可能会引发严重的性能问题。  再进一步， 如果老年代满了， Minor GC 后会进行 Full GC， 这将导致遍历整个堆， 称为提升失败（Promotion Failure）。

提升失败日志： 

提升失败原因：Minor GC 时发现 Survivor 空间放不下，而老年代的空闲也不够
新生代提升太快
老年代碎片太多，放不下大对象提升（表现为老年代还有很多空间但是，出现了 promotion failed）

解决方法：
       两条和上面 concurrent mode failure 一样
       另一条，是因为 Survivor Unused 不足，那么可以尝试调大 Survivor 来尝试下 
三. 在 GC 的时候其他系统活动影响
有些时候系统活动诸如内存换入换出（vmstat）、网络活动（netstat）、I/O （iostat）在 GC 过程中发生会使 GC 时间变长。

前提是你的服务器上是有 SWAP 区域（用 top、 vmstat 等命令可以看出）用于内存的换入换出，那么操作系统可能会将 JVM 中不活跃的内存页换到 SWAP 区域用以释放内存给线程使用（这也透露出内存开始不够用了）。内存换入换出是一个开销巨大的磁盘操作，比内存访问慢好几个数量级。

看一段 GC 日志：耗时 29.47 秒 

再看看此时的 vmstat 命令中 si、so 列的数值，如果数值大说明换入换出严重，这是内存不足的表现。

解决方法：减少线程，这样可以降低内存换入换出；增加内存；如果是 JVM 内存设置过大导致线程所用内存不足，则适当调低 -Xmx 和 -Xms。



四. JVM 的 bug
这种原因就根据自己使用的 JDK 版本去查一下，如果是 JVM 的 bug，升级到解决的版本一般就能解决。



五. 总结
       长时间停顿问题的排查及解决首先需要一定的信息和方法论：
详细的 GC 日志
借助 Linux 平台下的 iostat、vmstat、netstat、mpstat 等命令监控系统情况
使用 GCHisto 这个 GC 图形用户界面工具，可以统计出 Minor GC 及 Full GC 频率及时长分布，可参考:http://blog.csdn.net/wenniuwuren/article/details/50760259
查看 GC 日志中是否出现了上述的典型内存异常问题（promotion failed, concurrent mode failure），整体来说把上述两个典型内存异常情况控制在可接受的发生频率即可，对 CMS 碎片问题来说杜绝以上问题似乎不太可能，只能靠 G1 来解决了
是不是 JVM 本身的 bug 导致的
如果程序没问题，参数调了几次还是不能解决，可能说明流量太大，需要加机器把压力分散到更多 JVM 上

     

六. 参考资料
         《Java 性能优化权威指南》
           https://blogs.oracle.com/poonam/entry/troubleshooting_long_gc_pauses
JDK8的Metaspace区域的调优
JDK8用metaSpace区域来代替了以前的永久区，这个区域主要存放被加载的class信息，

业务场景：我手上一个项目每次启动时候都会伴随一次fullgc，排查思路是：

一、查看内存使用率
命令：jstat -gcutil PID ，这里的PID是Java进程ID



可以看到老年代使用率只有1.96%，但是MetaSpace区域使用率是96.13%，初步怀疑是metaSpace区域设置太小。

二、查看gc日志（JVM参数里可以配置打印gc信息）


项目采用的CMS垃圾回收期来回收老年代，CMS垃圾回收期在fullgc的步骤可以分为：
初始标记(STW)->并发标记->并发预清理->重标记(STW)-> 并发清理->重置，对应到截图里是一次完整的fullGC的过程，下面分析日志：

标记1的位置是[ParNew: 218582K->15126K(235968K), 0.0166181 secs]，这句意思是新生代在垃圾回收前占用218582K，回收后占用15126K，总容量235968K，耗时0.0166181，这是一次正常的minor gc
标记2的信息很可疑，218582K->15126K(1022400K)，这句意思是整个堆在gc前占用218582K，回收后15126K，总大小为1022400K，也就是说在使用量20%左右时就出现了fullgc，标记3的地方也说明整个堆的使用量很低。
从gc日志上得到的信息也差不多，和堆没关系。

已知的触发fullgc的条件有：

1．老年代空间不足 – 排除
2．永久带空间不足，也就是jdk8里metaSpace区域
3．CMS在gc时出现promotion failed和concurrent mode failure
promotion failed是指新生代对象晋升到老年代时，老年代空间不足时触发fullgc，这个可以排除
concurrent mode failure 是指CMS在进行老年代垃圾回收时，由于CMS在gc时为了减少STW停顿时间，采用用户线程和垃圾回收线程并行执行的方式，所以此时用户线程还是会产生内存对象，这些对象就是浮动对象，如果此时有浮动对象晋升到老年代，而老年代空间不足时，就会触发concurrent mode failure，导致JVM不得不停止用户线程来进行fullgc，这时候就会STW，而我这边的老年代空间充足，可以排除。

这样基本可以确定是metaSpace问题，于是网上查了下参数，在JVM启动参数里加上-XX:MetaspaceSize=64m -XX:MaxMetaspaceSize=512m，想到最小64M，最大512M，这下总可以了，结果启动项目后还是会产生fullGc…，然后再看了下
MetaspaceSize参数含义：

那么-XX:MetaspaceSize=256m的含义到底是什么呢？其实，这个JVM参数是指Metaspace扩容时触发FullGC的初始化阈值，也是最小的阈值。这里有几个要点需要明确：

如果没有配置-XX:MetaspaceSize，那么触发FGC的阈值是21807104（约20.8m），可以通过jinfo -flag MetaspaceSize pid得到这个值；
如果配置了-XX:MetaspaceSize，那么触发FGC的阈值就是配置的值；
Metaspace由于使用不断扩容到-XX:MetaspaceSize参数指定的量，就会发生FGC；且之后每次Metaspace扩容都可能会发生FGC（至于什么时候会，比较复杂，跟几个参数有关）；
如果Old区配置CMS垃圾回收，那么扩容引起的FGC也会使用CMS算法进行回收；
如果MaxMetaspaceSize设置太小，可能会导致频繁FullGC，甚至OOM；


任何一个JVM参数的默认值可以通过java -XX:+PrintFlagsFinal -version |grep JVMParamName获取，例如：java -XX:+PrintFlagsFinal -version |grep MetaspaceSize

最后将MetaspaceSize参数调大就解决问题了。


JVM总结（三）Minor GC、Major GC和Full GC
一、Minor GC
Minor GC是指从年轻代空间（包括 Eden 和 Survivor 区域）回收内存。当 JVM 无法为一个新的对象分配空间时会触发
Minor GC，比如当 Eden 区满了。
Eden区满了触发MinorGC，这时会把Eden区存活的对象复制到Survivor区，当对象在Survivor区熬过一定次数的Minor
GC之后，就会晋升到老年代（当然并不是所有的对象都是这样晋升的到老年代的），当老年代满了，就会报OutofMemory异常。
所有的MinorGC都会触发全世界的暂停（stop-the-world），停止应用程序的线程，不过这个过程非常短暂。 执行 Minor GC 操作时，不会影响到永久代。
二、Major GC vs Full GC
在目前的项目中还没有明确的定义，这点需要注意。JVM规范和垃圾收集研究论文都没有提及，但是乍一看，这些建立在我们掌握了Minor GC清理新生代上的定义并非难事：

Major GC清理Tenured区(老年代)。
Full GC清理整个heap区，包括Yong区和Tenured区。
Full GC触发条件
（1）调用System.gc时，系统建议执行Full GC，但是不必然执行
（2）老年代空间不足
（3）方法去空间不足
（4）通过Minor GC后进入老年代的平均大小 > 老年代的可用内存
（5）由Eden区、From Space区向To Space区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代，且老年代的可用内存小于该对象大小。即老年代无法存放下新年代过度到老年代的对象的时候，会触发Full GC。

补充
以上的GC总结，只是在非并发GC的触发条件下的大致原理。真正的GC情况跟实际GC器的回收机制有关。不同的GC器对Major GC 和 Full GC 的机制还是有区别的。如JVM中Serial GC, Parallel GC, CMS, G1 GC。会在后续的总结中去总结。
集合
HashMap为什么比ArrayList速度快？
源码：

我们发现它首先获得的是这个key的hash值，然后在一个table里去遍历，找到符合条件的entry，这里的这个table是什么呢，看下面这张图就清楚了。

hashmap实际上维护了这样的一个数组加链表的结构，每一个链表就是我们上面看到的那个table，对我们的每个需要entry，根据key的hash值传到相应的地方，这样在get时，我们只需去对于的那个table去找而不是遍历所有的entry，这样在大数据时效率的提高是很明显的。
List与set区别？哪个效率高？为什么？
List和Set是用来存放集合的接口，并且二者都继承自接口Collection。

1、 在List中的元素存放是有序的，可以存放重复的元素，检索效率较高，插入删除效率较低。
ArrayList、LinkedList、Vector是List的两个实现类。
ArrayList（可变数组）：
底层的实现就是一个可变数组非同步实现,当数组长度不够用的时候就会重新开辟一个新的数组，然后将原来的数据拷贝到新的数组内。由于这一底层实现，所以ArrayList集合中元素存储的位置是连续的（为什么查询效率高），查询起来效率比较高，插入删除效率较低。
LinkedList（双向循环链表）：

底层实现是双向循环链表数据结构非同步实现，数据结构如下代码

LinkList中元素存储位置是不连续的，插入删除的执行效率高，查询效率低。
Vector：

Vector作为List的另外一个典型实现类，完全支持List的全部功能，Vector类也封装了一个动态的，允许在分配的Object[]数组，Vector是一个比较古老的集合，JDK1.0就已经存在，建议尽量不要使用这个集合，Vector与ArrayList的主要是区别是，Vector是线程安全的，但是性能比ArrayList要低。

2、 set无序、不可重复。检索效率较低，插入删除效率较高，由于set集合储存位置是由他的HashCode码决定的，所以他的存储对象必须有equals()方法，而且set遍历只能用迭代，没有下标。

HashSet（底层是一个HashMap）：

底层由哈希表（实际上是一个HashMap实例）支持，不能保证元素的顺序，元素是无序的，可以有null，但是null只能有一个，不能有重复的元素。HashSet不是同步的，需要外部保持线程之间的同步问题。
hashSet数据结构原理：

从源码中发现HashSet源码内部维护一个HashMap变量，来看看add方法：

add添加的元素存放在HashMap中，其他方法结合源码分析，参考HashMap
HashSet为什么不能存放相同元素：
在HashMap的put API中，在存入一个元素时，会调用其hashcode方法计算hashcode值，然后在计算出存放entry数组的index，当index相同时调用equals方法判断是否是同一个元素，是则不能存放。



特性
1.HashSet中不能有相同的元素，可以有一个Null元素，存入的元素是无序的。
2.HashSet如何保证唯一性？
1).HashSet底层数据结构是哈希表，哈希表就是存储唯一系列的表，而哈希值是由对象的hashCode()方法生成。
2).确保唯一性的两个方法：hashCode()和equals()方法。
3.添加、删除操作时间复杂度都是O(1)。
4.非线程安全

LinkedHashSet（底层使用 LinkedHashMap ）
对于 LinkedHashSet 而言，它继承与 HashSet、又基于 LinkedHashMap 来实现的。

LinkedHashSet 底层使用 LinkedHashMap 来保存所有元素，它继承与 HashSet，其所有的方法操作上又与 HashSet 相同，因此 LinkedHashSet 的实现上非常简单，只提供了四个构造方法，并通过传递一个标识参数，调用父类的构造器，底层构造一个 LinkedHashMap 来实现，在相关操作上与父类 HashSet 的操作相同，直接调用父类 HashSet 的方法即可。
特点：

1.LinkedHashSet中不能有相同元素，可以有一个Null元素，元素严格按照放入的顺序排列。
2.LinkedHashSet如何保证有序和唯一性？
1).底层数据结构由哈希表和链表组成。
2).链表保证了元素的有序即存储和取出一致，哈希表保证了元素的唯一性。
3.添加、删除操作时间复杂度都是O(1)。
4.非线程安全
TreeSet（红黑树数据结构）：

TreeSet实现了SortedSet接口，它是一个有序的集合类，TreeSet的底层是通过TreeMap实现的。TreeSet并不是根据插入的顺序来排序，而是根据实际的值的大小来排序。TreeSet也支持两种排序方式：自然排序和自定义排序。不能放入重复元素和null。

特点：

1.TreeSet是中不能有相同元素，不可以有Null元素，根据元素的自然顺序进行排序。

2.TreeSet如何保证元素的排序和唯一性？

底层的数据结构是红黑树(一种自平衡二叉查找树)

3.添加、删除操作时间复杂度都是O(log(n))

4.非线程安全
HashSet、LinkedHashSet和TreeSet三者区别与联系
一.HashSet
特点：

1.HashSet中不能有相同的元素，可以有一个Null元素，存入的元素是无序的。

2.HashSet如何保证唯一性？

1).HashSet底层数据结构是哈希表，哈希表就是存储唯一系列的表，而哈希值是由对象的hashCode()方法生成。

2).确保唯一性的两个方法：hashCode()和equals()方法。

3.添加、删除操作时间复杂度都是O(1)。

4.非线程安全

二.LinkedHashSet
特点：

1.LinkedHashSet中不能有相同元素，可以有一个Null元素，元素严格按照放入的顺序排列。

2.LinkedHashSet如何保证有序和唯一性？

1).底层数据结构由哈希表和链表组成。

2).链表保证了元素的有序即存储和取出一致，哈希表保证了元素的唯一性。

3.添加、删除操作时间复杂度都是O(1)。

4.非线程安全

三.TreeSet
特点：

1.TreeSet是中不能有相同元素，不可以有Null元素，根据元素的自然顺序进行排序。

2.TreeSet如何保证元素的排序和唯一性？

底层的数据结构是红黑树(一种自平衡二叉查找树)

3.添加、删除操作时间复杂度都是O(log(n))

4.非线程安全

四.总结：
通过以上特点可以分析出，三者都保证了元素的唯一性，如果无排序要求可以选用HashSet；如果想取出元素的顺序和放入元素的顺序相同，那么可以选用LinkedHashSet。如果想插入、删除立即排序或者按照一定规则排序可以选用TreeSet。
比较元组（Set）和列表（List）的查询效率

从上面的查询时间比较后可以看出：
1、含有10000000个元素的顺序列表，查询第9999999个元素所用时间约为0.203秒；
2、含有10000000个元素的顺序元组，查询第9999999个元素所用时间约为0.0秒；

简易粗略结论：在同等条件下，元组的查询效率会优于列表。

list与set的查询效率，大量数据查询匹配，必须选set。

为什么？
set底层是hashmap，数据结构是数组+链表。 根据key的散列值(hashcode())映射到散列数组的下标再根据key的equals方法逐一比较key，最后找到value。
List是遍历

Map与List查找性能比较

//散列表性能更高 

1，查找

     根据key的散列值(hashcode())映射到散列数组的下标再根据key的equals方法逐一比较key，最后找到value。

2，添加/替换:

    根据key的散列值(hashCode())映射到散列数组的下标，再根据key的equals方法逐一比较key，最后找到插入(替换)位置，插入(替换)key-value数据。

记住:无论查找还是添加都是先用key的hashCode再使用key的equals方法

注意:使用散列表时候，作为key的对象，必须重写equals和hashCode。


List和Set都源自Collection，而Map自成体系;
2.HashMap,ArrayList与HashTable,Vector的区别联系
    HashMap,ArrayList是异步执行的这样有助于提高工作效率，但并不是线程安全的.并且HashMap允许键值对的值为null;
    HashTable和Vector是同步执行的，但是线程安全,这样的效率不如HashMap和ArrayList
3.List集合性能比较:
    ArrayList是首选，在多用于查询使用时,ArrayList的效率更高;但在频繁进行删除，插入操作的时候应该用LinkedList，比如在进行堆栈和队列的操作时，就应该用LinkedList执行，此时的执行效率比JAVA自带的stack要高
4.Map集合性能比较:
    HashMap是首选，但是在整个Map类会需要更多的内存空间，以为有键值对的存在.HashMap用到了哈希函数，所以其key必须是唯一的，TreeMap是用红黑树进行实现的;Map还可用containKey()检查是否含量有某个key/value键值对

Hashtable为什么是线程安全的，而HashMap是线程不安全的？
HashMap是线程不安全的，多线程情况下不推荐使用HashMap。它的key,value运行为null
Hashtable在jdk1.1就有了，那么它是怎样实现线程安全的呢？主要看put、remove、get方法猜它肯定进行的同步控制的。于是看源码

每个操作数据的方法都进行同步控制之后，由此带来的问题任何一个时刻只能有一个线程可以操纵Hashtable，所以其效率比较低

ArrayList为什么是线程不安全的，Vector是线程安全的？？
Vector肯定是做了同步控制了的。看源码

ConcurrentHashMap
为什么ConcurrentHashMap可以多线程访问呢？是因为ConcurrentHashMap将Map分段了，每个段进行加锁，而不是想Hashtable,SynchronizedMap是整个map加锁，这样就可以多线程访问了。

ConcurrentHashMap默认运行16个线程同时访问该map。但是我们可以通过一个函数来设置增加或减少最大可运行访问的线程数目。

所以就有了下面这几个问题：
（1） 多线程可以同时写一个ConcurrentHashMap的分段吗？

不行。分段就像一个单独的HashMap，只允许一个线程向其写入数据

（2）多个线程可以同时写入不同分段吗？

这当然可以咯！

（3）多个线程可以同时从一个分段中读数据吗？

可以

（4）如果一个线程正在向一个分段写入数据，其他线程可以从该分段中读取数据吗？

可以。但是读取到最后更新的数据。

最后需要注意的一点是CoucrrentHashMap是不允许key和vlaue为null的。

实现 Java list，要求实现 list 的 get(), add(), remove() 三个功能函数，不能直接使用 ArrayList、LinkedList 等 Java 自带高级类（阿里面试题）
package com.example.demo;

/**
 * 1.ArrayList的简单实现（手写）
 * 2.包括以下方法：
 *              int size();
 *              MyArrayList();
 *              MyArrayList(int initialCapacity);
 *              boolean isEmpty();
 *              Object get(int index);
 *              boolean add(Object obj);
 *              void add(int index,Object obj)
 *              Object remove(int index)
 *              boolean remove(Object obj)
 *              Object set(int index,Object obj)
 *              void rangeCheck(int index)
 *              void ensureCapacity()
 */


import javax.swing.plaf.synth.SynthSpinnerUI;

public class MyArrayList {

  private Object[] elementData;       //底层数组
  private int size;                   //数组大小

  public int size(){
    /*
     * 返回数组大小
     */
    return size;
  }

  public MyArrayList(){
    /*
     * 无参构造器，通过显式调用含参构造器
     */
    this(10);
  }

  public MyArrayList(int initialCapacity){
    /*
     * 1.含参构造器
     * 2.要对传入的初始量的合法性进行检测
     * 3.通过新建数组实现
     */
    if(initialCapacity<0){
      try {
        throw new Exception();
      } catch (Exception e) {
        // TODO Auto-generated catch block
        e.printStackTrace();
      }
    }
    elementData=new Object[initialCapacity];
  }

  public boolean isEmpty(){
    /*
     * 判断是否为空
     */
    return size==0;
  }

  public Object get(int index){
    /*
     * 1.获取指定下标的对象
     * 2.下标合法性检测
     */
    rangeCheck(index);
    return elementData[index];
  }

  public boolean add(Object obj){
    /*
     * 添加对象（不指定位置）
     * 注意数组扩容
     */
    ensureCapacity();
    elementData[size]=obj;
    size++;
    return true;
  }

  public void add(int index,Object obj){
    /*
     * 插入操作（指定位置）
     * 1.下标合法性检查
     * 2.数组容量检查、扩容
     * 3.数组复制（原数组，开始下标，目的数组，开始下标，长度）
     */
    rangeCheck(index);
    ensureCapacity();
    System.arraycopy(elementData, index, elementData, index+1,size-index);
    elementData[index]=obj;
    size++;
  }
  public Object remove(int index){
    /*
     * 1.删除指定下标对象，并返回其值
     * 2.下标合法性检测
     * 3.通过数组复制实现
     * 4.因为前移，数组最后一位要置为空
     */
    rangeCheck(index);
    int arrnums=size-index-1;
    Object oldValue=elementData[index];
    if(arrnums>0){
      System.arraycopy(elementData, index+1, elementData,index, arrnums);
    }
    elementData[--size]=null;
    return oldValue;
  }

  public boolean remove(Object obj){
    /*
     * 1.删除指定对象
     * 2.通过遍历
     * 3.equals的底层运用，找到下标，调用remove(int i)
     */
    for(int i=0;i<size;i++){
      if(get(i).equals(obj)){         //注意底层用的是equals不是“==”
        remove(i);
      }
      break;
    }
    return true;
  }

  public Object set(int index,Object obj){
    /*
     * 1.将指定下标的对象改变
     * 2.下标合法性检查
     * 3.直接通过数组的赋值来实现改变
     * 4.返回旧值
     */
    rangeCheck(index);
    Object oldValue=elementData[index];
    elementData[index]=obj;
    return oldValue;
  }

  private void rangeCheck(int index){
    /*
     * 对下标的检查
     */
    if(index<0||index>=size){
      try {
        throw new Exception();
      } catch (Exception e) {
        // TODO Auto-generated catch block
        e.printStackTrace();
      }
    }
  }

  private void ensureCapacity(){
    /*
     * 1.对容器容量的检查
     * 2.数组扩容，通过数组复制来实现（量和值两者都要保障）
     */
    if(size==elementData.length){
      Object[] newArray=new Object[size*2+1];
      System.arraycopy(elementData, 0, newArray, 0, elementData.length);
      elementData=newArray;
    }
  }
  public static void main(String[] args) {
    MyArrayList mylist=new MyArrayList();
    mylist.add("123");
    mylist.add("呵呵呵");
    mylist.add("哦哦");
    mylist.add("哈哈哈");
    mylist.add(1,"你好");
    String old=(String)mylist.remove(3);    //返回的是旧值
    System.out.println(old);
    System.out.println(mylist.remove("哦哦"));
    System.out.println(mylist.isEmpty());
    System.out.println(mylist.get(1));
    System.out.println(mylist.size());
    System.out.println(mylist.set(2, "你好啊"));   //返回旧值
    System.out.println(mylist.get(2));
  }
}

数据结构与算法精选面试题[添加链接描述](https://www.cnblogs.com/developer_chan/p/11439711.html)
HashMap源码解读
1. 数据结构
在Java中，是通过数组和链表这俩种数据结构来进行数据存储的。

数组：
数组的存储空间的连续的，所以占内存严重
二分查找复杂度小
寻址容易，插入删除难
链表：
区间离散，占用内存比较宽松
空间复杂度很小，时间复杂度很大
寻址难，插入删除容易
哈希表(Hash Table)就是这两者的结合，插入删除简单，寻址容易，占用内存相对宽松。

拉链法(链地址法)：链表的数组


拉链法
最常用的哈希表排列法，一目了然的数组+链表组合。
存储规则：整个数组长度为16，每个元素存储的是一个链表的头结点。比如：1%16 = 1，337%16 = 1，353%16 = 1 。余数皆为1，所以这三个元素被存储在数组下标为1的位置。

解决哈希冲突：同数组下标下，通过链式结构避免索引值冲突。

而HashMap就是典型的拉链法哈希表结构。

他实际上是一个线性数组。他的静态内部类继承了一个Entry接口。这里注意，在jdk1.8中，在链表中加入了红黑树（平衡二分查找树）。所以1.8版本的HashMap是由数组+（链表+红黑树）实现的。

public class HashMap<K,V> extends AbstractMap<K,V> implements Map<K,V>, Cloneable, Serializable {
    ......
    static class Node<K,V> implements Map.Entry<K,V> {
        final int hash;//hash code value for this map entry
        final K key;
        V value;
        Node<K,V> next;

        Node(int hash, K key, V value, Node<K,V> next) {
            this.hash = hash;
            this.key = key;
            this.value = value;
            this.next = next;
        }

        public final K getKey()        { return key; }
        public final V getValue()      { return value; }
        public final String toString() { return key + "=" + value; }

        public final int hashCode() {
            return Objects.hashCode(key) ^ Objects.hashCode(value);
        }

        public final V setValue(V newValue) {
            V oldValue = value;
            value = newValue;
            return oldValue;
        }

        public final boolean equals(Object o) {
            if (o == this)
                return true;
            if (o instanceof Map.Entry) {
                Map.Entry<?,?> e = (Map.Entry<?,?>)o;
                if (Objects.equals(key, e.getKey()) &&
                    Objects.equals(value, e.getValue()))
                    return true;
            }
            return false;
        }
    }
    ......
}
乍一眼就能看出是一个bean类，关键的参数有三个：key，value，next，接下来我们看下HashMap的方法与Entry的关联

get方法
public V get(Object key) {
   Node<K,V> e;
   return (e = getNode(hash(key), key)) == null ? null : e.value;//key是否存在，存在返回key的value值，不存在返回null
  //hash(key)获得key的hash值
}
getNode方法


    /**
     * Implements Map.get and related methods
     *
     * @param hash hash for key
     * @param key the key
     * @return the node, or null if none
     */
    final Node<K,V> getNode(int hash, Object key) {
        Node<K,V>[] tab; //Entry数组
        Node<K,V> first, e; 
        int n; //数组长度
        K k;
        // 1. 定位键值对所在桶的位置
        if ((tab = table) != null && (n = tab.length) > 0 &&
            (first = tab[(n - 1) & hash]) != null) {
            //2.判断键值的hashcode相等，对象相等
            if (first.hash == hash && // always check first node
                ((k = first.key) == key || (key != null && key.equals(k))))
                return first;
            if ((e = first.next) != null) {
                // 3..如果 first 是 TreeNode 类型，则调用黑红树查找方法
                if (first instanceof TreeNode)
                    return ((TreeNode<K,V>)first).getTreeNode(hash, key);
                do {
                    if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                        return e;
                } while ((e = e.next) != null);
            }
        }
        return null;
    }
我们看一下判断条件。前两个是tab非空判断，没什么好说的，看下第三个条件。
(first = tab[(n - 1) & hash]) != null其中

tab[(n - 1) & hash]这个可能有点难理解。这里是为了运算出桶在桶数组中的位置。HashMap 中桶数组的大小 length 总是2的幂，此时，(n - 1) & hash 等价于对 length 取余。
随便假设一对值，比如hash = 98，n = 21 ，98 & (21-1) = 0，是不是和下方图示运算结果一样？位运算效率是高于运算符的，这算是java优化中的小细节。

这里需注意，hashcode相同不代表是同一个对象，只有equals才能判断两个是否是同一对象（键值）
黑红树是jdk1.8在HashMap的性能优化中新添加的特性。它主要有五大性质：
节点是红色或黑色。
根是黑色。
所有叶子都是黑色（叶子是NIL节点）。
每个红色节点必须有两个黑色的子节点。（从每个叶子到根的所有路径上不能有两个连续的红色节点。）
从任一节点到其每个叶子的所有简单路径都包含相同数目的黑色节点（简称黑高）。

红黑树
put方法
    public V put(K key, V value) {
        return putVal(hash(key), key, value, false, true);//1. onlyIfAbsent参数
    }
/**
     * Implements Map.put and related methods
     *
     * @param hash hash for key
     * @param key the key
     * @param value the value to put
     * @param onlyIfAbsent if true, don't change existing value
     * @param evict if false, the table is in creation mode.
     * @return previous value, or null if none
     */
    final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                   boolean evict) {

        Node<K,V>[] tab; Node<K,V> p; int n, i;
        // 初始化桶数组 table
        if ((tab = table) == null || (n = tab.length) == 0)
            //扩容方法
            n = (tab = resize()).length;
        // 当前key不存在，新建键值对加入
        if ((p = tab[i = (n - 1) & hash]) == null)

            tab[i] = newNode(hash, key, value, null);

        else {
            Node<K,V> e; K k;
            // 如果键的值以及节点 hash 等于链表中的第一个键值对节点时，则将 e 指向该键值对
            if (p.hash == hash &&
                ((k = p.key) == key || (key != null && key.equals(k))))
                e = p;
            //如果节点下引用数据结构为红黑树，调用红黑树插入法
            else if (p instanceof TreeNode)
                e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
            else {
                // 链表结构，遍历
                for (int binCount = 0; ; ++binCount) {
                    //不存在当前需要插入的节点  
                    if ((e = p.next) == null) {
                        //新建一个节点插入
                        p.next = newNode(hash, key, value, null);
                        //链表长度超过或等于树化阙值（8），对链表进行树化
                        if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                            treeifyBin(tab, hash);
                        break;
                    }
                    //需要插入的节点已经存在了
                    if (e.hash == hash &&
                        ((k = e.key) == key || (key != null && key.equals(k))))
                        break;
                    p = e;
                }
            }
            if (e != null) { // existing mapping for key
                V oldValue = e.value;
                if (!onlyIfAbsent || oldValue == null)//1.onlyIfAbsent 判断
                    e.value = value;
                afterNodeAccess(e);
                return oldValue;
            }
        }
        ++modCount;
        if (++size > threshold)
            resize();
        afterNodeInsertion(evict);
        return null;
    }
onlyIfAbsent这个参数，牵扯到另外一个put方法：putIfAbsent方法
  public V putIfAbsent(K key, V value) {
        return putVal(hash(key), key, value, true, true);
  }
他和put方法唯一的区别就是onlyIfAbsent的值为true

特点：putIfAbsent在放入数据时，如果存在重复的key，那么putIfAbsent不会放入值。
如果传入key对应的value已经存在，就返回存在的value，不进行替换。如果不存在，就添加key和value，返回null

resize方法
resize(),咱们称之为扩容方法，只有在两种情况下会被调用：

HashMap实行了懒加载: 新建HashMap时不会对table进行赋值, 而是到第一次插入时, 进行resize时构建table;
当HashMap的size值大于 threshold时, 会进行resize(); 看一下threshold在源码中的注解:
// (The javadoc description is true upon serialization.
   // Additionally, if the table array has not been allocated, this
   // field holds the initial array capacity, or zero signifying
   // DEFAULT_INITIAL_CAPACITY.)
   int threshold;

  /**
   * The default initial capacity - MUST be a power of two.
   */
  static final int DEFAULT_INITIAL_CAPACITY = 1 << 4; // aka 16
1 << 4是位运算，结果threshold默认值为16。

resize()方法源码：

   /**
     * Initializes or doubles table size.  If null, allocates in
     * accord with initial capacity target held in field threshold.
     * Otherwise, because we are using power-of-two expansion, the
     * elements from each bin must either stay at same index, or move
     * with a power of two offset in the new table.
     *
     * @return the table
     */

     final Node<K,V>[] resize() {

        Node<K,V>[] oldTab = table;//将当前table暂存到oldtab来操作

        int oldCap = (oldTab == null) ? 0 : oldTab.length;

        int oldThr = threshold;

        int newCap, newThr = 0;

        if (oldCap > 0) {

            if (oldCap >= MAXIMUM_CAPACITY) {//如果老容量大于最大容量

                threshold = Integer.MAX_VALUE;//阈值设置为Integer的最大值，好像是2147483647，远大于默认的最大容量

                return oldTab;//直接返回当前table，不用扩容

            }

            else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&

                     oldCap >= DEFAULT_INITIAL_CAPACITY)

                newThr = oldThr << 1; // 双倍扩大老内存和老阈值并赋给新的table

        }

        else if (oldThr > 0) // initial capacity was placed in threshold

            newCap = oldThr;

        else {               //这种情况是初始化HashMap时啥参数都没加

            newCap = DEFAULT_INITIAL_CAPACITY;

            newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);

        }

        if (newThr == 0) {//当只满足老阈值大于0的条件时，新阈值等于新容量*默认扩容因子

            float ft = (float)newCap * loadFactor;

            newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?

                      (int)ft : Integer.MAX_VALUE);

        }

        threshold = newThr;//把新的阈值赋给当前table

        @SuppressWarnings({"rawtypes","unchecked"})

            Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];//创建容量为newCap的新table

        table = newTab;

        if (oldTab != null) {

            for (int j = 0; j < oldCap; ++j) {//对老table进行遍历

                Node<K,V> e;

                if ((e = oldTab[j]) != null) {//遍历到的赋给e进行暂存，同时将老table对应项赋值为null

                    oldTab[j] = null;

                    if (e.next == null)//将不为空的元素复制到新table中

                        newTab[e.hash & (newCap - 1)] = e;//等于是创建一个新的空table然后重新进行元素的put，这里的table长度是原table的两倍

                    else if (e instanceof TreeNode)//暂时没了解红黑树

                        ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);

                    else { // preserve order

                        Node<K,V> loHead = null, loTail = null;//用于保存put后不移位的链表

                        Node<K,V> hiHead = null, hiTail = null;//用于保存put后移位的链表

                        Node<K,V> next;

                        do {

                            next = e.next;

                            if ((e.hash & oldCap) == 0) {//如果与的结果为0，表示不移位，将桶中的头结点添加到lohead和lotail中，往后如果桶中还有不移位的结点，就向tail继续添加

                                if (loTail == null)//在后面遍历lohead和lotail保存到table中时，lohead用于保存头结点的位置，lotail用于判断是否到了末尾

                                    loHead = e;

                                else

                                    loTail.next = e;

                                loTail = e;

                            }

                            else {//这是添加移位的结点，与不移位的类似

                                if (hiTail == null)

                                    hiHead = e;

                                else

                                    hiTail.next = e;

                                hiTail = e;

                            }

                        } while ((e = next) != null);

                        if (loTail != null) {//把不移位的结点添加到对应的链表数组中去

                            loTail.next = null;

                            newTab[j] = loHead;

                        }

                        if (hiTail != null) {//把移位的结点添加到对应的链表数组中去

                            hiTail.next = null;

                            newTab[j + oldCap] = hiHead;

                        }

                    }

                }

            }

        }
        return newTab;
    }

4.线程安全
HashMap是线程不安全的，但是HashTable由于每次put操作就进行锁死效率十分低下。那有没有既效率又线程安全的HashMap呢？

答案就是：ConcurrentHashMap

底层采用分段的数组+链表实现，线程安全
通过把整个Map分为N个Segment，可以提供相同的线程安全，但是效率提升N倍，默认提升16倍。(读操作不加锁，由于HashEntry的value变量是 volatile的，也能保证读取到最新的值。)
Hashtable的synchronized是针对整张Hash表的，即每次锁住整张表让线程独占，ConcurrentHashMap允许多个修改操作并发进行，其关键在于使用了锁分离技术
有些方法需要跨段，比如size()和containsValue()，它们可能需要锁定整个表而而不仅仅是某个段，这需要按顺序锁定所有段，操作完毕后，又按顺序释放所有段的锁
扩容：段内扩容（段内元素超过该段对应Entry数组长度的75%触发扩容，不会对整个Map进行扩容），插入前检测需不需要扩容，有效避免无效扩容


ArrayList源码解读
简介：
　　ArrayList是我们开发中非常常用的数据存储容器之一，其底层是数组实现的，我们可以在集合中存储任意类型的数据，ArrayList是线程不安全的，非常适合用于对元素进行查找，效率非常高。
线程安全性：
　　对ArrayList的操作一般分为两个步骤，改变位置(size)和操作元素(e)。所以这个过程在多线程的环境下是不能保证具有原子性的，因此ArrayList在多线程的环境下是线程不安全的。
源码分析：
1.属性分析：

    /**
     * Default initial capacity.
     */
    private static final int DEFAULT_CAPACITY = 10;

    /**
     * Shared empty array instance used for empty instances.
     */
    private static final Object[] EMPTY_ELEMENTDATA = {};

    /**
     * The array buffer into which the elements of the ArrayList are stored.
     * The capacity of the ArrayList is the length of this array buffer. Any
     * empty ArrayList with elementData == EMPTY_ELEMENTDATA will be expanded to
     * DEFAULT_CAPACITY when the first element is added.
     */
    private transient Object[] elementData;

    /**
     * The size of the ArrayList (the number of elements it contains).
     *
     * @serial
     */
    private int size;

扩展：什么是序列化
序列化是指：将对象转换成以字节序列的形式来表示，以便用于持久化和传输。
实现方法：实现Serializable接口。
然后用的时候拿出来进行反序列化即可又变成Java对象。
transient关键字解析
Java中transient关键字的作用，简单地说，就是让某些被修饰的成员属性变量不被序列化。
有了transient关键字声明，则这个变量不会参与序列化操作，即使所在类实现了Serializable接口，反序列化后该变量为空值。
那么问题来了：ArrayList中数组声明：transient Object[] elementData;，事实上我们使用ArrayList在网络传输用的很正常，并没有出现空值。
原来：ArrayList在序列化的时候会调用writeObject()方法，将size和element写入ObjectOutputStream；反序列化时调用readObject()，从ObjectInputStream获取size和element，再恢复到elementData。
那为什么不直接用elementData来序列化，而采用上述的方式来实现序列化呢？
原因在于elementData是一个缓存数组，它通常会预留一些容量，等容量不足时再扩充容量，那么有些空间可能就没有实际存储元素，采用上诉的方式来实现序列化时，就可以保证只序列化实际存储的那些元素，而不是整个数组，从而节省空间和时间。
2.构造方法分析
根据initialCapacity 初始化一个空数组，如果值为0，则初始化一个空数组:

    /**
     * Constructs an empty list with the specified initial capacity.
     *
     * @param  initialCapacity  the initial capacity of the list
     * @throws IllegalArgumentException if the specified initial capacity
     *         is negative
     */
    public ArrayList(int initialCapacity) {
        super();
        if (initialCapacity < 0)
            throw new IllegalArgumentException("Illegal Capacity: "+
                                               initialCapacity);
        this.elementData = new Object[initialCapacity];
    }

不带参数初始化，默认容量为10:

    /**
     * Constructs an empty list with an initial capacity of ten.
     */
    public ArrayList() {
        super();
        this.elementData = EMPTY_ELEMENTDATA;
    }

通过集合做参数的形式初始化：如果集合为空，则初始化为空数组：

    /**
     * Constructs a list containing the elements of the specified
     * collection, in the order they are returned by the collection's
     * iterator.
     *
     * @param c the collection whose elements are to be placed into this list
     * @throws NullPointerException if the specified collection is null
     */
    public ArrayList(Collection<? extends E> c) {
        elementData = c.toArray();
        size = elementData.length;
        // c.toArray might (incorrectly) not return Object[] (see 6260652)
        if (elementData.getClass() != Object[].class)
            elementData = Arrays.copyOf(elementData, size, Object[].class);
    }

3.主干方法
trimToSize() 用来最小化实例存储，将容器大小调整为当前元素所占用的容量大小。

    /**
     * Trims the capacity of this <tt>ArrayList</tt> instance to be the
     * list's current size.  An application can use this operation to minimize
     * the storage of an <tt>ArrayList</tt> instance.
     */
    public void trimToSize() {
        modCount++;
        if (size < elementData.length) {
            elementData = Arrays.copyOf(elementData, size);
        }
    }

clone()方法：用来克隆出一个新数组。

    /**
     * Returns a shallow copy of this <tt>ArrayList</tt> instance.  (The
     * elements themselves are not copied.)
     *
     * @return a clone of this <tt>ArrayList</tt> instance
     */
    public Object clone() {
        try {
            @SuppressWarnings("unchecked")
                ArrayList<E> v = (ArrayList<E>) super.clone();
            v.elementData = Arrays.copyOf(elementData, size);
            v.modCount = 0;
            return v;
        } catch (CloneNotSupportedException e) {
            // this shouldn't happen, since we are Cloneable
            throw new InternalError();
        }
    }

通过调用Object的clone()方法来得到一个新的ArrayList对象，然后将elementData复制给该对象并返回。
add(E e)方法：在数组末尾添加元素

    /**
     * Appends the specified element to the end of this list.
     *
     * @param e element to be appended to this list
     * @return <tt>true</tt> (as specified by {@link Collection#add})
     */
    public boolean add(E e) {
        ensureCapacityInternal(size + 1);  // Increments modCount!!
        elementData[size++] = e;
        return true;
    }

 
看到它首先调用了ensureCapacityInternal()方法.注意参数是size+1,这是个面试考点

    private void ensureCapacityInternal(int minCapacity) {
        if (elementData == EMPTY_ELEMENTDATA) {
            minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity);
        }

        ensureExplicitCapacity(minCapacity);
    }

这个方法里又嵌套调用了两个方法:计算容量+确保容量
计算容量：如果elementData是空，则返回默认容量10和size+1的最大值，否则返回size+1
计算完容量后，进行确保容量可用：(modCount不用理它，它用来计算修改次数)
如果size+1 > elementData.length证明数组已经放满，则增加容量，调用grow()。
    private void ensureExplicitCapacity(int minCapacity) {
        modCount++;
        // overflow-conscious code
        if (minCapacity - elementData.length > 0)
            grow(minCapacity);
    }
增加容量：默认1.5倍扩容。
获取当前数组长度=>oldCapacity
oldCapacity>>1 表示将oldCapacity右移一位(位运算)，相当于除2。再加上1，相当于新容量扩容1.5倍。
如果newCapacity&gt;1=1,1&lt;2所以如果不处理该情况，扩容将不能正确完成。
如果新容量比最大值还要大，则将新容量赋值为VM要求最大值。
将elementData拷贝到一个新的容量中。

    /**
     * Increases the capacity to ensure that it can hold at least the
     * number of elements specified by the minimum capacity argument.
     *
     * @param minCapacity the desired minimum capacity
     */
    private void grow(int minCapacity) {
        // overflow-conscious code
        int oldCapacity = elementData.length;
        int newCapacity = oldCapacity + (oldCapacity >> 1);
        if (newCapacity - minCapacity < 0)
            newCapacity = minCapacity;
        if (newCapacity - MAX_ARRAY_SIZE > 0)
            newCapacity = hugeCapacity(minCapacity);
        // minCapacity is usually close to size, so this is a win:
        elementData = Arrays.copyOf(elementData, newCapacity);
    }

size+1的问题
好了，那到这里可以说一下为什么要size+1。
size+1代表的含义是：
如果集合添加元素成功后，集合中的实际元素个数。
为了确保扩容不会出现错误。
假如不加一处理，如果默认size是0，则0+0>>1还是0。
如果size是1，则1+1>>1还是1。有人问:不是默认容量大小是10吗?事实上，jdk1.8版本以后，ArrayList的扩容放在add()方法中。之前放在构造方法中。我用的是1.8版本，所以默认ArrayList arrayList = new ArrayList();后，size应该是0.所以,size+1对扩容来讲很必要.
add(int index, E element)方法

    /**
     * Inserts the specified element at the specified position in this
     * list. Shifts the element currently at that position (if any) and
     * any subsequent elements to the right (adds one to their indices).
     *
     * @param index index at which the specified element is to be inserted
     * @param element element to be inserted
     * @throws IndexOutOfBoundsException {@inheritDoc}
     */
    public void add(int index, E element) {
        rangeCheckForAdd(index);

        ensureCapacityInternal(size + 1);  // Increments modCount!!
        System.arraycopy(elementData, index, elementData, index + 1,
                         size - index);
        elementData[index] = element;
        size++;
    }

rangeCheckForAdd()是越界异常检测方法。

    /**
     * A version of rangeCheck used by add and addAll.
     */
    private void rangeCheckForAdd(int index) {
        if (index > size || index < 0)
            throw new IndexOutOfBoundsException(outOfBoundsMsg(index));
    }

 
ensureCapacityInternal()之前有讲，着重说一下System.arrayCopy方法：
    public static native void arraycopy(Object src,  int  srcPos,
                                        Object dest, int destPos,
                                        int length);
代码解释:
Object src : 原数组
int srcPos : 从元数据的起始位置开始
Object dest : 目标数组
int destPos : 目标数组的开始起始位置
int length : 要copy的数组的长度
示例：size为6，我们调用add(2,element)方法，则会从index=2+1=3的位置开始，将数组元素替换为从index起始位置为index=2，长度为6-2=4的数据。

set(int index,E element)方法：逻辑很简单，覆盖旧值并返回。

        public E set(int index, E e) {
            rangeCheck(index);
            checkForComodification();
            E oldValue = ArrayList.this.elementData(offset + index);
            ArrayList.this.elementData[offset + index] = e;
            return oldValue;
        }

indexOf(Object o)方法：根据Object对象获取数组中的索引值。

    public int indexOf(Object o) {
        if (o == null) {
            for (int i = 0; i < size; i++)
                if (elementData[i]==null)
                    return i;
        } else {
            for (int i = 0; i < size; i++)
                if (o.equals(elementData[i]))
                    return i;
        }
        return -1;
    }

如果o为空，则返回数组中第一个为空的索引；不为空也类似。
注意：通过源码可以看到，该方法是允许传空值进来的。
get(int index)方法：返回指定下标处的元素的值。
    public E get(int index) {
        rangeCheck(index);

        return elementData(index);
    }
rangeCheck(index)会检测index值是否合法，如果合法则返回索引对应的值。
remove(int index)方法：删除指定下标的元素。

    public E remove(int index) {
        rangeCheck(index);

        modCount++;
        E oldValue = elementData(index);

        int numMoved = size - index - 1;
        if (numMoved > 0)
            System.arraycopy(elementData, index+1, elementData, index,
                             numMoved);
        elementData[--size] = null; // clear to let GC do its work

        return oldValue;
    }

这里又碰到了System.arraycopy()方法，详情请查阅上文。
大概思路：将该元素后面的元素前移，最后一个元素置空。
 
ArrayList优缺点
优点：
因为其底层是数组，所以修改和查询效率高。
可自动扩容(1.5倍)。
缺点：
插入和删除效率不高。
线程不安全。
 
手写简易ArrayList:

package basic;
public class MyArrayList {

    // 非私有，以简化嵌套类访问
    // transient 在已经实现序列化的类中，不允许某变量序列化
    transient Object[] elementData;

    // 默认容量
    private static final int DEFAULT_CAPACITY = 10;

    // 用于空实例的 空数组实例
    private static final Object[] EMPTY_ELEMENTDATA = {};

    private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {};
    // 实际ArrayList集合大小
    private int size;

    /**
     * 构造方法
     */
    public MyArrayList(int initialCapacity) {
        if (initialCapacity > 0) {
            this.elementData = new Object[initialCapacity];
        } else if (initialCapacity == 0) {
            this.elementData = EMPTY_ELEMENTDATA;
        } else {
            throw new IllegalArgumentException("Illegal Capacity: " + initialCapacity);
        }
    }

    public MyArrayList() {
        this(DEFAULT_CAPACITY);
    }

    public void add(Object o) {
        // 1. 判断数据容量是否大于 elementData
        ensureExplicitCapacity(size + 1);
        // 2. 使用下标进行赋值
        elementData[size++] = o;
    }

    private void ensureExplicitCapacity(int minCapacity) {
        if (size == elementData.length) {
            // 需要扩容,扩容1.5倍(ArrayList默认扩容1.5倍)
            // 注意：如果oldCapacity值为1
            int oldCapacity = elementData.length;
            int newCapacity = oldCapacity + (oldCapacity >> 1);
            // 如果新容量 < 最小容量， 则将最小容量赋值给新容量
            // 如果 oldCapacity=1, 则 minCapacity=1+1=2 newCapacity=1+(1>>1)=1
            if (newCapacity - minCapacity < 0) {
                newCapacity = minCapacity;
            }
            // 创建新数组
            Object[] objects = new Object[newCapacity];
            // 将数据复制给新数组
            System.arraycopy(elementData, 0, objects, 0, elementData.length);
            // 修改引用
            elementData = objects;
        }
    }

    public Object get(int index) {
        rangeCheck(index);
        return elementData[index];
    }

    private void rangeCheck(int index) {
        if (index >= size)
            throw new IndexOutOfBoundsException("下标越界");
    }

    /**
     * 通过下标删除
     * @param index
     * @return
     */
    public Object remove(int index) {
        rangeCheck(index);

         //modCount++;
        // 先查出元素
        Object oldValue = elementData[index];
        // 找出置换结束位置
        int numMoved = size - index - 1;
        if (numMoved > 0)
            // 从 index+1 开始 将值覆盖为 index-numMoved 的值
            System.arraycopy(elementData, index + 1, elementData, index, numMoved);
        elementData[--size] = null; // clear to let GC do its work

        return oldValue;
    }

    public boolean remove(Object o) {
        for (int index = 0; index < size; index++) {
            if (o.equals(elementData[index])) {
                remove(index);
                return true;
            }
        }
        return false;
    }
}

 

LinkList源码解读
前言：LinkedList的底层数据结构是双向链表，下面具体分析其实现原理。
注：本文jdk源码版本为jdk1.8.0_172

1..LinkedList介绍
LinkedList继承于AbstractSequentialList的双向链表，实现List接口，因此也可以对其进行队列操作，它也实现了Deque接口，所以LinkedList也可当做双端队列使用，还有LinkedList是非同步的。

由于LinkedList的底层是双向链表，因此其顺序访问的效率非常高，而随机访问的效率就比较低了，因为通过索引去访问的时候，首先会比较索引值和链表长度的1/2，若前者大，则从链表尾开始寻找，否则从链表头开始寻找，这样就把双向链表与索引值联系起来了。
2.具体源码分析
LinkedList底层数据结构：

分析：Node为LinkedList的底层数据结构，关联了前驱节点，后续节点和值。
构造函数，LinkedList提供了两个构造函数：
add函数，添加元素时，是直接添加在链表的结尾：
分析：
从源码上可以非常清楚的了解LinkedList加入元素是直接放在链表尾的，主要点构成双向链表，整体逻辑并不复杂，通过上述注释理解应该不成问题。
add(int,element)，在具体index上插入元素：

分析：该函数并不是直接插入链表尾，需要进行一个判断，逻辑并不复杂，通过注释应该不难理解，但这里要注意一个函数node(index)，取出对应index上的Node元素，下面来具体分析一下。
接下来看构造函数中的addAll方法：
public boolean addAll(int index, Collection<? extends E> c) {
        // 检查index是否越界
        checkPositionIndex(index);

        Object[] a = c.toArray();
        int numNew = a.length;
        // 如果插入集合无数据，则直接返回
        if (numNew == 0)
            return false;

        // succ的前驱节点
        Node<E> pred, succ;
        // 如果index与size相同
        if (index == size) {
            // succ的前驱节点直接赋值为最后节点
            // succ赋值为null，因为index在链表最后
            succ = null;
            pred = last;
        } else {
            // 取出index上的节点
            succ = node(index);
            pred = succ.prev;
        }
        // 遍历插入集合
        for (Object o : a) {
            @SuppressWarnings("unchecked") E e = (E) o;
            // 创建新节点 前驱节点为succ的前驱节点，后续节点为null
            Node<E> newNode = new Node<>(pred, e, null);
            // succ的前驱节点为空，则表示succ为头，则重新赋值第一个结点
            if (pred == null)
                first = newNode;
            else
                // 构建双向链表
                pred.next = newNode;
            // 将前驱节点移动到新节点上，继续循环
            pred = newNode;
        }

        // index位置上为空 赋值last节点为pred，因为通过上述的循环pred已经走到最后了
        if (succ == null) {
            last = pred;
        } else {
            // 构建双向链表
            // 从这里可以看出插入集合是在succ[index位置上的节点]之前
            pred.next = succ;
            succ.prev = pred;
        }
        // 元素总数更新
        size += numNew;
        // 修改次数自增
        modCount++;
        return true;
    }
分析：逻辑并不复杂，注意一点即可，插入集合的元素是在index元素之前。
其他重要的源码分析：
// 通过index获取元素
public E get(int index) {
    // 检查index是否越界
    checkElementIndex(index);
    // 通过node函数返回节点值 node函数前面已经分析过
    return node(index).item;
}

// 增加元素在链表头位置
private void linkFirst(E e) {
    final Node<E> f = first;
    // 创建新节点 前驱节点为null，后续节点为first节点
    final Node<E> newNode = new Node<>(null, e, f);
    // 更新first节点
    first = newNode;
    // 如果f为空，表示原来为空，更新last节点为新节点
    if (f == null)
        last = newNode;
    else
        // 构建双向链表
        f.prev = newNode;
    // 元素总数自增
    size++;
    // 修改次数自增
    modCount++;
}
    
 // 释放头节点
private E unlinkFirst(Node<E> f) {
    // assert f == first && f != null;
    final E element = f.item;
    final Node<E> next = f.next;
    f.item = null;
    f.next = null; // help GC
    // 更新头节点
    first = next;
    if (next == null)
        last = null;
    else
        // 将头节点的前驱节点赋值为null
        next.prev = null;
    // 元素总数自减
    size--;
    // 修改次数自增
    modCount++;
    // 返回删除的节点数据
    return element;
}
 // 释放尾节点
private E unlinkLast(Node<E> l) {
    // assert l == last && l != null;
    final E element = l.item;
    // 和释放头节点相反，这里取出前驱节点，其他逻辑一样
    final Node<E> prev = l.prev;
    l.item = null;
    l.prev = null; // help GC
    last = prev;
    if (prev == null)
        first = null;
    else
        prev.next = null;
    size--;
    modCount++;
    return element;
}
3.总结
整体分析下来，其实LinkedList还是比较简单的，上面对一些重要的相关源码进行了分析，主要重点如下：
#1.LinkedList底层数据结构为双向链表，非同步。
#2.LinkedList允许null值。
#3.由于双向链表，顺序访问效率高，而随机访问效率较低。
#4.注意源码中的相关操作，主要是构建双向链表。

Hash是什么？HashCode是什么？Hash表是什么？为什么要用HashCode？
Hash
百度百科解释：
Hash 又叫 散列函数，是把任意长度的输入（又叫做预映射pre-image）通过散列算法变换成固定长度的输出，该输出就是散列值。



常用HASH函数
散列函数能使对一个数据序列的访问过程更加迅速有效，通过散列函数，数据元素将被更快地定位。常用Hash函数有：
1．直接寻址法。取关键字或关键字的某个线性函数值为散列地址。即H(key)=key或H(key) = a·key + b，其中a和b为常数（这种散列函数叫做自身函数）
2． 数字分析法。分析一组数据，比如一组员工的出生年月日，这时我们发现出生年月日的前几位数字大体相同，这样的话，出现冲突的几率就会很大，但是我们发现年月日的后几位表示月份和具体日期的数字差别很大，如果用后面的数字来构成散列地址，则冲突的几率会明显降低。因此数字分析法就是找出数字的规律，尽可能利用这些数据来构造冲突几率较低的散列地址。
3． 平方取中法。取关键字平方后的中间几位作为散列地址。
4． 折叠法。将关键字分割成位数相同的几部分，最后一部分位数可以不同，然后取这几部分的叠加和（去除进位）作为散列地址。
5． 随机数法。选择一随机函数，取关键字作为随机函数的种子生成随机值作为散列地址，通常用于关键字长度不同的场合。
6． 除留余数法。取关键字被某个不大于散列表表长m的数p除后所得的余数为散列地址。即 H(key) = key MOD p,p<=m。不仅可以对关键字直接取模，也可在折叠、平方取中等运算之后取模。对p的选择很重要，一般取素数或m，若p选的不好，容易产生碰撞。

处理冲突方法
1．开放寻址法；Hi=(H(key) + di) MOD m,i=1,2,…，k(k<=m-1)，其中H(key)为散列函数，m为散列表长，di为增量序列，可有下列三种取法：
1)． di=1,2,3,…，m-1，称线性探测再散列；
2)． di=12,-12,22,-22,32,…，±k2,(k<=m/2)称二次探测再散列；
3)． di=伪随机数序列，称伪随机探测再散列。
2． 再散列法：Hi=RHi(key),i=1,2,…，k RHi均是不同的散列函数，即在同义词产生地址冲突时计算另一个散列函数地址，直到冲突不再发生，这种方法不易产生“聚集”，但增加了计算时间。
3． 链地址法(拉链法)
4． 建立一个公共溢出区

查找性能分析
散列表的查找过程基本上和造表过程相同。一些关键码可通过散列函数转换的地址直接找到，另一些关键码在散列函数得到的地址上产生了冲突，需要按处理冲突的方法进行查找。在介绍的三种处理冲突的方法中，产生冲突后的查找仍然是给定值与关键码进行比较的过程。所以，对散列表查找效率的量度，依然用平均查找长度来衡量。
查找过程中，关键码的比较次数，取决于产生冲突的多少，产生的冲突少，查找效率就高，产生的冲突多，查找效率就低。因此，影响产生冲突多少的因素，也就是影响查找效率的因素。影响产生冲突多少有以下三个因素：
1．散列函数是否均匀；
2. 处理冲突的方法；
3．散列表的装填因子。
散列表的装填因子定义为：α= 填入表中的元素个数/散列表的长度
α是散列表装满程度的标志因子。由于表长是定值，α与“填入表中的元素个数”成正比，所以，α越大，填入表中的元素较多，产生冲突的可能性就越大；α越小，填入表中的元素较少，产生冲突的可能性就越小。
实际上，散列表的平均查找长度是装填因子α的函数，只是不同处理冲突的方法有不同的函数。
了解了hash基本定义，就不能不提到一些著名的hash算法，MD5和SHA-1可以说是应用最广泛的Hash算法，而它们都是以MD4为基础设计的。

常用hash算法的介绍：
（1)MD4
MD4(RFC 1320)是 MIT 的Ronald L. Rivest在 1990 年设计的，MD 是 Message Digest（消息摘要） 的缩写。它适用在32位字长的处理器上用高速软件实现——它是基于 32位操作数的位操作来实现的。
（2)MD5
MD5(RFC 1321)是 Rivest 于1991年对MD4的改进版本。它对输入仍以512位分组，其输出是4个32位字的级联，与 MD4 相同。MD5比MD4来得复杂，并且速度较之要慢一点，但更安全，在抗分析和抗差分方面表现更好。
（3)SHA-1及其他
SHA1是由NIST NSA设计为同DSA一起使用的，它对长度小于2^64的输入，产生长度为160bit的散列值，因此抗穷举(brute-force)性更好。SHA-1 设计时基于和MD4相同原理,并且模仿了该算法。 [2]


HashCode
百度百科的解释：

hashCode是jdk根据对象的地址或者字符串或者数字，通过hash算法，算出来的int类型的数值 详细了解请 参考 public int hashCode()返回该对象的哈希码值。支持此方法是为了提高哈希表（例如 java.util.Hashtable 提供的哈希表）的性能。


hash表
通过hash算法得到的hash值就在这张hash表中，也就是说，hash表就是所有的hash值组成的，有很多种hash函数，也就代表着有很多种算法得到hash值

为什么要用HashCode？

在Java集合中有两类，一类是List，一类是Set

他们之间的区别就在于List集合中的元素师有序的，且可以重复，而Set集合中元素是无序不可重复的。

对于List好处理，但是对于Set而言我们要如何来保证元素不重复呢？

通过迭代来equals()是否相等。数据量小还可以接受，当我们的数据量大的时候效率可想而知（当然我们可以利用算法进行优化）。

比如我们向HashSet插入1000数据，难道我们真的要迭代1000次，调用1000次equals()方法吗？hashCode提供了解决方案。

怎么实现？我们先看hashCode的源码(Object)。

public native int hashCode();  
1
它是一个本地方法，它的实现与本地机器有关，这里我们暂且认为他返回的是对象存储的物理位置（实际上不是，这里写是便于理解）。

当我们向一个集合中添加某个元素，集合会首先调用hashCode方法，这样就可以直接定位它所存储的位置，

若该处没有其他元素，则直接保存。

若该处已经有元素存在，就调用equals方法来匹配这两个元素是否相同，

相同则不存，不同则散列到其他位置。

这样处理，当我们存入大量元素时就可以大大减少调用equals()方法的次数，极大地提高了效率。

所以hashCode在上面扮演的角色为寻域（寻找某个对象在集合中区域位置）。

hashCode可以将集合分成若干个区域，每个对象都可以计算出他们的hash码，可以将hash码分组，每个分组对应着某个存储区域，根据一个对象的hash码就可以确定该对象所存储区域，这样就大大减少查询匹配元素的数量，提高了查询效率。
————————————————

hashcode详解
1 、hash是一个函数，该函数中的实现就是一种算法，就是通过一系列的算法来得到一个hash值，这个时候，我们就需要知道另一个东西，hash表，通过hash算法得到的hash值就在这张hash表中，也就是说，hash表就是所有的hash值组成的，有很多种hash函数，也就代表着有很多种算法得到hash值，如上面截图的三种，等会我们就拿第一种来说。

2、hashcode

有了前面的基础，这里讲解就简单了，hashcode就是通过hash函数得来的，通俗的说，就是通过某一种算法得到的，hashcode就是在hash表中有对应的位置。

每个对象都有hashcode，对象的hashcode怎么得来的呢？

首先一个对象肯定有物理地址，在别的博文中会hashcode说成是代表对象的地址，这里肯定会让读者形成误区，对象的物理地址跟这个hashcode地址不一样，hashcode代表对象的地址说的是对象在hash表中的位置，物理地址说的对象存放在内存中的地址，那么对象如何得到hashcode呢？通过对象的内部地址(也就是物理地址)转换成一个整数，然后该整数通过hash函数的算法就得到了hashcode，所以，hashcode是什么呢？就是在hash表中对应的位置。这里如果还不是很清楚的话，举个例子，hash表中有 hashcode为1、hashcode为2、(…)3、4、5、6、7、8这样八个位置，有一个对象A，A的物理地址转换为一个整数17(这是假如)，就通过直接取余算法，17%8=1，那么A的hashcode就为1，且A就在hash表中1的位置。肯定会有其他疑问，接着看下面，这里只是举个例子来让你们知道什么是hashcode的意义。

二、hashcode有什么作用呢？

前面说了这么多关于hash函数，和hashcode是怎么得来的，还有hashcode对应的是hash表中的位置，可能大家就有疑问，为什么hashcode不直接写物理地址呢，还要另外用一张hash表来代表对象的地址？接下来就告诉你hashcode的作用，

1、HashCode的存在主要是为了查找的快捷性，HashCode是用来在散列存储结构中确定对象的存储地址的(后半句说的用hashcode来代表对象就是在hash表中的位置)

为什么hashcode就查找的更快，比如：我们有一个能存放1000个数这样大的内存中，在其中要存放1000个不一样的数字，用最笨的方法，就是存一个数字，就遍历一遍，看有没有相同得数，当存了900个数字，开始存901个数字的时候，就需要跟900个数字进行对比，这样就很麻烦，很是消耗时间，用hashcode来记录对象的位置，来看一下。hash表中有1、2、3、4、5、6、7、8个位置，存第一个数，hashcode为1，该数就放在hash表中1的位置，存到100个数字，hash表中8个位置会有很多数字了，1中可能有20个数字，存101个数字时，他先查hashcode值对应的位置，假设为1，那么就有20个数字和他的hashcode相同，他只需要跟这20个数字相比较(equals)，如果每一个相同，那么就放在1这个位置，这样比较的次数就少了很多，实际上hash表中有很多位置，这里只是举例只有8个，所以比较的次数会让你觉得也挺多的，实际上，如果hash表很大，那么比较的次数就很少很少了。 通过对原始方法和使用hashcode方法进行对比，我们就知道了hashcode的作用，并且为什么要使用hashcode了

三、equals方法和hashcode的关系？

通过前面这个例子，大概可以知道，先通过hashcode来比较，如果hashcode相等，那么就用equals方法来比较两个对象是否相等，用个例子说明：上面说的hash表中的8个位置，就好比8个桶，每个桶里能装很多的对象，对象A通过hash函数算法得到将它放到1号桶中，当然肯定有别的对象也会放到1号桶中，如果对象B也通过算法分到了1号桶，那么它如何识别桶中其他对象是否和它一样呢，这时候就需要equals方法来进行筛选了。

1、如果两个对象equals相等，那么这两个对象的HashCode一定也相同

2、如果两个对象的HashCode相同，不代表两个对象就相同，只能说明这两个对象在散列存储结构中，存放于同一个位置

这两条你们就能够理解了。

四、为什么equals方法重写的话，建议也一起重写hashcode方法？

（如果对象的equals方法被重写，那么对象的HashCode方法也尽量重写）

举个例子，其实就明白了这个道理，

比如：有个A类重写了equals方法，但是没有重写hashCode方法，看输出结果，对象a1和对象a2使用equals方法相等，按照上面的hashcode的用法，那么他们两个的hashcode肯定相等，但是这里由于没重写hashcode方法，他们两个hashcode并不一样，所以，我们在重写了equals方法后，尽量也重写了hashcode方法，通过一定的算法，使他们在equals相等时，也会有相同的hashcode值。



实例：现在来看一下String的源码中的equals方法和hashcode方法。这个类就重写了这两个方法，现在为什么需要重写这两个方法了吧？

equals方法：其实跟我上面写的那个例子是一样的原理，所以通过源码又知道了String的equals方法验证的是两个字符串的值是否一样。还有Double类也重写了这些方法。很多类有比较这类的，都重写了这两个方法，因为在所有类的父类Object中。equals的功能就是 “”号的功能。你们还可以比较String对象的equals和的区别啦。这里不再说明。




hashcode方法




HashMap是如何存储的？
HashMap的工作原理是近年来常见的Java面试题。几乎每个Java程序员都知道HashMap，都知道哪里要用HashMap，知道HashTable和HashMap之间的区别，那么为何这道面试题如此特殊呢？是因为这道题考察的深度很深。这题经常出现在高级或中高级面试中。投资银行更喜欢问这个问题，甚至会要求你实现HashMap来考察你的编程能力。ConcurrentHashMap和其它同步集合的引入让这道题变得更加复杂。让我们开始探索的旅程吧！



先来些简单的问题
　　“你用过HashMap吗？” “什么是HashMap？你为什么用到它？”

几乎每个人都会回答“是的”，然后回答HashMap的一些特性，譬如HashMap可以接受null键值和值，而HashTable则不能；HashMap是非synchronized;HashMap很快；以及HashMap储存的是键值对等等。这显示出你已经用过HashMap，而且对它相当的熟悉。但是面试官来个急转直下，从此刻开始问出一些刁钻的问题，关于HashMap的更多基础的细节。面试官可能会问出下面的问题：

“你知道HashMap的工作原理吗？” “你知道HashMap的get()方法的工作原理吗？”

你也许会回答“我没有详查标准的Java API，你可以看看Java源代码或者Open JDK。”“我可以用Google找到答案。”

但一些面试者可能可以给出答案，“HashMap是基于hashing的原理，我们使用put(key, value)存储对象到HashMap中，使用get(key)从HashMap中获取对象。当我们给put()方法传递键和值时，我们先对键调用hashCode()方法，返回的hashCode用于找到bucket位置来储存Entry对象。”这里关键点在于指出，HashMap是在bucket中储存键对象和值对象，作为Map.Entry。这一点有助于理解获取对象的逻辑。如果你没有意识到这一点，或者错误的认为仅仅只在bucket中存储值的话，你将不会回答如何从HashMap中获取对象的逻辑。这个答案相当的正确，也显示出面试者确实知道hashing以及HashMap的工作原理。但是这仅仅是故事的开始，当面试官加入一些Java程序员每天要碰到的实际场景的时候，错误的答案频现。下个问题可能是关于HashMap中的碰撞探测(collision detection)以及碰撞的解决方法：

“当两个对象的hashcode相同会发生什么？” 从这里开始，真正的困惑开始了，一些面试者会回答因为hashcode相同，所以两个对象是相等的，HashMap将会抛出异常，或者不会存储它们。然后面试官可能会提醒他们有equals()和hashCode()两个方法，并告诉他们两个对象就算hashcode相同，但是它们可能并不相等。一些面试者可能就此放弃，而另外一些还能继续挺进，他们回答“因为hashcode相同，所以它们的bucket位置相同，‘碰撞’会发生。因为HashMap使用LinkedList存储对象，这个Entry(包含有键值对的Map.Entry对象)会存储在LinkedList中。”这个答案非常的合理，虽然有很多种处理碰撞的方法，这种方法是最简单的，也正是HashMap的处理方法。但故事还没有完结，面试官会继续问：

“如果两个键的hashcode相同，你如何获取值对象？” 面试者会回答：当我们调用get()方法，HashMap会使用键对象的hashcode找到bucket位置，然后获取值对象。面试官提醒他如果有两个值对象储存在同一个bucket，他给出答案:将会遍历LinkedList直到找到值对象。面试官会问因为你并没有值对象去比较，你是如何确定确定找到值对象的？除非面试者直到HashMap在LinkedList中存储的是键值对，否则他们不可能回答出这一题。

其中一些记得这个重要知识点的面试者会说，找到bucket位置之后，会调用keys.equals()方法去找到LinkedList中正确的节点，最终找到要找的值对象。完美的答案！

许多情况下，面试者会在这个环节中出错，因为他们混淆了hashCode()和equals()方法。因为在此之前hashCode()屡屡出现，而equals()方法仅仅在获取值对象的时候才出现。一些优秀的开发者会指出使用不可变的、声明作final的对象，并且采用合适的equals()和hashCode()方法的话，将会减少碰撞的发生，提高效率。不可变性使得能够缓存不同键的hashcode，这将提高整个获取对象的速度，使用String，Interger这样的wrapper类作为键是非常好的选择。

如果你认为到这里已经完结了，那么听到下面这个问题的时候，你会大吃一惊。“如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？”除非你真正知道HashMap的工作原理，否则你将回答不出这道题。默认的负载因子大小为0.75，也就是说，当一个map填满了75%的bucket时候，和其它集合类(如ArrayList等)一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中。这个过程叫作rehashing，因为它调用hash方法找到新的bucket位置。

如果你能够回答这道问题，下面的问题来了：“你了解重新调整HashMap大小存在什么问题吗？”你可能回答不上来，这时面试官会提醒你当多线程的情况下，可能产生条件竞争(race condition)。

当重新调整HashMap大小的时候，确实存在条件竞争，因为如果两个线程都发现HashMap需要重新调整大小了，它们会同时试着调整大小。在调整大小的过程中，存储在LinkedList中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将元素放在LinkedList的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件竞争发生了，那么就死循环了。这个时候，你可以质问面试官，为什么这么奇怪，要在多线程的环境下使用HashMap呢？：）

热心的读者贡献了更多的关于HashMap的问题：

为什么String, Interger这样的wrapper类适合作为键？ String, Interger这样的wrapper类作为HashMap的键是再适合不过了，而且String最为常用。因为String是不可变的，也是final的，而且已经重写了equals()和hashCode()方法了。其他的wrapper类也有这个特点。不可变性是必要的，因为为了要计算hashCode()，就要防止键值改变，如果键值在放入时和获取时返回不同的hashcode的话，那么就不能从HashMap中找到你想要的对象。不可变性还有其他的优点如线程安全。如果你可以仅仅通过将某个field声明成final就能保证hashCode是不变的，那么请这么做吧。因为获取对象的时候要用到equals()和hashCode()方法，那么键对象正确的重写这两个方法是非常重要的。如果两个不相等的对象返回不同的hashcode的话，那么碰撞的几率就会小些，这样就能提高HashMap的性能。
我们可以使用自定义的对象作为键吗？ 这是前一个问题的延伸。当然你可能使用任何对象作为键，只要它遵守了equals()和hashCode()方法的定义规则，并且当对象插入到Map中之后将不会再改变了。如果这个自定义对象时不可变的，那么它已经满足了作为键的条件，因为当它创建之后就已经不能改变了。
我们可以使用CocurrentHashMap来代替HashTable吗？这是另外一个很热门的面试题，因为ConcurrentHashMap越来越多人用了。我们知道HashTable是synchronized的，但是ConcurrentHashMap同步性能更好，因为它仅仅根据同步级别对map的一部分进行上锁。ConcurrentHashMap当然可以代替HashTable，但是HashTable提供更强的线程安全性。看看这篇博客查看Hashtable和ConcurrentHashMap的区别。
　　我个人很喜欢这个问题，因为这个问题的深度和广度，也不直接的涉及到不同的概念。让我们再来看看这些问题设计哪些知识点：

hashing的概念
HashMap中解决碰撞的方法
equals()和hashCode()的应用，以及它们在HashMap中的重要性
不可变对象的好处
HashMap多线程的条件竞争
重新调整HashMap的大小
　　总结
　　HashMap的工作原理
HashMap基于hashing原理，我们通过put()和get()方法储存和获取对象。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，让后找到bucket位置来储存值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用LinkedList来解决碰撞问题，当发生碰撞了，对象将会储存在LinkedList的下一个节点中。 HashMap在每个LinkedList节点中储存键值对对象。

当两个不同的键对象的hashcode相同时会发生什么？ 它们会储存在同一个bucket位置的LinkedList中。键对象的equals()方法用来找到键值对。

因为HashMap的好处非常多，我曾经在电子商务的应用中使用HashMap作为缓存。因为金融领域非常多的运用Java，也出于性能的考虑，我们会经常用到HashMap和ConcurrentHashMap。你可以查看更多的关于HashMap和HashTable的文章。
HashMap碰撞原理，jdk是如何解决碰撞问题的？
hashmap冲突的解决方法以及原理分析：
在Java编程语言中，最基本的结构就是两种，一种是数组，一种是模拟指针(引用),所有的数据结构都可以用这两个基本结构构造，HashMap也一样。当程序试图将多个 key-value 放入 HashMap 中时，以如下代码片段为例：

HashMap<String,Object> m=new HashMap<String,Object>(); 
m.put("a", "rrr1"); 
m.put("b", "tt9"); 
m.put("c", "tt8"); 
m.put("d", "g7"); 
m.put("e", "d6"); 
m.put("f", "d4"); 
m.put("g", "d4"); 
m.put("h", "d3"); 
m.put("i", "d2"); 
m.put("j", "d1"); 
m.put("k", "1"); 
m.put("o", "2"); 
m.put("p", "3"); 
m.put("q", "4"); 
m.put("r", "5"); 
m.put("s", "6"); 
m.put("t", "7"); 
m.put("u", "8"); 
m.put("v", "9");
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
HashMap 采用一种所谓的“Hash 算法”来决定每个元素的存储位置。当程序执行 map.put(String,Obect)方法 时，系统将调用String的 hashCode() 方法得到其 hashCode 值——每个 Java 对象都有 hashCode() 方法，都可通过该方法获得它的 hashCode 值。得到这个对象的 hashCode 值之后，系统会根据该 hashCode 值来决定该元素的存储位置。源码如下:

Java代码 收藏代码

   public V put(K key, V value) {  
        if (key == null)  
            return putForNullKey(value);  
        int hash = hash(key.hashCode());  
        int i = indexFor(hash, table.length);  
        for (Entry<K,V> e = table[i]; e != null; e = e.next) {  
            Object k;  
            //判断当前确定的索引位置是否存在相同hashcode和相同key的元素，如果存在相同的hashcode和相同的key的元素，那么新值覆盖原来的旧值，并返回旧值。  
            //如果存在相同的hashcode，那么他们确定的索引位置就相同，这时判断他们的key是否相同，如果不相同，这时就是产生了hash冲突。  
            //Hash冲突后，那么HashMap的单个bucket里存储的不是一个 Entry，而是一个 Entry 链。  
            //系统只能必须按顺序遍历每个 Entry，直到找到想搜索的 Entry 为止——如果恰好要搜索的 Entry 位于该 Entry 链的最末端（该 Entry 是最早放入该 bucket 中），  
            //那系统必须循环到最后才能找到该元素。  
            if (e.hash == hash && ((k = e.key) == key || key.equals(k))) {  
                V oldValue = e.value;  
                e.value = value;  
                return oldValue;  
            }  
        }  
        modCount++;  
        addEntry(hash, key, value, i);  
        return null;  
    }  
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
上面程序中用到了一个重要的内部接口：Map.Entry，每个 Map.Entry 其实就是一个 key-value 对。从上面程序中可以看出：当系统决定存储 HashMap 中的 key-value 对时，完全没有考虑 Entry 中的 value，仅仅只是根据 key 来计算并决定每个 Entry 的存储位置。这也说明了前面的结论：我们完全可以把 Map 集合中的 value 当成 key 的附属，当系统决定了 key 的存储位置之后，value 随之保存在那里即可.HashMap程序经过我改造，我故意的构造出了hash冲突现象，因为HashMap的初始大小16,但是我在hashmap里面放了超过16个元素，并且我屏蔽了它的resize()方法。不让它去扩容。这时HashMap的底层数组Entry[] table结构如下:




Hashmap里面的bucket出现了单链表的形式，散列表要解决的一个问题就是散列值的冲突问题，通常是两种方法：链表法和开放地址法。链表法就是将相同hash值的对象组织成一个链表放在hash值对应的槽位；开放地址法是通过一个探测算法，当某个槽位已经被占据的情况下继续查找下一个可以使用的槽位。java.util.HashMap采用的链表法的方式，链表是单向链表。形成单链表的核心代码如下：

Java代码 收藏代码

void addEntry(int hash, K key, V value, int bucketIndex) {  
    Entry<K,V> e = table[bucketIndex];  
    table[bucketIndex] = new Entry<K,V>(hash, key, value, e);  
    if (size++ >= threshold)  
        resize(2 * table.length);  
bsp;  
1
2
3
4
5
6
 上面方法的代码很简单，但其中包含了一个设计：系统总是将新添加的 Entry 对象放入 table 数组的 bucketIndex 索引处——如果 bucketIndex 索引处已经有了一个 Entry 对象，那新添加的 Entry 对象指向原有的 Entry 对象（产生一个 Entry 链），如果 bucketIndex 索引处没有 Entry 对象，也就是上面程序代码的 e 变量是 null，也就是新放入的 Entry 对象指向 null，也就是没有产生 Entry 链。
1
HashMap里面没有出现hash冲突时，没有形成单链表时，hashmap查找元素很快,get()方法能够直接定位到元素，但是出现单链表后，单个bucket 里存储的不是一个 Entry，而是一个 Entry 链，系统只能必须按顺序遍历每个 Entry，直到找到想搜索的 Entry 为止——如果恰好要搜索的 Entry 位于该 Entry 链的最末端（该 Entry 是最早放入该 bucket 中），那系统必须循环到最后才能找到该元素。 当创建 HashMap 时，有一个默认的负载因子（load factor），其默认值为 0.75，这是时间和空间成本上一种折衷：增大负载因子可以减少 Hash 表（就是那个 Entry 数组）所占用的内存空间，但会增加查询数据的时间开销，而查询是最频繁的的操作（HashMap 的 get() 与 put() 方法都要用到查询）；减小负载因子会提高数据查询的性能，但会增加 Hash 表所占用的内存空间。
一、HashMap概述

HashMap基于哈希表的 Map 接口的实现。此实现提供所有可选的映射操作，并允许使用 null 值和 null 键。（除了不同步和允许使用 null 之外，HashMap 类与 Hashtable 大致相同。）此类不保证映射的顺序，特别是它不保证该顺序恒久不变。

值得注意的是HashMap不是线程安全的，如果想要线程安全的HashMap，可以通过Collections类的静态方法synchronizedMap获得线程安全的HashMap。

Map map = Collections.synchronizedMap(new HashMap());
1
二、HashMap的数据结构

HashMap的底层主要是基于数组和链表来实现的，它之所以有相当快的查询速度主要是因为它是通过计算散列码来决定存储的位置。HashMap中主要是通过key的hashCode来计算hash值的，只要hashCode相同，计算出来的hash值就一样。如果存储的对象对多了，就有可能不同的对象所算出来的hash值是相同的，这就出现了所谓的hash冲突。学过数据结构的同学都知道，解决hash冲突的方法有很多，HashMap底层是通过链表来解决hash冲突的。




图中，紫色部分即代表哈希表，也称为哈希数组，数组的每个元素都是一个单链表的头节点，链表是用来解决冲突的，如果不同的key映射到了数组的同一位置处，就将其放入单链表中。

我们看看HashMap中Entry类的代码：

/** Entry是单向链表。    
 * 它是 “HashMap链式存储法”对应的链表。    
 *它实现了Map.Entry 接口，即实现getKey(), getValue(), setValue(V value), equals(Object o), hashCode()这些函数  
**/  
static class Entry<K,V> implements Map.Entry<K,V> {    
    final K key;    
    V value;    
    // 指向下一个节点    
    Entry<K,V> next;    
    final int hash;    
   
    // 构造函数。    
    // 输入参数包括"哈希值(h)", "键(k)", "值(v)", "下一节点(n)"    
    Entry(int h, K k, V v, Entry<K,V> n) {    
        value = v;    
        next = n;    
        key = k;    
        hash = h;    
    }    
   
    public final K getKey() {    
        return key;    
    }    
   
    public final V getValue() {    
        return value;    
    }    
   
    public final V setValue(V newValue) {    
        V oldValue = value;    
        value = newValue;    
        return oldValue;    
    }    
   
    // 判断两个Entry是否相等    
    // 若两个Entry的“key”和“value”都相等，则返回true。    
    // 否则，返回false    
    public final boolean equals(Object o) {    
        if (!(o instanceof Map.Entry))    
            return false;    
        Map.Entry e = (Map.Entry)o;    
        Object k1 = getKey();    
        Object k2 = e.getKey();    
        if (k1 == k2 || (k1 != null && k1.equals(k2))) {    
            Object v1 = getValue();    
            Object v2 = e.getValue();    
            if (v1 == v2 || (v1 != null && v1.equals(v2)))    
                return true;    
        }    
        return false;    
    }    
   
    // 实现hashCode()    
    public final int hashCode() {    
        return (key==null   ? 0 : key.hashCode()) ^    
               (value==null ? 0 : value.hashCode());    
    }    
   
    public final String toString() {    
        return getKey() + "=" + getValue();    
    }    
   
    // 当向HashMap中添加元素时，绘调用recordAccess()。    
    // 这里不做任何处理    
    void recordAccess(HashMap<K,V> m) {    
    }    
   
    // 当从HashMap中删除元素时，绘调用recordRemoval()。    
    // 这里不做任何处理    
    void recordRemoval(HashMap<K,V> m) {    
    }    
}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
HashMap其实就是一个Entry数组，Entry对象中包含了键和值，其中next也是一个Entry对象，它就是用来处理hash冲突的，形成一个链表。

三、HashMap源码分析

   1、关键属性
1
先看看HashMap类中的一些关键属性：

1 transient Entry[] table;//存储元素的实体数组
2  
3 transient int size;//存放元素的个数
4  
5 int threshold; //临界值   当实际大小超过临界值时，会进行扩容threshold = 加载因子*容量
6 
7  final float loadFactor; //加载因子
8  
9 transient int modCount;//被修改的次数
1
2
3
4
5
6
7
8
9
其中loadFactor加载因子是表示Hsah表中元素的填满的程度.

若:加载因子越大,填满的元素越多,好处是,空间利用率高了,但:冲突的机会加大了.链表长度会越来越长,查找效率降低。

反之,加载因子越小,填满的元素越少,好处是:冲突的机会减小了,但:空间浪费多了.表中的数据将过于稀疏（很多空间还没用，就开始扩容了）

冲突的机会越大,则查找的成本越高.

因此,必须在 "冲突的机会"与"空间利用率"之间寻找一种平衡与折衷. 这种平衡与折衷本质上是数据结构中有名的"时-空"矛盾的平衡与折衷.

如果机器内存足够，并且想要提高查询速度的话可以将加载因子设置小一点；相反如果机器内存紧张，并且对查询速度没有什么要求的话可以将加载因子设置大一点。不过一般我们都不用去设置它，让它取默认值0.75就好了。

2、构造方法

下面看看HashMap的几个构造方法：

public HashMap(int initialCapacity, float loadFactor) {
 2         //确保数字合法
 3         if (initialCapacity < 0)
 4             throw new IllegalArgumentException("Illegal initial capacity: " +
 5                                               initialCapacity);
 6         if (initialCapacity > MAXIMUM_CAPACITY)
 7             initialCapacity = MAXIMUM_CAPACITY;
 8         if (loadFactor <= 0 || Float.isNaN(loadFactor))
 9             throw new IllegalArgumentException("Illegal load factor: " +
10                                               loadFactor);
11 
12         // Find a power of 2 >= initialCapacity
13         int capacity = 1;   //初始容量
14         while (capacity < initialCapacity)   //确保容量为2的n次幂，使capacity为大于initialCapacity的最小的2的n次幂
15             capacity <<= 1;
16 
17         this.loadFactor = loadFactor;
18         threshold = (int)(capacity * loadFactor);
19         table = new Entry[capacity];
20        init();
21    }
22 
23     public HashMap(int initialCapacity) {
24         this(initialCapacity, DEFAULT_LOAD_FACTOR);
25    }
26 
27     public HashMap() {
28         this.loadFactor = DEFAULT_LOAD_FACTOR;
29         threshold = (int)(DEFAULT_INITIAL_CAPACITY * DEFAULT_LOAD_FACTOR);
30         table = new Entry[DEFAULT_INITIAL_CAPACITY];
31        init();
32     }
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
我们可以看到在构造HashMap的时候如果我们指定了加载因子和初始容量的话就调用第一个构造方法，否则的话就是用默认的。默认初始容量为16，默认加载因子为0.75。我们可以看到上面代码中13-15行，这段代码的作用是确保容量为2的n次幂，使capacity为大于initialCapacity的最小的2的n次幂，至于为什么要把容量设置为2的n次幂，我们等下再看。

重点分析下HashMap中用的最多的两个方法put和get

   3、存储数据
1
下面看看HashMap存储数据的过程是怎样的，首先看看HashMap的put方法：

public V put(K key, V value) {
     // 若“key为null”，则将该键值对添加到table[0]中。
         if (key == null) 
            return putForNullKey(value);
     // 若“key不为null”，则计算该key的哈希值，然后将其添加到该哈希值对应的链表中。
         int hash = hash(key.hashCode());
     //搜索指定hash值在对应table中的索引
         int i = indexFor(hash, table.length);
     // 循环遍历Entry数组,若“该key”对应的键值对已经存在，则用新的value取代旧的value。然后退出！
         for (Entry<K,V> e = table[i]; e != null; e = e.next) { 
             Object k;
              if (e.hash == hash && ((k = e.key) == key || key.equals(k))) { //如果key相同则覆盖并返回旧值
                  V oldValue = e.value;
                 e.value = value;
                 e.recordAccess(this);
                 return oldValue;
              }
         }
     //修改次数+1
         modCount++;
     //将key-value添加到table[i]处
     addEntry(hash, key, value, i);
     return null;
}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
上面程序中用到了一个重要的内部接口：Map.Entry，每个 Map.Entry 其实就是一个 key-value 对。从上面程序中可以看出：当系统决定存储 HashMap 中的 key-value 对时，完全没有考虑 Entry 中的 value，仅仅只是根据 key 来计算并决定每个 Entry 的存储位置。这也说明了前面的结论：我们完全可以把 Map 集合中的 value 当成 key 的附属，当系统决定了 key 的存储位置之后，value 随之保存在那里即可。

我们慢慢的来分析这个函数，第2和3行的作用就是处理key值为null的情况，我们看看putForNullKey(value)方法：

1 private V putForNullKey(V value) {
 2         for (Entry<K,V> e = table[0]; e != null; e = e.next) {
 3             if (e.key == null) {   //如果有key为null的对象存在，则覆盖掉
 4                 V oldValue = e.value;
 5                 e.value = value;
 6                 e.recordAccess(this);
 7                 return oldValue;
 8            }
 9        }
10         modCount++;
11         addEntry(0, null, value, 0); //如果键为null的话，则hash值为0
12         return null;
13     }
1
2
3
4
5
6
7
8
9
10
11
12
13
注意：如果key为null的话，hash值为0，对象存储在数组中索引为0的位置。即table[0]

我们再回去看看put方法中第4行，它是通过key的hashCode值计算hash码，下面是计算hash码的函数：

1  //计算hash值的方法 通过键的hashCode来计算
2     static int hash(int h) {
3         // This function ensures that hashCodes that differ only by
4         // constant multiples at each bit position have a bounded
5         // number of collisions (approximately 8 at default load factor).
6         h ^= (h >>> 20) ^ (h >>> 12);
7         return h ^ (h >>> 7) ^ (h >>> 4);
8     }
1
2
3
4
5
6
7
8
得到hash码之后就会通过hash码去计算出应该存储在数组中的索引，计算索引的函数如下：

1     static int indexFor(int h, int length) { //根据hash值和数组长度算出索引值
2         return h & (length-1);  //这里不能随便算取，用hash&(length-1)是有原因的，这样可以确保算出来的索引是在数组大小范围内，不会超出
3     }
1
2
3
这个我们要重点说下，我们一般对哈希表的散列很自然地会想到用hash值对length取模（即除法散列法），Hashtable中也是这样实现的，这种方法基本能保证元素在哈希表中散列的比较均匀，但取模会用到除法运算，效率很低，HashMap中则通过h&(length-1)的方法来代替取模，同样实现了均匀的散列，但效率要高很多，这也是HashMap对Hashtable的一个改进。

接下来，我们分析下为什么哈希表的容量一定要是2的整数次幂。首先，length为2的整数次幂的话，h&(length-1)就相当于对length取模，这样便保证了散列的均匀，同时也提升了效率；其次，length为2的整数次幂的话，为偶数，这样length-1为奇数，奇数的最后一位是1，这样便保证了h&(length-1)的最后一位可能为0，也可能为1（这取决于h的值），即与后的结果可能为偶数，也可能为奇数，这样便可以保证散列的均匀性，而如果length为奇数的话，很明显length-1为偶数，它的最后一位是0，这样h&(length-1)的最后一位肯定为0，即只能为偶数，这样任何hash值都只会被散列到数组的偶数下标位置上，这便浪费了近一半的空间，因此，length取2的整数次幂，是为了使不同hash值发生碰撞的概率较小，这样就能使元素在哈希表中均匀地散列。
1
这看上去很简单，其实比较有玄机的，我们举个例子来说明：

假设数组长度分别为15和16，优化后的hash码分别为8和9，那么&运算后的结果如下：

   h & (table.length-1)                     hash                             table.length-1
   8 & (15-1)：                                 0100                   &              1110                   =                0100
   9 & (15-1)：                                 0101                   &              1110                   =                0100
   -----------------------------------------------------------------------------------------------------------------------
   8 & (16-1)：                                 0100                   &              1111                   =                0100
   9 & (16-1)：                                 0101                   &              1111                   =                0101
1
2
3
4
5
6
从上面的例子中可以看出：当它们和15-1（1110）“与”的时候，产生了相同的结果，也就是说它们会定位到数组中的同一个位置上去，这就产生了碰撞，8和9会被放到数组中的同一个位置上形成链表，那么查询的时候就需要遍历这个链 表，得到8或者9，这样就降低了查询的效率。同时，我们也可以发现，当数组长度为15的时候，hash值会与15-1（1110）进行“与”，那么 最后一位永远是0，而0001，0011，0101，1001，1011，0111，1101这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率！而当数组长度为16时，即为2的n次方时，2n-1得到的二进制数的每个位上的值都为1，这使得在低位上&时，得到的和原hash的低位相同，加之hash(int h)方法对key的hashCode的进一步优化，加入了高位计算，就使得只有相同的hash值的两个值才会被放到数组中的同一个位置上形成链表。

所以说，当数组长度为2的n次幂的时候，不同的key算得得index相同的几率较小，那么数据在数组上分布就比较均匀，也就是说碰撞的几率小，相对的，查询的时候就不用遍历某个位置上的链表，这样查询效率也就较高了。

根据上面 put 方法的源代码可以看出，当程序试图将一个key-value对放入HashMap中时，程序首先根据该 key 的 hashCode() 返回值决定该 Entry 的存储位置：如果两个 Entry 的 key 的 hashCode() 返回值相同，那它们的存储位置相同。如果这两个 Entry 的 key 通过 equals 比较返回 true，新添加 Entry 的 value 将覆盖集合中原有 Entry 的 value，但key不会覆盖。如果这两个 Entry 的 key 通过 equals 比较返回 false，新添加的 Entry 将与集合中原有 Entry 形成 Entry 链，而且新添加的 Entry 位于 Entry 链的头部——具体说明继续看 addEntry() 方法的说明。

1 void addEntry(int hash, K key, V value, int bucketIndex) {
2         Entry<K,V> e = table[bucketIndex]; //如果要加入的位置有值，将该位置原先的值设置为新entry的next,也就是新entry链表的下一个节点
3         table[bucketIndex] = new Entry<>(hash, key, value, e);
4         if (size++ >= threshold) //如果大于临界值就扩容
5             resize(2 * table.length); //以2的倍数扩容
6     }
 
1
2
3
4
5
6
7
参数bucketIndex就是indexFor函数计算出来的索引值，第2行代码是取得数组中索引为bucketIndex的Entry对象，第3行就是用hash、key、value构建一个新的Entry对象放到索引为bucketIndex的位置，并且将该位置原先的对象设置为新对象的next构成链表。

第4行和第5行就是判断put后size是否达到了临界值threshold，如果达到了临界值就要进行扩容，HashMap扩容是扩为原来的两倍。

4、调整大小

resize()方法如下：

重新调整HashMap的大小，newCapacity是调整后的单位

 1     void resize(int newCapacity) {
 2         Entry[] oldTable = table;
 3         int oldCapacity = oldTable.length;
 4         if (oldCapacity == MAXIMUM_CAPACITY) {
 5             threshold = Integer.MAX_VALUE;
 6             return;
 7        }
 8 
 9         Entry[] newTable = new Entry[newCapacity];
10         transfer(newTable);//用来将原先table的元素全部移到newTable里面
11         table = newTable;  //再将newTable赋值给table
12         threshold = (int)(newCapacity * loadFactor);//重新计算临界值
13     }
1
2
3
4
5
6
7
8
9
10
11
12
13
新建了一个HashMap的底层数组，上面代码中第10行为调用transfer方法，将HashMap的全部元素添加到新的HashMap中,并重新计算元素在新的数组中的索引位置

当HashMap中的元素越来越多的时候，hash冲突的几率也就越来越高，因为数组的长度是固定的。所以为了提高查询的效率，就要对HashMap的数组进行扩容，数组扩容这个操作也会出现在ArrayList中，这是一个常用的操作，而在HashMap数组扩容之后，最消耗性能的点就出现了：原数组中的数据必须重新计算其在新数组中的位置，并放进去，这就是resize。

那么HashMap什么时候进行扩容呢？当HashMap中的元素个数超过数组大小loadFactor时，就会进行数组扩容，loadFactor的默认值为0.75，这是一个折中的取值。也就是说，默认情况下，数组大小为16，那么当HashMap中元素个数超过160.75=12的时候，就把数组的大小扩展为 2*16=32，即扩大一倍，然后重新计算每个元素在数组中的位置，扩容是需要进行数组复制的，复制数组是非常消耗性能的操作，所以如果我们已经预知HashMap中元素的个数，那么预设元素的个数能够有效的提高HashMap的性能。

5、数据读取

1.public V get(Object key) {   
2.    if (key == null)   
3.        return getForNullKey();   
4.    int hash = hash(key.hashCode());   
5.    for (Entry<K,V> e = table[indexFor(hash, table.length)];   
6.        e != null;   
7.        e = e.next) {   
8.        Object k;   
9.        if (e.hash == hash && ((k = e.key) == key || key.equals(k)))   
10.            return e.value;   
11.    }   
12.    return null;   
13.}  
1
2
3
4
5
6
7
8
9
10
11
12
13
有了上面存储时的hash算法作为基础，理解起来这段代码就很容易了。从上面的源代码中可以看出：从HashMap中get元素时，首先计算key的hashCode，找到数组中对应位置的某一元素，然后通过key的equals方法在对应位置的链表中找到需要的元素。

6、HashMap的性能参数：

HashMap 包含如下几个构造器：

HashMap()：构建一个初始容量为 16，负载因子为 0.75 的 HashMap。

HashMap(int initialCapacity)：构建一个初始容量为 initialCapacity，负载因子为 0.75 的 HashMap。

HashMap(int initialCapacity, float loadFactor)：以指定初始容量、指定的负载因子创建一个 HashMap。

HashMap的基础构造器HashMap(int initialCapacity, float loadFactor)带有两个参数，它们是初始容量initialCapacity和加载因子loadFactor。

initialCapacity：HashMap的最大容量，即为底层数组的长度。

loadFactor：负载因子loadFactor定义为：散列表的实际元素数目(n)/ 散列表的容量(m)。

负载因子衡量的是一个散列表的空间的使用程度，负载因子越大表示散列表的装填程度越高，反之愈小。对于使用链表法的散列表来说，查找一个元素的平均时间是O(1+a)，因此如果负载因子越大，对空间的利用更充分，然而后果是查找效率的降低；如果负载因子太小，那么散列表的数据将过于稀疏，对空间造成严重浪费。

HashMap的实现中，通过threshold字段来判断HashMap的最大容量：

threshold = (int)(capacity * loadFactor);  
1
结合负载因子的定义公式可知，threshold就是在此loadFactor和capacity对应下允许的最大元素数目，超过这个数目就重新resize，以降低实际的负载因子。默认的的负载因子0.75是对空间和时间效率的一个平衡选择。当容量超出此最大容量时， resize后的HashMap容量是容量的两倍

Java 8中HashMap冲突解决
在Java 8 之前，HashMap和其他基于map的类都是通过链地址法解决冲突，它们使用单向链表来存储相同索引值的元素。在最坏的情况下，这种方式会将HashMap的get方法的性能从O(1)降低到O(n)。为了解决在频繁冲突时hashmap性能降低的问题，Java 8中使用平衡树来替代链表存储冲突的元素。这意味着我们可以将最坏情况下的性能从O(n)提高到O(logn)。
在Java 8中使用常量TREEIFY_THRESHOLD来控制是否切换到平衡树来存储。目前，这个常量值是8，这意味着当有超过8个元素的索引一样时，HashMap会使用树来存储它们。
这一改变是为了继续优化常用类。大家可能还记得在Java 7中为了优化常用类对ArrayList和HashMap采用了延迟加载的机制，在有元素加入之前不会分配内存，这会减少空的链表和HashMap占用的内存。
这一动态的特性使得HashMap一开始使用链表，并在冲突的元素数量超过指定值时用平衡二叉树替换链表。不过这一特性在所有基于hash table的类中并没有，例如Hashtable和WeakHashMap。
目前，只有ConcurrentHashMap,LinkedHashMap和HashMap会在频繁冲突的情况下使用平衡树。

什么时候会产生冲突
HashMap中调用hashCode()方法来计算hashCode。
由于在Java中两个不同的对象可能有一样的hashCode,所以不同的键可能有一样hashCode，从而导致冲突的产生。

总结
HashMap在处理冲突时使用链表存储相同索引的元素。
从Java 8开始，HashMap，ConcurrentHashMap和LinkedHashMap在处理频繁冲突时将使用平衡树来代替链表，当同一hash桶中的元素数量超过特定的值便会由链表切换到平衡树，这会将get()方法的性能从O(n)提高到O(logn)。
当从链表切换到平衡树时，HashMap迭代的顺序将会改变。不过这并不会造成什么问题，因为HashMap并没有对迭代的顺序提供任何保证。
从Java 1中就存在的Hashtable类为了保证迭代顺序不变，即便在频繁冲突的情况下也不会使用平衡树。这一决定是为了不破坏某些较老的需要依赖于Hashtable迭代顺序的Java应用。
除了Hashtable之外，WeakHashMap和IdentityHashMap也不会在频繁冲突的情况下使用平衡树。
使用HashMap之所以会产生冲突是因为使用了键对象的hashCode()方法，而equals()和hashCode()方法不保证不同对象的hashCode是不同的。需要记住的是，相同对象的hashCode一定是相同的，但相同的hashCode不一定是相同的对象。
在HashTable和HashMap中，冲突的产生是由于不同对象的hashCode()方法返回了一样的值。
以上就是Java中HashMap如何处理冲突。这种方法被称为链地址法，因为使用链表存储同一桶内的元素。通常情况HashMap，HashSet，LinkedHashSet，LinkedHashMap，ConcurrentHashMap，HashTable，IdentityHashMap和WeakHashMap均采用这种方法处理冲突。

从JDK 8开始，HashMap，LinkedHashMap和ConcurrentHashMap为了提升性能，在频繁冲突的时候使用平衡树来替代链表。因为HashSet内部使用了HashMap，LinkedHashSet内部使用了LinkedHashMap，所以他们的性能也会得到提升。

HashMap的快速高效，使其使用非常广泛。其原理是，调用hashCode（）和equals（）方法，并对hashcode进行一定的哈希运算得到相应value的位置信息，将其分到不同的桶里。桶的数量一般会比所承载的实际键值对多。当通过key进行查找的时候，往往能够在常数时间内找到该value。

但是，当某种针对key的hashcode的哈希运算得到的位置信息重复了之后，就发生了哈希碰撞。这会对HashMap的性能产生灾难性的影响。

在Java 8 之前， 如果发生碰撞往往是将该value直接链接到该位置的其他所有value的末尾，即相互碰撞的所有value形成一个链表。

因此，在最坏情况下，HashMap的查找时间复杂度将退化到O（n）.

但是在Java 8 中，该碰撞后的处理进行了改进。当一个位置所在的冲突过多时，存储的value将形成一个排序二叉树，排序依据为key的hashcode。

则，在最坏情况下，HashMap的查找时间复杂度将从O（1）退化到O（logn）。

虽然是一个小小的改进，但意义重大：

1、O（n）到O（logn）的时间开销。

2、如果恶意程序知道我们用的是Hash算法，则在纯链表情况下，它能够发送大量请求导致哈希碰撞，然后不停访问这些key导致HashMap忙于进行线性查找，最终陷入瘫痪，即形成了拒绝服务攻击（DoS）。
HashMap的key重复,那么value会被覆盖吗？
如果key相同，但是hashcode不同，那么value不会被覆盖

如果key相同，并且hashCode相同，那么value会被覆盖
HashMap如何实现相同key存入数据后不被覆盖？
需求：

实现一个在HashMap中存入（任意类型）相同的key值后，key中的value不会被覆盖，而是能够进行叠加！

拿到一个需求的时候，我们要先进行分析，看此需求能否实现，基于已有的知识（经验），然后在通过目前的一些技术看此需求如何实现。

要实现在HashMap中插入相同的key值，内容不被覆盖，那么肯定要了解HashMap的一些机制，首先看一下HashMap的put方法：

从JDK API中看到HashMap的put如何先前存储了一个key（键），在指定相同的key（键）的时候，会用新的值替换旧的值。

如下的代码示例：

public static void main(String[] args) {
    Map<String, Object> map = new HashMap<>();

    map.put("aflyun", "Java编程技术乐园");

    map.put("aflyun", "生活在长沙的延安人");

    System.out.println(map.toString());
}
--打印：--
{aflyun=生活在长沙的延安人}    
1
2
3
4
5
6
7
8
9
10
11
通过上面的示例分析：为什么存入相同的key后，旧值就被新值替换了呢？

要想知道具体原因，那只能去看HashMap的源码实现了。看一下put(K key, V value)方法了，本篇HashMap源码是JDK1.8版本！

/**

HashMap 的put方法
/
public V put(K key, V value) {
return putVal(hash(key), key, value, false, true);
}
/
HashMap 的containsKey方法
**/
public boolean containsKey(Object key) {
return getNode(hash(key), key) != null;
}
/**

将存入的key进行hash操作，也就是使用key.hashCode()！
**/
static final int hash(Object key) {
int h;
return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
判断put和判断key是否是同一个key的时候，使用大概如下判断逻辑：
if (e.hash == hash && ((k = e.key) == key || (key != null && key.equals(k))))
先判断Hash是否一致，然后在判断传入key和当前集合中是否有相同的key。如果key相同，则新值替换旧值。其中在判断中使用了

==
equals
==和 equals 的区别有时候面试会问到，如何你知道这两个的区别不仅看源码能够很好的理解，并且遇到面试也不怕了。

tips：简述==和 equals 的区别>

1）对于==，如果作用于基本数据类型的变量，则直接比较其存储的 “值”是否相等；如果作用于引用类型的变量，则比较的是所指向的对象的地址！（确切的说，是堆内存地址）
　
　2）对于equals方法，注意：equals方法不能作用于基本数据类型的变量。如果没有对equals方法进行重写，则比较的是引用类型的变量所指向的对象的地址；诸如String等类对equals方法进行了重写的话，比较的是所指向的对象的内容。

注：对于第二种类型，除非是同一个new出来的对象，他们的比较后的结果为true，否则比较后结果为false。因为每new一次，都会重新开辟堆内存空间。
equals()方法介绍：

JAVA当中所有的类都是继承于Object这个超类的，在Object类中定义了一个equals的方法，equals的源码是这样写的：

public boolean equals(Object obj) {
    //this - s1
    //obj - s2
    return (this == obj);
}
1
2
3
4
5
可以看到，这个方法的初始默认行为是比较对象的内存地址值，一般来说，意义不大。所以，在一些类库当中这个方法被重写了，如String、Integer、Date。在这些类当中equals有其自身的实现（一般都是用来比较对象的成员变量值是否相同），而不再是比较类在堆内存中的存放地址了。
所以说，对于复合数据类型之间进行equals比较，在没有覆写equals方法的情况下，他们之间的比较还是内存中的存放位置的地址值，跟双等号（==）的结果相同；如果被复写，按照复写的要求来。

我们对上面的两段内容做个总结吧：

== 的作用：
　　基本类型：比较的就是值是否相同
　　引用类型：比较的就是地址值是否相同
equals 的作用:
　　引用类型：默认情况下，比较的是地址值。
注：不过，我们可以根据情况自己重写该方法。一般重写都是自动生成，比较对象的成员变量值是否相同

有了上面的分析基础，那针对上面String类型的key的话，那实现起来就比较简单了！因为String中已经实现了HashCode和 equals代码如下：

自定义HashMap

public class MyHashMap<K> extends HashMap<K,String> {

    /**
     * 使用HashMap中containsKey判断key是否已经存在
     * @param key
     * @param value
     * @return
     */
    @Override
    public String put(K key, String value) {
        String newV = value;
        if (containsKey(key)) {
            String oldV = get(key);
            newV = oldV + "---" + newV;
        }
        return super.put(key, newV);
    }
}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
String类型key的进行put操作

public static void main(String[] args) {
    MyHashMap<String> map = new MyHashMap<String>();

    map.put("aflyun", "Java编程技术乐园");

    map.put("aflyun", "生活在长沙的延安人");

    map.put("aflyun", "期待你加入乐园");

    System.out.println(map.toString());
}

--打印：---
{aflyun=Java编程技术乐园---生活在长沙的延安人---期待你加入乐园}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
此时同样的key内容是进行叠加的，不是进行替换！那如何是自定义的类，要当作key，那要怎么做呢？
其实也就是重写了hashCode和equals就可以了。

public class PrettyGirl {
    /**
     * 姑娘唯一认证ID
     */
    private String id;
    /**
     * 姑娘姓字名谁
     */
    private String name;


    @Override
    public boolean equals(Object o) {
        if (this == o) {return true;}
        if (o == null || getClass() != o.getClass()) {return false;}
        PrettyGirl that = (PrettyGirl) o;
        return Objects.equals(id, that.id) &&
                Objects.equals(name, that.name);
    }

    @Override
    public int hashCode() {
        return Objects.hash(id, name);
    }
}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
自定义类型当做key的进行put操作

public static void main(String[] args) {

    PrettyGirl prettyGirl = new PrettyGirl();

    Map<PrettyGirl,String> map = new HashMap<>();

    map.put(prettyGirl, "Java编程技术乐园");

    map.put(prettyGirl, "生活在长沙的延安人");

    map.put(prettyGirl, "期待和你加入乐园");

    System.out.println("map :" + map.toString());

    MyHashMap<PrettyGirl> myMap = new MyHashMap<PrettyGirl>();

    myMap.put(prettyGirl, "Java编程技术乐园");

    myMap.put(prettyGirl, "生活在长沙的延安人");

    myMap.put(prettyGirl, "期待和你加入乐园");

    System.out.println("myMap :" + myMap.toString());
}
--打印：---
map :{com.happy.PrettyGirl@3c1=期待和你加入乐园}
myMap :{com.happy.PrettyGirl@3c1=Java编程技术乐园---生活在长沙的延安人---期待和你加入乐园}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
总结：要实现开头的需求

1、如果是类似String这种，已经重写了hashCode和equals的。则只需要创建一个自己的HashMap类，重写put即可。

2、如果是自定义的类，那就必须重写了hashCode和equals的，然后在使用自定义的HashMap类了。

具体的代码判断逻辑：

判断key是否存在的时候是先比较key的hashCode，再比较相等或equals的，所以重写hashCode()和equals()方法即可实现添加重复元素。重写这两个方法之后就可以覆盖重复的键值对，如果需要对value进行叠加，调用put()方法之前用containsKey()方法判断是否有重复的键值，如果有，则用get()方法获取原有的value，再加上新加入的value即可。
什么时候需要重写HashCode、equals？为什么重写equals方法时必须重写hashcode方法？
什么时候需要重写HashCode、equals
当我们自定义了对象，并且想要将自定义的对象加到Map中时，我们就必须对自定义的对象重写这两个方法，才能正确使用Map。

为什么重写equals方法时必须重写hashcode方法？

总结： 因为不重写hashcode，创建的每个对象都不相等。就没办法找到相等的两个对象
在我们的业务系统中判断对象时有时候需要的不是一种严格意义上的相等，而是一种业务上的对象相等。在这种情况下，原生的equals方法就不能满足我们的需求了
所以这个时候我们需要重写equals方法，来满足我们的业务系统上的需求。那么为什么在重写equals方法的时候需要重写hashCode方法呢？
我们先来看一下Object.hashCode的通用约定（摘自《Effective Java》第45页）

在一个应用程序执行期间，如果一个对象的equals方法做比较所用到的信息没有被修改的话，那么，对该对象调用hashCode方法多次，它必须始终如一地返回
同一个整数。在同一个应用程序的多次执行过程中，这个整数可以不同，即这个应用程序这次执行返回的整数与下一次执行返回的整数可以不一致。
如果两个对象根据equals(Object)方法是相等的，那么调用这两个对象中任一个对象的hashCode方法必须产生同样的整数结果。
如果两个对象根据equals(Object)方法是不相等的，那么调用这两个对象中任一个对象的hashCode方法，不要求必须产生不同的整数结果。然而，程序员应该意识到这样的事实，对于不相等的对象产生截然不同的整数结果，有可能提高散列表（hash
table）的性能。
如果只重写了equals方法而没有重写hashCode方法的话，则会违反约定的第二条： **相等的对象必须具有相等的散列码（hashCode）。** 同时对于HashSet和HashMap这些基于散列值（hash）实现的类。HashMap的底层处理机制是以数组的方法保存放入的数据的(Node
常见的误区
看下面这段代码：

import java.util.HashMap;

public class HashCodeEqual {
    public static void main(String[] args) {
        Apple a1 = new Apple("Blue");
        Apple a2 = new Apple("Green");
        
        HashMap<Apple, Integer> map = new HashMap<>();
        map.put(a1, 10);
        map.put(a2, 20);
        
        System.out.println(map.get(new Apple("Green")));
    }
}

class Apple {
    public String color;
    
    public Apple(String color) {
        this.color = color;
    }
    
    @Override
    public boolean equals(Object obj) {
        if(! (obj instanceof Apple))
            return false;
        if(obj == this)
            return true;
        return this.color.equals(((Apple)obj).color);
    }
}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
我们执行上面这段代码




却发现与我们预想的结果并不一样，我们想取出map中颜色为Green的apple，最后却得到一个null值，这说明map没有我们需要的颜色为green的apple对象，但实际上，我们明明向其中添加了一个颜色为green的apple对象，也重写了equals方法，为什么最后却取不出这个对象呢？
![Upload Paste_Image.png failed. Please try again.]
错误出现的原因
这个问题引起的原因是因为我们没有重写“hashCode”方法，这就需要我们深入理解equals方法和hashCode方法的原理：

如果两个对象是相等的，那么他们必须拥有一样的hashcode，这是第一个前提
如果两个对象有一样的hashcode，但仍不一定相等，因为还需要第二个要求，也就是equals方法的判断。
其实，map判断对象的方法就是先判断hashcode是否相等，如果相等再判断equals方法是否返回true，只有同时满足两个条件，最后才会被认为是相等的。
Map查找元素比线性搜索更快，这是因为map利用hashkey去定位元素，这个定位查找的过程分成两步，内部原理中，map将对象存储在类似数组的数组的区域，所以要经过两个查找，先找到hashcode相等的，然后在再在其中按线性搜索使用equals方法，通过这两部来查找一个对象。





就像上图这个结构，每个hashcode对应一个桶，每个tongli桶里还有多个对象，确定桶的方法是hashCode，在桶中遍历线性查找的方法是equals。
在Object中的默认的hashCode方法的实现是为不同的对象返回不同的hashcode,因此如果我们不重写hashcode方法，那么没有任何两个对象会是相等的，因为object类中的hashcode实现是为不同的对象返回不同的hashcode。
所以，我们就搞清楚了上一段代码出错的原因，由于没有重写hashcode方法，所有的对象都是不一样的，所以我们需要重写hashcode方法，让颜色的对象的hashcode是一样的，比较直接的写法就是直接用color的length作为hashcode。

public int hashCode(){
return this.color.length();
}
1
2
3



** 切记，一定要同时重写hashCode和equals方法 **
Hashmap的结构，1.7和1.8有哪些区别
https://blog.csdn.net/qq_36520235/article/details/82417949
Concurrenthashmap
数据结构？ 源码？如何存储？为什么支持并发？

多线程
为什么要用多线程？
使用多线程的目的
1．在多个CPU核心下，多线程的好处是显而易见的，不然多个CPU核心只跑一个线程其他的核心就都浪费了
2．即便不考虑多核心，在单核下，多线程也是有意义的，因为在一些操作，比如IO操作阻塞的时候，是不需要CPU参与的，这时候CPU就可以另开一个线程去做别的事情，等待IO操作完成再回到之前的线程继续执行即可
线程的生命周期（线程的状态）
线程的生命周期包含5个阶段，包括：新建、就绪、运行、阻塞、销毁。

新建：就是刚使用new方法，new出来的线程；

就绪：就是调用的线程的start()方法后，这时候线程处于等待CPU分配资源阶段，谁先抢的CPU资源，谁开始执行;

运行：当就绪的线程被调度并获得CPU资源时，便进入运行状态，run方法定义了线程的操作和功能;

阻塞：在运行状态的时候，可能因为某些原因导致运行状态的线程变成了阻塞状态，比如sleep()、wait()之后线程就处于了阻塞状态，这个时候需要其他机制将处于阻塞状态的线程唤醒，比如调用notify或者notifyAll()方法。唤醒的线程不会立刻执行run方法，它们要再次等待CPU分配资源进入运行状态;

销毁：如果线程正常执行完毕后或线程被提前强制性的终止或出现异常导致结束，那么线程就要被销毁，释放资源;

完整的生命周期图如下：



Thread的几个重要方法？
a、start()方法，开始执行该线程；
b、stop()方法，强制结束该线程执行；
c、join方法，等待该线程结束。
d、sleep()方法，线程进入等待。
e、run()方法，直接执行线程的run()方法，但是线程调用start()方法时也会运行run()方法，区别就是一个是由线程调度运行run()方法，一个是直接调用了线程中的run()方法！！
wait()和notify()区别？
其实wait()与notify()方法是Object的方法，不是Thread的方法！！同时，wait()与notify()会配合使用，分别表示线程挂起和线程恢复。
wait()与sleep()的区别
wait()与sleep()的区别，简单来说wait()会释放对象锁而sleep()不会释放对象锁。

1.这两个方法来自不同的类分别是Thread和Object  
2.最主要是sleep方法没有释放锁，而wait方法释放了锁，使得其他线程可以使用同步控制块或者方法(锁代码块和方法锁)。  
3.wait，notify和notifyAll只能在同步控制方法或者同步控制块里面使用，而sleep可以在任何地方使用(使用范围)  
4.sleep必须捕获异常，而wait，notify和notifyAll不需要捕获异常  
5.sleep方法属于Thread类中方法，表示让一个线程进入睡眠状态，等待一定的时间之后，自动醒来进入到可运行状态，不会马上进入运行状态，因为线程调度机制恢复线程的运行也需要时间，一个线程对象调用了sleep方法之后，并不会释放他所持有的所有对象锁，所以也就不会影响其他进程对象的运行。但在sleep的过程中过程中有可能被其他对象调用它的interrupt(),产生InterruptedException异常，如果你的程序不捕获这个异常，线程就会异常终止，进入TERMINATED状态，如果你的程序捕获了这个异常，那么程序就会继续执行catch语句块(可能还有finally语句块)以及以后的代码。  
6.注意sleep()方法是一个静态方法，也就是说他只对当前对象有效，通过t.sleep()让t对象进入sleep，这样的做法是错误的，它只会是使当前线程被sleep 而不是t线程  
7. wait属于Object的成员方法，一旦一个对象调用了wait方法，必须要采用notify()和notifyAll()方法唤醒该进程;如果线程拥有某个或某些对象的同步锁，那么在调用了wait()后，这个线程就会释放它持有的所有同步资源，而不限于这个被调用了wait()方法的对象。wait()方法也同样会在wait的过程中有可能被其他对象调用interrupt()方法而产生  
 

yield()和join()
yield方法  
暂停当前正在执行的线程对象。  
yield()方法是停止当前线程，让同等优先权的线程或更高优先级的线程有执行的机会。如果没有的话，那么yield()方法将不会起作用，并且由可执行状态后马上又被执行。   
join方法是用于在某一个线程的执行过程中调用另一个线程执行，等到被调用的线程执行结束后，再继续执行当前线程。如：t.join();//主要用于等待t线程运行结束，若无此句，main则会执行完毕，导致结果不可预测。  

Java多线程实现（四种方法）
1.继承Thread类，重写run方法（其实Thread类本身也实现了Runnable接口）
2.实现Runnable接口，重写run方法
3.实现Callable接口，重写call方法（有返回值）
4.使用线程池（有返回值）

1.继承Thread类，重写run方法
　　每次创建一个新的线程，都要新建一个Thread子类的对象
　　启动线程，new Thread子类（）.start（）
　　创建线程实际调用的是父类Thread空参的构造器

public class MyThread {

    public static void main(String ards[]){
        for(int i=0;i<10;i++){
            new ExtendsThread().start();
        }
        System.out.println(Thread.currentThread().getName());
    }
    
}

class ExtendsThread extends Thread{
    @Override
    public void run() {
        System.out.println(Thread.currentThread().getName());
    }
}

 
2.实现Runnable接口，重写run方法
　　不论创建多少个线程，只需要创建一个Runnable接口实现类的对象
　　启动线程，new Thread（Runnable接口实现类的对象）.start()
 　　创建线程调用的是Thread类Runable类型参数的构造器

public class MyThread {

    public static void main(String ards[]){
        Runnable implRunnable = new ImplRunnable();
        for(int i=0;i<10;i++){
            new Thread(implRunnable).start();
        }
        System.out.println(Thread.currentThread().getName());
    }
    
}

class ImplRunnable implements Runnable{
    private volatile  int i = 0;
    @Override
    public void run() {
        System.out.println(Thread.currentThread().getName()+"--"+ i++);
        
    }
}

 
3.实现Callable接口，重写call方法（有返回值）
　　自定义类实现Callable接口时，必须指定泛型，该泛型即返回值的类型
　　每次创建一个新的线程，都要创建一个新的Callable接口的实现类、
　　如何启动线程？
　　　　（1）创建一个Callable接口的实现类的对象
　　　　（2）创建一个FutureTask对象，传入Callable类型的参数
　　　　　　　　public FutureTask(Callable<V> callable){……}
　　　　（3）调用Thread类重载的参数为Runnable的构造器创建Thread对象
　　　　　　　　将FutureTask作为参数传递
　　　　　　　　public class FutureTask<V> implements RunnableFuture<V>
　　　　　　　　public interface RunnableFuture<V> extends Runnable, Future<V>
　　如何获取返回值？
　　　　调用FutureTask类的get()方法

public class MyThread {

    public static void main(String ards[]) throws InterruptedException, ExecutionException{

        for(int i=0;i<10;i++){
            Callable<Integer> implCallable = new ImplCallable();
            FutureTask<Integer> futureTask = new FutureTask<Integer>(implCallable);
            new Thread(futureTask).start();
            System.out.println(Thread.currentThread().getName()+"----"+futureTask.get());
        }

        System.out.println(Thread.currentThread().getName());
    }
    
}

class ImplCallable implements Callable<Integer>{

    @Override
    public Integer call() throws Exception {
        int result = 0;
        for(int i=0;i<10;i++){
            result += i;
        }
        System.out.println(Thread.currentThread().getName());
        return result;
    }

}

 
4.使用线程池
Executors类
 

/**
 *
 * 线程池
 * 跟数据库连接池类似
 * 避免了线程的创建和销毁造成的额外开销
 *
 * java.util.concurrent
 *
 * Executor    负责现成的使用和调度的根接口
 *    |--ExecutorService    线程池的主要接口
 *          |--ThreadPoolExecutor    线程池的实现类
 *          |--ScheduledExecutorService    接口，负责线程的调度
 *              |--ScheduledThreadPoolExecutor    (extends ThreadPoolExecutor implements ScheduledExecutorService)
 *
 *
 * Executors工具类
 * 提供了创建线程池的方法
 *
 */public class ThreadPool {
    public static void main(String[] args){

        //使用Executors工具类中的方法创建线程池
        ExecutorService pool = Executors.newFixedThreadPool(5);

        ThreadPoolDemo demo = new ThreadPoolDemo();

        //为线程池中的线程分配任务,使用submit方法，传入的参数可以是Runnable的实现类，也可以是Callable的实现类
        for(int i=1;i<=5;i++){
            pool.submit(demo);
        }

        //关闭线程池
        //shutdown ： 以一种平和的方式关闭线程池，在关闭线程池之前，会等待线程池中的所有的任务都结束，不在接受新任务
        //shutdownNow ： 立即关闭线程池        pool.shutdown();


    }
}class ThreadPoolDemo implements Runnable{

    /**多线程的共享数据*/
    private int i = 0;

    @Override
    public void run() {
        while(i<=50){
            System.out.println(Thread.currentThread().getName()+"---"+ i++);
        }
    }
}

 
 

public class ThreadPool2 {
    
    public static void main(String args[]){
        ExecutorService executorService = Executors.newFixedThreadPool(5);
        
        for(int i=0;i<5;i++){
            Future<Integer> future = executorService.submit(new Callable<Integer>() {

                @Override
                public Integer call() throws Exception {
                    int result = 0;
                    for(int i=0;i<=10;i++){
                        result += i;
                    }
                    return result;
                }
            });
            
            try {
                System.out.println(Thread.currentThread().getName()+"--"+future.get());
            } catch (InterruptedException | ExecutionException e) {
                e.printStackTrace();
            }
        }
        
        executorService.shutdown();

    }

}



什么是线程安全和线程不安全
线程安全就是多线程访问时，采用了加锁机制，当一个线程访问该类的某个数据时，进行保护，其他线程不能进行访问直到该线程读取完，其他线程才可使用。不会出现数据不一致或者数据污染。

线程不安全就是不提供数据访问保护，有可能出现多个线程先后更改数据造成所得到的数据是脏数据。
Java 同步与异步-阻塞与非阻塞理解
背景：
对于我们开发的网站，如果网站的访问量非常大的话，那么我们就需要考虑相关的并发访问问题了。而并发问题是绝大部分的程序员头疼的问题，

但话又说回来了，既然逃避不掉，那我们就坦然面对吧~今天就让我们一起来研究一下常见的并发和同步吧。

为了更好的理解并发和同步，我们需要先明白两个重要的概念:同步和异步

Java 中同步与异步，阻塞与非阻塞都是用来形容交互方式，区别在于它们描述的是交互的两个不同层面。

同步与异步
同步与异步更关注交互双方是否可以同时工作。以同步的方式完成任务意味着多个任务的完成次序是串行的，假设任务 A 依赖于任务 B，那么任务 A 必须等到任务 B 完成之后才能继续，执行流程为 A->B；以异步的方式完成任务意味着多个任务的完成可以是并行的，这种情况多适用于任务之间没有因果关系，假如任务 A 中需要执行任务 B，而任务 A 的完成不依赖于任务 B 的结果，那么任务 A 调用任务 B 后可以继续执行后续步骤而不需要等待任务 B 完成，也不关心任务 B 是否执行完毕，此时任务 A 和任务 B 是并行的。

为了加深对同步和异步的理解，可以使用打电话和发短信的类别同步和异步的交互方式。打电话时，一方的后续操作必须等到另一方说完才能进行，这种交互方式就是同步的。发短信则意味着我们不关心对方看到短信后的结果，我们关心自己是否发了短信，发完短信后，我们可以接着手头上的工作，这种交互方式就是异步的。

阻塞与非阻塞
阻塞与非阻塞关注的是交互双方是否可以弹性工作。假设对象 A 和对象 B 进行交互，而对象 B 对一个问题需要思考一段时间才能回复 A，那么对象 A 可以选择等待对象 B 回复，这种方式就是一种阻塞式交互，与此同时，对象 A 可以选择在对象 B 进行思考的时间去完成别的工作，等到对象 B 完成思考后再进行后续交互，这种方式就是一种非阻塞式的交互。

一般来说，阻塞与非阻塞式用来形容 CPU 消耗的。我们把 CPU 停下来等待慢操作完成以后再接着工作称为阻塞；把 CPU 在慢操作完成之前去完成其他工作，等慢操作完成后再接着工作称为非阻塞。


一、关键字： 

thread（线程）、thread-safe(线程安全)、intercurrent（并发的） 

synchronized(同步的)、asynchronized(异步的)、 

volatile（易变的）、atomic（原子的）、share（共享） 

二、总结背景： 

一次读写共享文件编写，嚯，好家伙，竟然揪出这些零碎而又是一路的知识点。于是乎，Google和翻阅了《Java参考大全》、《Effective Java Second Edition》，特此总结一下供日后工作学习参考。 

三、概念： 

1、 什么时候必须同步？什么叫同步？如何同步？ 

       要跨线程维护正确的可见性，只要在几个线程之间共享非 final 变量，就必须使用 synchronized（或 volatile）以确保一个线程可以看见另一个线程做的更改。 

为了在线程之间进行可靠的通信，也为了互斥访问，同步是必须的。这归因于java语言规范的内存模型，它规定了：一个线程所做的变化何时以及如何变成对其它线程可见。 

因为多线程将异步行为引进程序，所以在需要同步时，必须有一种方法强制进行。例如：如果2个线程想要通信并且要共享一个复杂的数据结构，如链表，此时需要确保它们互不冲突，也就是必须阻止B线程在A线程读数据的过程中向链表里面写数据（A获得了锁，B必须等A释放了该锁）。 

为了达到这个目的，java在一个旧的的进程同步模型——监控器（Monitor）的基础上实现了一个巧妙的方案：监控器是一个控制机制，可以认为是一个很小的、只能容纳一个线程的盒子，一旦一个线程进入监控器，其它的线程必须等待，直到那个线程退出监控为止。通过这种方式，一个监控器可以保证共享资源在同一时刻只可被一个线程使用。这种方式称之为同步。（一旦一个线程进入一个实例的任何同步方法，别的线程将不能进入该同一实例的其它同步方法，但是该实例的非同步方法仍然能够被调用）。 

错误的理解：同步嘛，就是几个线程可以同时进行访问。 

同步和多线程关系：没多线程环境就不需要同步;有多线程环境也不一定需要同步。 

锁提供了两种主要特性：互斥（mutual exclusion） 和可见性（visibility）。 

互斥即一次只允许一个线程持有某个特定的锁，因此可使用该特性实现对共享数据的协调访问协议，这样，一次就只有一个线程能够使用该共享数据。 

可见性要更加复杂一些，它必须确保释放锁之前对共享数据做出的更改对于随后获得该锁的另一个线程是可见的 —— 如果没有同步机制提供的这种可见性保证，线程看到的共享变量可能是修改前的值或不一致的值，这将引发许多严重问题 

小结：为了防止多个线程并发对同一数据的修改，所以需要同步，否则会造成数据不一致（就是所谓的：线程安全。如java集合框架中Hashtable和Vector是线程安全的。我们的大部分程序都不是线程安全的，因为没有进行同步，而且我们没有必要，因为大部分情况根本没有多线程环境）。 



2、 什么叫原子的（原子操作）？ 

     Java原子操作是指：不会被打断地的操作。（就是做到互斥 和可见性？！） 

那难道原子操作就可以真的达到线程安全同步效果了吗？实际上有一些原子操作不一定是线程安全的。 

那么，原子操作在什么情况下不是线程安全的呢？也许是这个原因导致的：java线程允许线程在自己的内存区保存变量的副本。允许线程使用本地的私有拷贝进行工作而非每次都使用主存的值是为了提高性能（本人愚见：虽然原子操作是线程安全的，可各线程在得到变量（读操作）后，就是各自玩弄自己的副本了，更新操作（写操作）因未写入主存中，导致其它线程不可见）。 

那该如何解决呢？因此需要通过java同步机制。 

     在java中，32位或者更少位数的赋值是原子的。在一个32位的硬件平台上，除了double和long型的其它原始类型通常都是使用32位进行表示，而double和long通常使用64位表示。另外，对象引用使用本机指针实现，通常也是32位的。对这些32位的类型的操作是原子的。 

     这些原始类型通常使用32位或者64位表示，这又引入了另一个小小的神话：原始类型的大小是由语言保证的。这是不对的。java语言保证的是原始类型的表数范围而非JVM中的存储大小。因此，int型总是有相同的表数范围。在一个JVM上可能使用32位实现，而在另一个JVM上可能是64位的。在此再次强调：在所有平台上被保证的是表数范围，32位以及更小的值的操作是原子的。 

     

3、 不要搞混了：同步、异步 

举个例子：普通B/S模式（同步）AJAX技术（异步） 

同步：提交请求->等待服务器处理->处理完返回 这个期间客户端浏览器不能干任何事 

异步：请求通过事件触发->服务器处理（这是浏览器仍然可以作其他事情）->处理完毕 

可见，彼“同步”非此“同步”——我们说的java中的那个共享数据同步（synchronized） 

一个同步的对象是指行为（动作），一个是同步的对象是指物质（共享数据）。 



4、 Java同步机制有4种实现方式：（部分引用网上资源） 

①    ThreadLocal ② synchronized( ) ③ wait() 与 notify() ④ volatile 

目的：都是为了解决多线程中的对同一变量的访问冲突 
ThreadLocal 
    ThreadLocal 保证不同线程拥有不同实例，相同线程一定拥有相同的实例，即为每一个使用该变量的线程提供一个该变量值的副本，每一个线程都可以独立改变自己的副本，而不是与其它线程的副本冲突。 

优势：提供了线程安全的共享对象 

与其它同步机制的区别：同步机制是为了同步多个线程对相同资源的并发访问，是为了多个线程之间进行通信；而 ThreadLocal 是隔离多个线程的数据共享，从根本上就不在多个线程之间共享资源，这样当然不需要多个线程进行同步了。 

volatile 
     volatile 修饰的成员变量在每次被线程访问时，都强迫从共享内存中重读该成员变量的值。而且，当成员变量发生变化时，强迫线程将变化值回写到共享内存。 
    优势：这样在任何时刻，两个不同的线程总是看到某个成员变量的同一个值。 
    缘由：Java 语言规范中指出，为了获得最佳速度，允许线程保存共享成员变量的私有拷贝，而且只当线程进入或者离开同步代码块时才与共享成员变量的原始值对比。这样当多个线程同时与某个对象交互时，就必须要注意到要让线程及时的得到共享成员变量的变化。而 volatile 关键字就是提示 VM ：对于这个成员变量不能保存它的私有拷贝，而应直接与共享成员变量交互。 
     使用技巧：在两个或者更多的线程访问的成员变量上使用 volatile 。当要访问的变量已在 synchronized 代码块中，或者为常量时，不必使用。 
        线程为了提高效率，将某成员变量(如A)拷贝了一份（如B），线程中对A的访问其实访问的是B。只在某些动作时才进行A和B的同步，因此存在A和B不一致的情况。volatile就是用来避免这种情况的。 volatile告诉jvm，它所修饰的变量不保留拷贝，直接访问主内存中的（读操作多时使用较好；线程间需要通信，本条做不到） 

   Volatile 变量具有 synchronized 的可见性特性，但是不具备原子特性。这就是说线程能够自动发现 volatile 变量的最新值。Volatile 变量可用于提供线程安全，但是只能应用于非常有限的一组用例：多个变量之间或者某个变量的当前值与修改后值之间没有约束。 

            您只能在有限的一些情形下使用 volatile 变量替代锁。要使 volatile 变量提供理想的线程安全，必须同时满足下面两个条件： 

对变量的写操作不依赖于当前值；该变量没有包含在具有其他变量的不变式中。 



sleep() vs wait() 
sleep是线程类（Thread）的方法，导致此线程暂停执行指定时间，把执行机会给其他线程，但是监控状态依然保持，到时后会自动恢复。调用sleep不会释放对象锁。 
wait是Object类的方法，对此对象调用wait方法导致本线程放弃对象锁，进入等待此对象的等待锁定池，只有针对此对象发出notify方法（或notifyAll）后本线程才进入对象锁定池准备获得对象锁进入运行状态。 

（如果变量被声明为volatile，在每次访问时都会和主存一致；如果变量在同步方法或者同步块中被访问，当在方法或者块的入口处获得锁以及方法或者块退出时释放锁时变量被同步。）

java对共享资源变量的同步控制方式？
1.同步代码块，
2.同步方法，
3.或者是用java提供的锁机制
线程池的原理？

从图可以看出，线程池执行所提交的任务过程主要有这样几个阶段：### Synchronized实现原理？

从图可以看出，线程池执行所提交的任务过程主要有这样几个阶段：

先判断线程池中核心线程池所有的线程是否都在执行任务。如果不是，则新创建一个线程执行刚提交的任务，否则，核心线程池中所有的线程都在执行任务，则进入下一步；
判断当前阻塞队列是否已满，如果未满，则将提交的任务放置在阻塞队列中；否则，则进入下一步；
判断线程池中所有的线程是否都在执行任务，如果没有，则创建一个新的线程来执行任务，否则，则交给饱和策略进行处理

源码：
public void execute(Runnable command) {
    if (command == null)
        throw new NullPointerException();
    /*
     * Proceed in 3 steps:
     *
     * 1. If fewer than corePoolSize threads are running, try to
     * start a new thread with the given command as its first
     * task.  The call to addWorker atomically checks runState and
     * workerCount, and so prevents false alarms that would add
     * threads when it shouldn't, by returning false.
     *
     * 2. If a task can be successfully queued, then we still need
     * to double-check whether we should have added a thread
     * (because existing ones died since last checking) or that
     * the pool shut down since entry into this method. So we
     * recheck state and if necessary roll back the enqueuing if
     * stopped, or start a new thread if there are none.
     *
     * 3. If we cannot queue task, then we try to add a new
     * thread.  If it fails, we know we are shut down or saturated
     * and so reject the task.
     */
    int c = ctl.get();
	//如果线程池的线程个数少于corePoolSize则创建新线程执行当前任务
    if (workerCountOf(c) < corePoolSize) {
        if (addWorker(command, true))
            return;
        c = ctl.get();
    }
	//如果线程个数大于corePoolSize或者创建线程失败，则将任务存放在阻塞队列workQueue中
    if (isRunning(c) && workQueue.offer(command)) {
        int recheck = ctl.get();
        if (! isRunning(recheck) && remove(command))
            reject(command);
        else if (workerCountOf(recheck) == 0)
            addWorker(null, false);
    }
	//如果当前任务无法放进阻塞队列中，则创建新的线程来执行任务
    else if (!addWorker(command, false))
        reject(command);
}


ThreadPoolExecutor的execute方法执行逻辑请见注释。下图为ThreadPoolExecutor的execute方法的执行示意图：


execute方法执行逻辑有这样几种情况：

如果当前运行的线程少于corePoolSize，则会创建新的线程来执行新的任务；
如果运行的线程个数等于或者大于corePoolSize，则会将提交的任务存放到阻塞队列workQueue中；
如果当前workQueue队列已满的话，则会创建新的线程来执行任务；
如果线程个数已经超过了maximumPoolSize，则会使用饱和策略RejectedExecutionHandler来进行处理。
需要注意的是，线程池的设计思想就是使用了核心线程池corePoolSize，阻塞队列workQueue和线程池maximumPoolSize，这样的缓存策略来处理任务，实际上这样的设计思想在需要框架中都会使用。
为什么要用线程池（优点）？
在实际使用中，线程是很占用系统资源的，如果对线程管理不善很容易导致系统问题。因此，在大多数并发框架中都会使用线程池来管理线程，使用线程池管理线程主要有如下好处：

降低资源消耗. 通过复用已存在的线程和降低线程关闭的次数来尽可能降低系统性能损耗；
提升系统响应速度。通过复用线程，省去创建线程的过程，因此整体上提升了系统的响应速度；
提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，因此，需要使用线程池来管理线程。
a. 重用存在的线程，减少对象创建、消亡的开销，性能佳。
b. 可有效控制最大并发线程数，提高系统资源的使用率，同时避免过多资源竞争，避免堵塞。
c. 提供定时执行、定期执行、单线程、并发数控制等功能。

使用线程池的风险（缺点）
1．死锁
2．资源不足
3．并发错误
4．线程泄漏
5．请求过载
如何创建线程池
创建线程池主要是ThreadPoolExecutor类来完成，ThreadPoolExecutor的有许多重载的构造方法，通过参数最多的构造方法来理解创建线程池有哪些需要配置的参数。ThreadPoolExecutor的构造方法为：

下面对参数进行说明：

corePoolSize：表示核心线程池的大小。当提交一个任务时，如果当前核心线程池的线程个数没有达到corePoolSize，则会创建新的线程来执行所提交的任务，即使当前核心线程池有空闲的线程。如果当前核心线程池的线程个数已经达到了corePoolSize，则不再重新创建线程。如果调用了prestartCoreThread()或者 prestartAllCoreThreads()，线程池创建的时候所有的核心线程都会被创建并且启动。
maximumPoolSize：表示线程池能创建线程的最大个数。如果当阻塞队列已满时，并且当前线程池线程个数没有超过maximumPoolSize的话，就会创建新的线程来执行任务。
keepAliveTime：空闲线程存活时间。如果当前线程池的线程个数已经超过了corePoolSize，并且线程空闲时间超过了keepAliveTime的话，就会将这些空闲线程销毁，这样可以尽可能降低系统资源消耗。
unit：时间单位。为keepAliveTime指定时间单位。
workQueue：阻塞队列。用于保存任务的阻塞队列，关于阻塞队列可以看这篇文章。可以使用ArrayBlockingQueue, LinkedBlockingQueue, SynchronousQueue, PriorityBlockingQueue。
threadFactory：创建线程的工程类。可以通过指定线程工厂为每个创建出来的线程设置更有意义的名字，如果出现并发问题，也方便查找问题原因。
handler：饱和策略。当线程池的阻塞队列已满和指定的线程都已经开启，说明当前线程池已经处于饱和状态了，那么就需要采用一种策略来处理这种情况。采用的策略有这几种：

AbortPolicy： 直接拒绝所提交的任务，并抛出RejectedExecutionException异常；
CallerRunsPolicy：只用调用者所在的线程来执行任务；
DiscardPolicy：不处理直接丢弃掉任务；
DiscardOldestPolicy：丢弃掉阻塞队列中存放时间最久的任务，执行当前任务


Executors提供四种线程池
newCachedThreadPool	创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。线程池的规模不存在限制。（数量不固定的线程池）
newFixedThreadPool	创建一个固定长度线程池，可控制线程最大并发数，超出的线程会在队列中等待。（固定数量的线程池）
newScheduledThreadPool	创建一个固定长度线程池，支持定时及周期性任务执行。（定时线程池）
newSingleThreadExecutor	创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。（单线程的线程池）

举例：
1、newCachedThreadPool
创建数量不固定的线程池

static ExecutorService	newCachedThreadPool()
          创建一个可根据需要创建新线程的线程池，但是在以前构造的线程可用时将重用它们。
static ExecutorService	newCachedThreadPool(ThreadFactory threadFactory)
          创建一个可根据需要创建新线程的线程池，但是在以前构造的线程可用时将重用它们，并在需要时使用提供的 ThreadFactory 创建新线程。
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
/* 
 * 具有缓冲功能的线程池，系统根据需要创建线程，线程会被缓冲到线程池中
 * 如果线程池大小超过了处理任务所需要的线程线程池就会回收空闲的线程池，
 * 当处理任务增加时，线程池可以增加线程来处理任务线程池不会对线程的大
 * 小进行限制线程池的大小依赖于操作系统
 */
public class Cached {
	public static void main(String[] args){
		ExecutorService ex = Executors.newCachedThreadPool();
		for(int i=0; i<5; i++){
			runnable rn = new runnable();
			ex.execute(rn);
		}
		ex.shutdown();
	}
}
 
class runnable implements Runnable{
	public void run(){
		System.out.println(Thread.currentThread().getName());
	}
}
 

package com.th.threadPool;
 
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
 
/**
 * Created by Administrator on 2018/6/6.
 * newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，
 * 若无可回收，则新建线程。线程池的规模不存在限制。
 */
public class NewCachedThreadPool {
    public static void main(String[] args) {
        ExecutorService cachedThreadPool = Executors.newCachedThreadPool();
        for (int i = 0; i < 10; i++) {
            final int index = i;
            try {
                Thread.sleep(index * 1000);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
 
            //1- 在未来某个时间执行给定的命令。
            // 该命令可能在新的线程、已入池的线程或者正调用的线程中执行，这由 Executor 实现决定。
            cachedThreadPool.execute(new Runnable() {
                @Override
                public void run() {
                    System.out.println(index);
                }
            });
 
            //2- 提交一个 Runnable 任务用于执行，并返回一个表示该任务的 Future。
            // 该 Future 的 get 方法在成功完成时将会返回给定的结果
            cachedThreadPool.submit(new Runnable() {
                @Override
                public void run() {
                    System.out.println(index);
                }
            });
        }
        cachedThreadPool.shutdown();
    }
}
2、newFixedThreadPool
创建固定数量的线程池

static ExecutorService	newFixedThreadPool(int nThreads)
          创建一个可重用固定线程数的线程池，以共享的无界队列方式来运行这些线程。
static ExecutorService	newFixedThreadPool(int nThreads, ThreadFactory threadFactory)
          创建一个可重用固定线程数的线程池，以共享的无界队列方式来运行这些线程，在需要时使用提供的 ThreadFactory 创建新线程。
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
/*
 * 创建具一个可重用的，有固定数量的线程池
 * 每次提交一个任务就提交一个线程，直到线程达到线城池大小，就不会创建新线程了
 * 线程池的大小达到最大后达到稳定不变，如果一个线程异常终止，则会创建新的线程
 */
public class newThreadpool {
	private static int pool = 2;
	public static void main(String[] args){
		ExecutorService ex = Executors.newFixedThreadPool(pool);
		for(int i=0; i<5; i++){
			runnable rn = new runnable();
			ex.execute(rn);
		}
		ex.shutdown();
	}
}
class runnable implements Runnable{
	public void run(){
		System.out.println(Thread.currentThread().getName());
	}
}
 

package com.th.threadPool;
 
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
 
/**
 * Created by Administrator on 2018/6/6.
 *
 * newFixedThreadPool 创建一个固定长度线程池，可控制线程最大并发数，超出的线程会在队列中等待。
 */
public class NewFixedThreadPool {
    public static void main(String[] args) {
        ExecutorService fixedThreadPool = Executors.newFixedThreadPool(2);
        for (int i = 0; i < 10; i++)
        {
            final int index = i;
 
            //1- 在未来某个时间执行给定的命令。
            // 该命令可能在新的线程、已入池的线程或者正调用的线程中执行，这由 Executor 实现决定。
            fixedThreadPool.execute(new Runnable() {
                @Override
                public void run() {
                    threadRunMethod(index);
                }
            });
 
            //2- 提交一个 Runnable 任务用于执行，并返回一个表示该任务的 Future。
            // 该 Future 的 get 方法在成功完成时将会返回给定的结果
            fixedThreadPool.submit(new Runnable() {
                @Override
                public void run() {
                    threadRunMethod(index);
                }
            });
        }
        fixedThreadPool.shutdown();
    }
 
    /**
     *
     * @param index
     */
    private static void threadRunMethod(int index) {
        try {
            System.out.println(index);
            Thread.sleep(2000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }
}
3、newScheduledThreadPool 
创建定时线程池

static ScheduledExecutorService	newScheduledThreadPool(int corePoolSize)
          创建一个线程池，它可安排在给定延迟后运行命令或者定期地执行。
static ScheduledExecutorService	newScheduledThreadPool(int corePoolSize, ThreadFactory threadFactory)
          创建一个线程池，它可安排在给定延迟后运行命令或者定期地执行。
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;
/*
 * 创建一个线程池，大小可以设置，此线程支持定时以及周期性的执行任务定时任务
 */
public class Scheduled {
	private static int pool = 2;
	public static void main(String[] args){
		ScheduledExecutorService ex = Executors.newScheduledThreadPool(pool);
		runnable rn = new runnable();
		//参数1：目标对象   参数2：隔多长时间开始执行线程，    参数3：执行周期       参数4：时间单位
        ex.scheduleAtFixedRate(rn, 3, 1, TimeUnit.MILLISECONDS);
	}
}
class runnable implements Runnable{
	public void run(){
		System.out.println(Thread.currentThread().getName());
	}
}
 

package com.th.threadPool;
 
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;
 
/**
 * Created by Administrator on 2018/6/6.
 * newScheduledThreadPool 创建一个固定长度线程池，支持定时及周期性任务执行。
 */
public class NewScheduledThreadPool {
    public static void main(String[] args) {
        ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(5);
 
        //testSchedule(scheduledExecutorService);
 
        //testScheduleAtFixedRate(scheduledExecutorService);
 
        testScheduleWithFixedDelay(scheduledExecutorService);
 
        // 终止线程池
        //scheduledExecutorService.shutdown();
    }
 
    /**
     *
     * 跟 testScheduleAtFixedRate 非常类似，就是延迟的时间有点区别
     * 创建并执行一个在给定初始延迟后首次启用的定期操作，后续操作具有给定的周期；
     * 也就是将在 initialDelay 后开始执行，然后在 initialDelay+period 后执行，
     * 接着在 initialDelay + 2 * period 后执行，依此类推。
     *
     * 如果任务里面执行的时间大于 period 的时间，下一次的任务会推迟执行。
     * 推迟的时间 ： 等到上次的任务执行完之后再延迟period 的时间后执行。
     * @param scheduledExecutorService
     */
    private static void testScheduleWithFixedDelay(ScheduledExecutorService scheduledExecutorService) {
        scheduledExecutorService.scheduleWithFixedDelay(new Runnable() {
            @Override
            public void run() {
                try {
                    System.out.println("延迟2秒，再3秒执行一次");
                    //如果任务里面执行的时间大于 period 的时间，下一次的任务会推迟执行。
                    //本次任务执行完后下次的任务还需要延迟period时间后再执行
                    Thread.sleep(6*1000);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
            }
        },2,3,TimeUnit.SECONDS);
    }
 
    /**
     * 创建并执行一个在给定初始延迟后首次启用的定期操作，后续操作具有给定的周期；
     * 也就是将在 initialDelay 后开始执行，然后在 initialDelay+period 后执行，
     * 接着在 initialDelay + 2 * period 后执行，依此类推。
     *
     * 如果任务里面执行的时间大于 period 的时间，下一次的任务会推迟执行。
     * 推迟的时间 ： 等到上次的任务执行完就立马执行。
     * @param scheduledExecutorService
     */
    private static void testScheduleAtFixedRate(ScheduledExecutorService scheduledExecutorService) {
        scheduledExecutorService.scheduleAtFixedRate(new Runnable() {
            @Override
            public void run() {
                try {
                    System.out.println("延迟2秒，再3秒执行一次");
                    //如果任务里面执行的时间大于 period 的时间，下一次的任务会推迟执行。
                    //如果任务里面执行的时间大于 period 的时间，本次任务执行完后，下次任务立马执行。
                    Thread.sleep(6*1000);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
 
            }
        },2,3,TimeUnit.SECONDS);
    }
 
    /**
     * 创建并执行在给定延迟后启用的一次性操作
     * @param scheduledExecutorService
     */
    private static void testSchedule(ScheduledExecutorService scheduledExecutorService) {
        scheduledExecutorService.schedule(new Runnable() {
            @Override
            public void run() {
                System.out.println("delay 3 seconds");
            }
        }, 3, TimeUnit.SECONDS);
    }
}
4、newSingleThreadExecutor 
创建单线程的线程池

static ExecutorService	newSingleThreadExecutor()
          创建一个使用单个 worker 线程的 Executor，以无界队列方式来运行该线程。
static ExecutorService	newSingleThreadExecutor(ThreadFactory threadFactory)
          创建一个使用单个 worker 线程的 Executor，以无界队列方式来运行该线程，并在需要时使用提供的 ThreadFactory 创建新线程。
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
/*
 * 创建只有一个线程的线程池
 */
public class Single {
	public static void main(String[] args){
		ExecutorService ex = Executors.newSingleThreadExecutor();
		for(int i=0; i<5; i++){
			runnable rn = new runnable();
			ex.execute(rn);
		}
		ex.shutdown();
	}
}
class runnable implements Runnable{
	public void run(){
		System.out.println(Thread.currentThread().getName());
	}
}
 

package com.th.threadPool;
 
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
 
/**
 * Created by Administrator on 2018/6/6.
 * newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，
 * 保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。
 */
public class NewSingleThreadExecutor {
    public static void main(String[] args) {
 
        ExecutorService singleThreadExecutor = Executors.newSingleThreadExecutor();
        for (int i = 0; i < 10; i++) {
            final int index = i;
            /*singleThreadExecutor.execute(new Runnable() {
                @Override
                public void run() {
                    try {
                        System.out.println("newSingleThreadExecutor: " + index);
                        Thread.sleep(2*1000);
                    } catch (Exception e) {
                        e.printStackTrace();
                    }
                }
            });*/
 
 
            singleThreadExecutor.submit(new Runnable() {
                @Override
                public void run() {
                    try {
                        System.out.println("newSingleThreadExecutor: " + index);
                        Thread.sleep(2*1000);
                    } catch (Exception e) {
                       e.printStackTrace();
                    }
                }
            });
        }
 
        singleThreadExecutor.shutdown();
    }
 
}
 
Synchronized原理
synchronized的底层是使用操作系统的mutex lock实现的。

内存可见性：同步快的可见性是由“如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前需要重新执行load或assign操作初始化变量的值”、“对一个变量执行unlock操作之前，必须先把此变量同步回主内存中（执行store和write操作）”这两条规则获得的。
操作原子性：持有同一个锁的两个同步块只能串行地进入
锁的内存语义：

当线程释放锁时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存中
当线程获取锁时，JMM会把该线程对应的本地内存置为无效。从而使得被监视器保护的临界区代码必须从主内存中读取共享变量
锁释放和锁获取的内存语义：

线程A释放一个锁，实质上是线程A向接下来将要获取这个锁的某个线程发出了（线程A对共享变量所做修改的）消息。
线程B获取一个锁，实质上是线程B接收了之前某个线程发出的（在释放这个锁之前对共享变量所做修改的）消息。
线程A释放锁，随后线程B获取这个锁，这个过程实质上是线程A通过主内存向线程B发送消息


synchronized锁
synchronized用的锁是存在Java对象头里的。
JVM基于进入和退出Monitor对象来实现方法同步和代码块同步。代码块同步是使用monitorenter和monitorexit指令实现的，monitorenter指令是在编译后插入到同步代码块的开始位置，而monitorexit是插入到方法结束处和异常处。任何对象都有一个monitor与之关联，当且一个monitor被持有后，它将处于锁定状态。

根据虚拟机规范的要求，在执行monitorenter指令时，首先要去尝试获取对象的锁，如果这个对象没被锁定，或者当前线程已经拥有了那个对象的锁，把锁的计数器加1；相应地，在执行monitorexit指令时会将锁计数器减1，当计数器被减到0时，锁就释放了。如果获取对象锁失败了，那当前线程就要阻塞等待，直到对象锁被另一个线程释放为止。

注意两点：

1、synchronized同步快对同一条线程来说是可重入的，不会出现自己把自己锁死的问题；

2、同步块在已进入的线程执行完之前，会阻塞后面其他线程的进入。

Mutex Lock
监视器锁（Monitor）本质是依赖于底层的操作系统的Mutex Lock（互斥锁）来实现的。每个对象都对应于一个可称为" 互斥锁" 的标记，这个标记用来保证在任一时刻，只能有一个线程访问该对象。

互斥锁：用于保护临界区，确保同一时间只有一个线程访问数据。对共享资源的访问，先对互斥量进行加锁，如果互斥量已经上锁，调用线程会阻塞，直到互斥量被解锁。在完成了对共享资源的访问后，要对互斥量进行解锁。

mutex的工作方式：




1.申请mutex
2.如果成功，则持有该mutex
3.如果失败，则进行spin自旋. spin的过程就是在线等待mutex, 不断发起mutex gets, 直到获得mutex或者达到spin_count限制为止
4.依据工作模式的不同选择yiled还是sleep
5.若达到sleep限制或者被主动唤醒或者完成yield, 则重复1)~4)步，直到获得为止
由于Java的线程是映射到操作系统的原生线程之上的，如果要阻塞或唤醒一条线程，都需要操作系统来帮忙完成，这就需要从用户态转换到核心态中，因此状态转换需要耗费很多的处理器时间。所以synchronized是Java语言中的一个重量级操作。在JDK1.6中，虚拟机进行了一些优化，譬如在通知操作系统阻塞线程之前加入一段自旋等待过程，避免频繁地切入到核心态中：
synchronized与java.util.concurrent包中的ReentrantLock相比，由于JDK1.6中加入了针对锁的优化措施（见后面），使得synchronized与ReentrantLock的性能基本持平。ReentrantLock只是提供了synchronized更丰富的功能，而不一定有更优的性能，所以在synchronized能实现需求的情况下，优先考虑使用synchronized来进行同步。

Java对象头


在运行期间，Mark Word里存储的数据会随着锁标志位的变化而变化，以32位的JDK为例：


锁优化
偏向锁、轻量级锁、重量级锁
Synchronized是通过对象内部的一个叫做监视器锁（monitor）来实现的，监视器锁本质又是依赖于底层的操作系统的Mutex Lock（互斥锁）来实现的。而操作系统实现线程之间的切换需要从用户态转换到核心态，这个成本非常高，状态之间的转换需要相对比较长的时间，这就是为什么Synchronized效率低的原因。因此，这种依赖于操作系统Mutex Lock所实现的锁我们称之为“重量级锁”。

Java SE 1.6为了减少获得锁和释放锁带来的性能消耗，引入了“偏向锁”和“轻量级锁”：锁一共有4种状态，级别从低到高依次是：无锁状态、偏向锁状态、轻量级锁状态和重量级锁状态。锁可以升级但不能降级。

偏向锁
HotSpot的作者经过研究发现，大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得。偏向锁是为了在只有一个线程执行同步块时提高性能。

当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需简单地测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁。引入偏向锁是为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的获取及释放依赖多次CAS原子指令，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令（由于一旦出现多线程竞争的情况就必须撤销偏向锁，所以偏向锁的撤销操作的性能损耗必须小于节省下来的CAS原子指令的性能消耗）。

偏向锁获取过程：

（1）访问Mark Word中偏向锁的标识是否设置成1，锁标志位是否为01——确认为可偏向状态。
（2）如果为可偏向状态，则测试线程ID是否指向当前线程，如果是，进入步骤（5），否则进入步骤（3）。
（3）如果线程ID并未指向当前线程，则通过CAS操作竞争锁。如果竞争成功，则将Mark Word中线程ID设置为当前线程ID，然后执行（5）；如果竞争失败，执行（4）。
（4）如果CAS获取偏向锁失败，则表示有竞争（CAS获取偏向锁失败说明至少有过其他线程曾经获得过偏向锁，因为线程不会主动去释放偏向锁）。当到达全局安全点（safepoint）时，会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着（因为可能持有偏向锁的线程已经执行完毕，但是该线程并不会主动去释放偏向锁），如果线程不处于活动状态，则将对象头设置成无锁状态（标志位为“01”），然后重新偏向新的线程；如果线程仍然活着，撤销偏向锁后升级到轻量级锁状态（标志位为“00”），此时轻量级锁由原持有偏向锁的线程持有，继续执行其同步代码，而正在竞争的线程会进入自旋等待获得该轻量级锁。
（5）执行同步代码。
偏向锁的释放过程：

如上步骤（4）。偏向锁使用了一种等到竞争出现才释放偏向锁的机制：偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动去释放偏向锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定状态，撤销偏向锁后恢复到未锁定（标志位为“01”）或轻量级锁（标志位为“00”）的状态。

关闭偏向锁：

偏向锁在Java 6和Java 7里是默认启用的。由于偏向锁是为了在只有一个线程执行同步块时提高性能，如果你确定应用程序里所有的锁通常情况下处于竞争状态，可以通过JVM参数关闭偏向锁：-XX:-UseBiasedLocking=false，那么程序默认会进入轻量级锁状态。

轻量级锁
轻量级锁是为了在线程近乎交替执行同步块时提高性能。

轻量级锁的加锁过程：


（1）在代码进入同步块的时候，如果同步对象锁状态为无锁状态（锁标志位为“01”状态，是否为偏向锁为“0”），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝，官方称之为 Displaced Mark Word。这时候线程堆栈与对象头的状态如下图所示。

（2）拷贝对象头中的Mark Word复制到锁记录中。
（3）拷贝成功后，虚拟机将使用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针，并将Lock record里的owner指针指向object mark word。如果更新成功，则执行步骤（3），否则执行步骤（4）。
（4）如果这个更新动作成功了，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位设置为“00”，即表示此对象处于轻量级锁定状态，这时候线程堆栈与对象头的状态如下图所示。



（5）如果这个更新操作失败了，虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果是就说明当前线程已经拥有了这个对象的锁，那就可以直接进入同步块继续执行。否则说明多个线程竞争锁，若当前只有一个等待线程，则可通过自旋稍微等待一下，可能另一个线程很快就会释放锁。 但是当自旋超过一定的次数，或者一个线程在持有锁，一个在自旋，又有第三个来访时，轻量级锁膨胀为重量级锁，重量级锁使除了拥有锁的线程以外的线程都阻塞，防止CPU空转，锁标志的状态值变为“10”，Mark Word中存储的就是指向重量级锁（互斥量）的指针，后面等待锁的线程也要进入阻塞状态。
轻量级锁的解锁过程：

（1）通过CAS操作尝试把线程中复制的Displaced Mark Word对象替换当前的Mark Word。
（2）如果替换成功，整个同步过程就完成了。
（3）如果替换失败，说明有其他线程尝试过获取该锁（此时锁已膨胀），那就要在释放锁的同时，唤醒被挂起的线程。
重量级锁
如上轻量级锁的加锁过程步骤（5），轻量级锁所适应的场景是线程近乎交替执行同步块的情况，如果存在同一时间访问同一锁的情况，就会导致轻量级锁膨胀为重量级锁。Mark Word的锁标记位更新为10，Mark Word指向互斥量（重量级锁）

Synchronized的重量级锁是通过对象内部的一个叫做监视器锁（monitor）来实现的，监视器锁本质又是依赖于底层的操作系统的Mutex Lock（互斥锁）来实现的。而操作系统实现线程之间的切换需要从用户态转换到核心态，这个成本非常高，状态之间的转换需要相对比较长的时间，这就是为什么Synchronized效率低的原因。

（具体见前面的mutex lock）

偏向锁、轻量级锁、重量级锁之间转换




偏向所锁，轻量级锁都是乐观锁，重量级锁是悲观锁。

一个对象刚开始实例化的时候，没有任何线程来访问它的时候。它是可偏向的，意味着，它现在认为只可能有一个线程来访问它，所以当第一个线程来访问它的时候，它会偏向这个线程，此时，对象持有偏向锁。偏向第一个线程，这个线程在修改对象头成为偏向锁的时候使用CAS操作，并将对象头中的ThreadID改成自己的ID，之后再次访问这个对象时，只需要对比ID，不需要再使用CAS在进行操作。
一旦有第二个线程访问这个对象，因为偏向锁不会主动释放，所以第二个线程可以看到对象时偏向状态，这时表明在这个对象上已经存在竞争了。检查原来持有该对象锁的线程是否依然存活，如果挂了，则可以将对象变为无锁状态，然后重新偏向新的线程。如果原来的线程依然存活，则马上执行那个线程的操作栈，检查该对象的使用情况，如果仍然需要持有偏向锁，则偏向锁升级为轻量级锁，（偏向锁就是这个时候升级为轻量级锁的），此时轻量级锁由原持有偏向锁的线程持有，继续执行其同步代码，而正在竞争的线程会进入自旋等待获得该轻量级锁；如果不存在使用了，则可以将对象回复成无锁状态，然后重新偏向。
轻量级锁认为竞争存在，但是竞争的程度很轻，一般两个线程对于同一个锁的操作都会错开，或者说稍微等待一下（自旋），另一个线程就会释放锁。
但是当自旋超过一定的次数，或者一个线程在持有锁，一个在自旋，又有第三个来访时，轻量级锁膨胀为重量级锁，重量级锁使除了拥有锁的线程以外的线程都阻塞，防止CPU空转。
其他锁优化
锁消除
锁消除即删除不必要的加锁操作。虚拟机即时编辑器在运行时，对一些“代码上要求同步，但是被检测到不可能存在共享数据竞争”的锁进行消除。

根据代码逃逸技术，如果判断到一段代码中，堆上的数据不会逃逸出当前线程，那么可以认为这段代码是线程安全的，不必要加锁。

看下面这段程序：

public class SynchronizedTest {

    public static void main(String[] args) {
        SynchronizedTest test = new SynchronizedTest();

        for (int i = 0; i < 100000000; i++) {
            test.append("abc", "def");
        }
    }

    public void append(String str1, String str2) {
        StringBuffer sb = new StringBuffer();
        sb.append(str1).append(str2);
    }
}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
虽然StringBuffer的append是一个同步方法，但是这段程序中的StringBuffer属于一个局部变量，并且不会从该方法中逃逸出去（即StringBuffer sb的引用没有传递到该方法外，不可能被其他线程拿到该引用），所以其实这过程是线程安全的，可以将锁消除。

锁粗化
如果一系列的连续操作都对同一个对象反复加锁和解锁，甚至加锁操作是出现在循环体中的，那即使没有出现线程竞争，频繁地进行互斥同步操作也会导致不必要的性能损耗。

如果虚拟机检测到有一串零碎的操作都是对同一对象的加锁，将会把加锁同步的范围扩展（粗化）到整个操作序列的外部。

举个例子：

public class StringBufferTest {
    StringBuffer stringBuffer = new StringBuffer();

    public void append(){
        stringBuffer.append("a");
        stringBuffer.append("b");
        stringBuffer.append("c");
    }
}
1
2
3
4
5
6
7
8
9
这里每次调用stringBuffer.append方法都需要加锁和解锁，如果虚拟机检测到有一系列连串的对同一个对象加锁和解锁操作，就会将其合并成一次范围更大的加锁和解锁操作，即在第一次append方法时进行加锁，最后一次append方法结束后进行解锁。

自旋锁与自适应自旋锁

引入自旋锁的原因：互斥同步对性能最大的影响是阻塞的实现，因为挂起线程和恢复线程的操作都需要转入内核态中完成，这些操作给系统的并发性能带来很大的压力。同时虚拟机的开发团队也注意到在许多应用上面，共享数据的锁定状态只会持续很短一段时间，为了这一段很短的时间频繁地阻塞和唤醒线程是非常不值得的。

自旋锁：让该线程执行一段无意义的忙循环（自旋）等待一段时间，不会被立即挂起（自旋不放弃处理器额执行时间），看持有锁的线程是否会很快释放锁。自旋锁在JDK
1.4.2中引入，默认关闭，但是可以使用-XX:+UseSpinning开开启；在JDK1.6中默认开启。

自旋锁的缺点：自旋等待不能替代阻塞，虽然它可以避免线程切换带来的开销，但是它占用了处理器的时间。如果持有锁的线程很快就释放了锁，那么自旋的效率就非常好；反之，自旋的线程就会白白消耗掉处理器的资源，它不会做任何有意义的工作，这样反而会带来性能上的浪费。所以说，自旋等待的时间（自旋的次数）必须要有一个限度，例如让其循环10次，如果自旋超过了定义的时间仍然没有获取到锁，则应该被挂起（进入阻塞状态）。通过参数-XX:PreBlockSpin可以调整自旋次数，默认的自旋次数为10。

自适应的自旋锁：JDK1.6引入自适应的自旋锁，自适应就意味着自旋的次数不再是固定的，它是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定：如果在同一个锁的对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功获得过，那在以后要获取这个锁时将可能省略掉自旋过程，以避免浪费处理器资源。简单来说，就是线程如果自旋成功了，则下次自旋的次数会更多，如果自旋失败了，则自旋的次数就会减少。

自旋锁使用场景：从轻量级锁获取的流程中我们知道，当线程在获取轻量级锁的过程中执行CAS操作失败时，是要通过自旋来获取重量级锁的。（见前面“轻量级锁”）

总结
synchronized特点：保证内存可见性、操作原子性
synchronized影响性能的原因：
1、加锁解锁操作需要额外操作；
2、互斥同步对性能最大的影响是阻塞的实现，因为阻塞涉及到的挂起线程和恢复线程的操作都需要转入内核态中完成（用户态与内核态的切换的性能代价是比较大的）
synchronized锁：对象头中的Mark Word根据锁标志位的不同而被复用

偏向锁：在只有一个线程执行同步块时提高性能。Mark
Word存储锁偏向的线程ID，以后该线程在进入和退出同步块时不需要进行CAS操作来加锁和解锁，只需简单比较ThreadID。特点：只有等到线程竞争出现才释放偏向锁，持有偏向锁的线程不会主动释放偏向锁。之后的线程竞争偏向锁，会先检查持有偏向锁的线程是否存活，如果不存货，则对象变为无锁状态，重新偏向；如果仍存活，则偏向锁升级为轻量级锁，此时轻量级锁由原持有偏向锁的线程持有，继续执行其同步代码，而正在竞争的线程会进入自旋等待获得该轻量级锁
轻量级锁：在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，尝试拷贝锁对象目前的Mark Word到栈帧的Lock
Record，若拷贝成功：虚拟机将使用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针，并将Lock
record里的owner指针指向对象的Mark
Word。若拷贝失败：若当前只有一个等待线程，则可通过自旋稍微等待一下，可能持有轻量级锁的线程很快就会释放锁。
但是当自旋超过一定的次数，或者一个线程在持有锁，一个在自旋，又有第三个来访时，轻量级锁膨胀为重量级锁
重量级锁：指向互斥量（mutex），底层通过操作系统的mutex
lock实现。等待锁的线程会被阻塞，由于Linux下Java线程与操作系统内核态线程一一映射，所以涉及到用户态和内核态的切换、操作系统内核态中的线程的阻塞和恢复。
锁的优化
4 锁的优化
从JDK5引入了现代操作系统新增加的CAS原子操作（ JDK5中并没有对synchronized关键字做优化，而是体现在J.U.C中，所以在该版本concurrent包有更好的性能 ），从JDK6开始，就对synchronized的实现机制进行了较大调整，包括使用JDK5引进的CAS自旋之外，还增加了自适应的CAS自旋、锁消除、锁粗化、偏向锁、轻量级锁这些优化策略。由于此关键字的优化使得性能极大提高，同时语义清晰、操作简单、无需手动关闭，所以推荐在允许的情况下尽量使用此关键字，同时在性能上此关键字还有优化的空间。
锁主要存在四种状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁。但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级。
在 JDK 1.6 中默认是开启偏向锁和轻量级锁的，可以通过-XX:-UseBiasedLocking来禁用偏向锁。
4.1 自旋锁
线程的阻塞和唤醒需要CPU从用户态转为核心态，频繁的阻塞和唤醒对CPU来说是一件负担很重的工作，势必会给系统的并发性能带来很大的压力。同时我们发现在许多应用上面，对象锁的锁状态只会持续很短一段时间，为了这一段很短的时间频繁地阻塞和唤醒线程是非常不值得的。
所以引入自旋锁，何谓自旋锁？
所谓自旋锁，就是指当一个线程尝试获取某个锁时，如果该锁已被其他线程占用，就一直循环检测锁是否被释放，而不是进入线程挂起或睡眠状态。
自旋锁适用于锁保护的临界区很小的情况，临界区很小的话，锁占用的时间就很短。自旋等待不能替代阻塞，虽然它可以避免线程切换带来的开销，但是它占用了CPU处理器的时间。如果持有锁的线程很快就释放了锁，那么自旋的效率就非常好，反之，自旋的线程就会白白消耗掉处理的资源，它不会做任何有意义的工作，典型的占着茅坑不拉屎，这样反而会带来性能上的浪费。所以说，自旋等待的时间（自旋的次数）必须要有一个限度，如果自旋超过了定义的时间仍然没有获取到锁，则应该被挂起。
自旋锁在JDK 1.4.2中引入，默认关闭，但是可以使用-XX:+UseSpinning开开启，在JDK1.6中默认开启。同时自旋的默认次数为10次，可以通过参数-XX:PreBlockSpin来调整。
如果通过参数-XX:PreBlockSpin来调整自旋锁的自旋次数，会带来诸多不便。假如将参数调整为10，但是系统很多线程都是等你刚刚退出的时候就释放了锁（假如多自旋一两次就可以获取锁），是不是很尴尬。于是JDK1.6引入自适应的自旋锁，让虚拟机会变得越来越聪明。
4.2 适应性自旋锁
JDK 1.6引入了更加聪明的自旋锁，即自适应自旋锁。所谓自适应就意味着自旋的次数不再是固定的，它是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。那它如何进行适应性自旋呢？
线程如果自旋成功了，那么下次自旋的次数会更加多，因为虚拟机认为既然上次成功了，那么此次自旋也很有可能会再次成功，那么它就会允许自旋等待持续的次数更多。反之，如果对于某个锁，很少有自旋能够成功，那么在以后要或者这个锁的时候自旋的次数会减少甚至省略掉自旋过程，以免浪费处理器资源。
有了自适应自旋锁，随着程序运行和性能监控信息的不断完善，虚拟机对程序锁的状况预测会越来越准确，虚拟机会变得越来越聪明。
4.3 锁消除
为了保证数据的完整性，在进行操作时需要对这部分操作进行同步控制，但是在有些情况下，JVM检测到不可能存在共享数据竞争，这是JVM会对这些同步锁进行锁消除。
锁消除的依据是逃逸分析的数据支持
如果不存在竞争，为什么还需要加锁呢？所以锁消除可以节省毫无意义的请求锁的时间。变量是否逃逸，对于虚拟机来说需要使用数据流分析来确定，但是对于程序员来说这还不清楚么？在明明知道不存在数据竞争的代码块前加上同步吗？但是有时候程序并不是我们所想的那样？虽然没有显示使用锁，但是在使用一些JDK的内置API时，如StringBuffer、Vector、HashTable等，这个时候会存在隐形的加锁操作。比如StringBuffer的append()方法，Vector的add()方法：
public void vectorTest(){
    Vector<String> vector = new Vector<String>();
    for(int i = 0 ; i < 10 ; i++){
        vector.add(i + "");
    }

    System.out.println(vector);}
在运行这段代码时，JVM可以明显检测到变量vector没有逃逸出方法vectorTest()之外，所以JVM可以大胆地将vector内部的加锁操作消除。
4.4 锁粗化
在使用同步锁的时候，需要让同步块的作用范围尽可能小—仅在共享数据的实际作用域中才进行同步，这样做的目的是 为了使需要同步的操作数量尽可能缩小，如果存在锁竞争，那么等待锁的线程也能尽快拿到锁。
在大多数的情况下，上述观点是正确的。但是如果一系列的连续加锁解锁操作，可能会导致不必要的性能损耗，所以引入锁粗话的概念。
锁粗话概念比较好理解，就是将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁
如上面实例：
vector每次add的时候都需要加锁操作，JVM检测到对同一个对象（vector）连续加锁、解锁操作，会合并一个更大范围的加锁、解锁操作，即加锁解锁操作会移到for循环之外。
4.5 偏向锁
偏向锁是JDK6中的重要引进，因为HotSpot作者经过研究实践发现，在大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低，引进了偏向锁。
偏向锁是在单线程执行代码块时使用的机制，如果在多线程并发的环境下（即线程A尚未执行完同步代码块，线程B发起了申请锁的申请），则一定会转化为轻量级锁或者重量级锁。
在JDK5中偏向锁默认是关闭的，而到了JDK6中偏向锁已经默认开启。如果并发数较大同时同步代码块执行时间较长，则被多个线程同时访问的概率就很大，就可以使用参数-XX:-UseBiasedLocking来禁止偏向锁(但这是个JVM参数，不能针对某个对象锁来单独设置)。
引入偏向锁主要目的是：为了在没有多线程竞争的情况下尽量减少不必要的轻量级锁执行路径。因为轻量级锁的加锁解锁操作是需要依赖多次CAS原子指令的，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令（由于一旦出现多线程竞争的情况就必须撤销偏向锁，所以偏向锁的撤销操作的性能损耗也必须小于节省下来的CAS原子指令的性能消耗）。
轻量级锁是为了在线程交替执行同步块时提高性能，而偏向锁则是在只有一个线程执行同步块时进一步提高性能。
那么偏向锁是如何来减少不必要的CAS操作呢？首先我们看下无竞争下锁存在什么问题：
现在几乎所有的锁都是可重入的，即已经获得锁的线程可以多次锁住/解锁监视对象，按照之前的HotSpot设计，每次加锁/解锁都会涉及到一些CAS操作（比如对等待队列的CAS操作），CAS操作会延迟本地调用，因此偏向锁的想法是 一旦线程第一次获得了监视对象，之后让监视对象“偏向”这个线程，之后的多次调用则可以避免CAS操作，说白了就是置个变量，如果发现为true则无需再走各种加锁/解锁流程。
CAS为什么会引入本地延迟？这要从SMP（对称多处理器）架构说起，下图大概表明了SMP的结构：

SMP（对称多处理器）架构
其意思是 所有的CPU会共享一条系统总线（BUS），靠此总线连接主存。每个核都有自己的一级缓存，各核相对于BUS对称分布，因此这种结构称为“对称多处理器”。
而CAS的全称为Compare-And-Swap，是一条CPU的原子指令，其作用是让CPU比较后原子地更新某个位置的值，经过调查发现，其实现方式是基于硬件平台的汇编指令，就是说CAS是靠硬件实现的，JVM只是封装了汇编调用，那些AtomicInteger类便是使用了这些封装后的接口。
例如：Core1和Core2可能会同时把主存中某个位置的值Load到自己的L1 Cache中，当Core1在自己的L1 Cache中修改这个位置的值时，会通过总线，使Core2中L1 Cache对应的值“失效”，而Core2一旦发现自己L1 Cache中的值失效（称为Cache命中缺失）则会通过总线从内存中加载该地址最新的值，大家通过总线的来回通信称为“Cache一致性流量”，因为总线被设计为固定的“通信能力”，如果Cache一致性流量过大，总线将成为瓶颈。而当Core1和Core2中的值再次一致时，称为“Cache一致性”，从这个层面来说，锁设计的终极目标便是减少Cache一致性流量。
而CAS恰好会导致Cache一致性流量，如果有很多线程都共享同一个对象，当某个Core CAS成功时必然会引起总线风暴，这就是所谓的本地延迟，本质上偏向锁就是为了消除CAS，降低Cache一致性流量。
Cache一致性：
上面提到Cache一致性，其实是有协议支持的，现在通用的协议是MESI（最早由Intel开始支持），具体参考：http://en.wikipedia.org/wiki/MESI_protocol
Cache一致性流量的例外情况：
其实也不是所有的CAS都会导致总线风暴，这跟Cache一致性协议有关，具体参考：
http://blogs.oracle.com/dave/entry/biased_locking_in_hotspot

NUMA(Non Uniform Memory Access Achitecture）
架构：
与SMP对应还有非对称多处理器架构，现在主要应用在一些高端处理器上，主要特点是没有总线，没有公用主存，每个Core有自己的内存，针对这种结构此处不做讨论。
所以，当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，以后该线程进入和退出同步块时不需要花费CAS操作来争夺锁资源，只需要检查是否为偏向锁、锁标识为以及ThreadID即可，处理流程如下：
1．检测Mark Word是否为可偏向状态，即是否为偏向锁1，锁标识位为01；
2．若为可偏向状态，则测试线程ID是否为当前线程ID，如果是，则执行步骤（5），否则执行步骤（3）；
3．如果测试线程ID不为当前线程ID，则通过CAS操作竞争锁，竞争成功，则将Mark Word的线程ID替换为当前线程ID，否则执行线程（4）；
4．通过CAS竞争锁失败，证明当前存在多线程竞争情况，当到达全局安全点，获得偏向锁的线程被挂起，偏向锁升级为轻量级锁，然后被阻塞在安全点的线程继续往下执行同步代码块；
5．执行同步代码块；
偏向锁的释放采用了 一种只有竞争才会释放锁的机制，线程是不会主动去释放偏向锁，需要等待其他线程来竞争。偏向锁的撤销需要 等待全局安全点（这个时间点是上没有正在执行的代码）。其步骤如下：
1暂停拥有偏向锁的线程；
2判断锁对象是否还处于被锁定状态，否，则恢复到无锁状态（01），以允许其余线程竞争。是，则挂起持有锁的当前线程，并将指向当前线程的锁记录地址的指针放入对象头Mark Word，升级为轻量级锁状态（00），然后恢复持有锁的当前线程，进入轻量级锁的竞争模式；

注意：此处将 当前线程挂起再恢复的过程中并没有发生锁的转移，仍然在当前线程手中，只是穿插了个 “将对象头中的线程ID变更为指向锁记录地址的指针” 这么个事。

偏向锁的获取和释放过程
4.6 轻量级锁
引入轻量级锁的主要目的是 在没有多线程竞争的前提下，减少传统的重量级锁使用操作系统互斥量产生的性能消耗。当关闭偏向锁功能或者多个线程竞争偏向锁导致偏向锁升级为轻量级锁，则会尝试获取轻量级锁，其步骤如下：

1在线程进入同步块时，如果同步对象锁状态为无锁状态（锁标志位为“01”状态，是否为偏向锁为“0”），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝，官方称之为 Displaced Mark Word。此时线程堆栈与对象头的状态如下图所示：





2拷贝对象头中的Mark Word复制到锁记录（Lock Record）中；


3拷贝成功后，虚拟机将使用CAS操作尝试将对象Mark Word中的Lock Word更新为指向当前线程Lock Record的指针，并将Lock record里的owner指针指向object mark word。如果更新成功，则执行步骤（4），否则执行步骤（5）；


4如果这个更新动作成功了，那么当前线程就拥有了该对象的锁，并且对象Mark Word的锁标志位设置为“00”，即表示此对象处于轻量级锁定状态，此时线程堆栈与对象头的状态如下图所示：




5如果这个更新操作失败了，虚拟机首先会检查对象Mark Word中的Lock Word是否指向当前线程的栈帧，如果是，就说明当前线程已经拥有了这个对象的锁，那就可以直接进入同步块继续执行。否则说明多个线程竞争锁，进入自旋执行（3），若自旋结束时仍未获得锁，轻量级锁就要膨胀为重量级锁，锁标志的状态值变为“10”，Mark Word中存储的就是指向重量级锁（互斥量）的指针，当前线程以及后面等待锁的线程也要进入阻塞状态。

轻量级锁的释放也是通过CAS操作来进行的，主要步骤如下：
1.通过CAS操作尝试把线程中复制的Displaced Mark Word对象替换当前的Mark Word；
2.如果替换成功，整个同步过程就完成了，恢复到无锁状态（01）；
3.如果替换失败，说明有其他线程尝试过获取该锁（此时锁已膨胀），那就要在释放锁的同时，唤醒被挂起的线程；


对于轻量级锁，其性能提升的依据是 “对于绝大部分的锁，在整个生命周期内都是不会存在竞争的”，如果打破这个依据则除了互斥的开销外，还有额外的CAS操作，因此在有多线程竞争的情况下，轻量级锁比重量级锁更慢。



为什么升级为轻量锁时要把对象头里的Mark Word复制到线程栈的锁记录中呢？

因为在申请对象锁时 需要以该值作为CAS的比较条件，同时在升级到重量级锁的时候，能通过这个比较判定是否在持有锁的过程中此锁被其他线程申请过，如果被其他线程申请了，则在释放锁的时候要唤醒被挂起的线程。


为什么会尝试CAS不成功以及什么情况下会不成功？

CAS本身是不带锁机制的，其是通过比较而来。假设如下场景：线程A和线程B都在对象头里的锁标识为无锁状态进入，那么如线程A先更新对象头为其锁记录指针成功之后，线程B再用CAS去更新，就会发现此时的对象头已经不是其操作前的对象HashCode了，所以CAS会失败。也就是说，只有两个线程并发申请锁的时候会发生CAS失败。

然后线程B进行CAS自旋，等待对象头的锁标识重新变回无锁状态或对象头内容等于对象HashCode（因为这是线程B做CAS操作前的值），这也就意味着线程A执行结束（参见后面轻量级锁的撤销，只有线程A执行完毕撤销锁了才会重置对象头），此时线程B的CAS操作终于成功了，于是线程B获得了锁以及执行同步代码的权限。如果线程A的执行时间较长，线程B经过若干次CAS时钟没有成功，则锁膨胀为重量级锁，即线程B被挂起阻塞、等待重新调度。

此处，如何理解“轻量级”？“轻量级”是相对于使用操作系统互斥量来实现的传统锁而言的。但是，首先需要强调一点的是，轻量级锁并不是用来代替重量级锁的，它的本意是在没有多线程竞争的前提下，减少传统的重量级锁使用产生的性能消耗。
轻量级锁所适应的场景是线程交替执行同步块的情况，如果存在同一时间访问同一锁的情况，必然就会导致轻量级锁膨胀为重量级锁。
4.7 重量级锁
Synchronized是通过对象内部的一个叫做 监视器锁（Monitor）来实现的。但是监视器锁本质又是依赖于底层的操作系统的Mutex Lock来实现的。而操作系统实现线程之间的切换这就需要从用户态转换到核心态，这个成本非常高，状态之间的转换需要相对比较长的时间，这就是为什么Synchronized效率低的原因。因此，这种依赖于操作系统Mutex Lock所实现的锁我们称之为 “重量级锁”。
4.8 重量级锁、轻量级锁和偏向锁之间转换






参考：https://www.jianshu.com/p/e62fa839aa41

锁类型
1.可重入锁（synchronized和ReentrantLock）：在执行对象中所有同步方法不用再次获得锁

2.可中断锁（synchronized就不是可中断锁，而Lock是可中断锁）：在等待获取锁过程中可中断

3.公平锁（ReentrantLock和ReentrantReadWriteLock）： 按等待获取锁的线程的等待时间进行获取，等待时间长的具有优先获取锁权利

4.读写锁（ReadWriteLock和ReentrantReadWriteLock）：对资源读取和写入的时候拆分为2部分处理，读的时候可以多线程一起读，写的时候必须同步地写



Synchronized三种用法
首先我们了解到Java中的线程同步锁可以是任意对象。
这里我们介绍synchronized的三种应用方式：
1.作用于实例方法，当前实例加锁，进入同步代码前要获得当前实例的锁；
2.作用于静态方法，当前类加锁，进去同步代码前要获得当前类对象的锁；
3.作用于代码块，这需要指定加锁的对象，对所给的指定对象加锁，进入同步代码前要获得指定对象的锁。
这三种应用方式接下来分别介绍
1．synchronized修饰实例方法（普通方法）
  使用时，作用范围为整个函数，这里所谓的实例锁就是调用该实例方法（不包括静态方法）的对象。不多BB，上代码：
【demo1】
public class SyncTest implements Runnable{
    //共享资源变量
    int count = 0;

    @Override
    public synchronized void run() {
        for (int i = 0; i < 5; i++) {
            increaseCount();
            System.out.println(Thread.currentThread().getName()+":"+count++);
        }
    }

    public static void main(String[] args) throws InterruptedException {
        SyncTest syncTest1 = new SyncTest();//        SyncTest syncTest2 = new SyncTest();
        Thread thread1 = new Thread(syncTest1,"thread1");
        Thread thread2 = new Thread(syncTest1, "thread2");
        thread1.start();
        thread2.start();
    }}
 /**
     * 输出结果
     thread1:0
     thread1:1
     thread1:2
     thread1:3
     thread1:4
     thread2:5
     thread2:6
     thread2:7
     thread2:8
     thread2:9
     */
  代码中开启了两个线程去操作一个变量（共享变量）,count++是先读取值，再写回一个新值。我们想一下，如果第一个线程执行这一过程中，第二个线程拿到写回之前的count值，做count++操作，那么这就造成了线程不安全。所以这里在run方法加上synchronized，获取一个对象锁，代码中的实例锁就是syncTest1了。
  同时我们从输出结果看出：当一个线程正在访问一个对象synchronized实例方法时，别的线程是访问不了的。一个对象一把锁说的就是这个，当线程获取了该对象的锁后，其他线程无法获取该对象的锁，当然就访问不了该对象的synchronized方法，但是！但是！但是！可以访问该对象的其他未被synchronized修饰的方法。
  如果是一个线程 A 需要访问实例对象 obj1 的 synchronized 方法 f1(当前对象锁是obj1)，另一个线程 B 需要访问实例对象 obj2 的 synchronized 方法 f2(当前对象锁是obj2)，这样是允许的，因为两个实例对象锁并不同相同，此时如果两个线程操作数据并非共享的，线程安全是有保障的，遗憾的是如果两个线程操作的是共享数据，那么线程安全就有可能无法保证了。我们把上面代码中的main方法中的注释放开，表达这一线程不安全的现象
【demo2】
public class SyncTest implements Runnable{
    //共享资源变量
    int count = 0;

    @Override
    public synchronized void run() {
        for (int i = 0; i < 5; i++) {
            System.out.println(Thread.currentThread().getName()+":"+count++);
        }
    }

    public static void main(String[] args) throws InterruptedException {
        SyncTest syncTest1 = new SyncTest();
        SyncTest syncTest2 = new SyncTest();
        Thread thread1 = new Thread(syncTest1,"thread1");
        Thread thread2 = new Thread(syncTest2, "thread2");
        thread1.start();
        thread2.start();
    }
    /**
     * 输出结果
        thread1:0
        thread2:0
        thread1:1
        thread2:1
        thread1:2
        thread2:2
        thread1:3
        thread2:3
        thread1:4
        thread2:4
     */}
  我们从输出结果来看，两个线程可能同时拿到共享变量去做count++操作。上述操作中虽然我们的run方法还是使用synchronized修饰，但是我们new了两个实例。这就意味存在了两个不同的实例锁，thread1和thread2分别进入了syncTest1和syncTest2的实例锁，当然保证不了线程安全。但是我们也有解决方案啦：如果synchronized修饰的是静态方法呢？下面我们再介绍修饰静态方法。
2．synchronized修饰静态方法
  我们知道静态方法是不属于当前实例的，而是属性类的，那么这个锁就是类的class对象锁，上述问题引刃而解，请看代码：
【demo3】
public class SyncTest implements Runnable {
    //共享资源变量
    static int count = 0;

    @Override
    public synchronized void run() {
        increaseCount();
    }

    private synchronized static void increaseCount() {
        for (int i = 0; i < 5; i++) {
            System.out.println(Thread.currentThread().getName() + ":" + count++);
            try {
                Thread.sleep(1000);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }
    }

    public static void main(String[] args) throws InterruptedException {
        SyncTest syncTest1 = new SyncTest();
        SyncTest syncTest2 = new SyncTest();
        Thread thread1 = new Thread(syncTest1, "thread1");
        Thread thread2 = new Thread(syncTest2, "thread2");
        thread1.start();
        thread2.start();
    }
    /**
     * 输出结果
     thread1:0
     thread1:1
     thread1:2
     thread1:3
     thread1:4
     thread2:5
     thread2:6
     thread2:7
     thread2:8
     thread2:9
     */}
  瞧瞧输出结果，问题解决了没？同样是new了两个不同实例，却保持了线程同步。那是我们synchronizd修饰的是静态方法，run方法中调用这个静态方法，再说一次 静态方法不属于当前实例，而是属于类。所以这个方案其实是用的一个把锁，而这个锁就是这个类的class对象锁。
  需要注意的是如果一个线程A调用一个实例对象的非static synchronized方法，而线程B需要调用这个实例对象所属类的静态 synchronized方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的class对象，而访问非静态 synchronized 方法占用的锁是当前实例对象锁(结合demo2，demo3)。
3．synchronized修饰代码块
  首先这个使用时的场景是：在某些情况下，我们编写的方法体可能比较大，同时存在一些比较耗时的操作，而需要同步的代码又只有一小部分，如果直接对整个方法进行同步操作，可能会得不偿失，此时我们可以使用同步代码块的方式对需要同步的代码进行包裹，这样就无需对整个方法进行同步操作了。所以他的作用范围为synchronizd（obj）{}的这个大括号中
【demo4】
public class SyncTest implements Runnable {
    //共享资源变量
    static int count = 0;
    private byte[] mBytes = new byte[0];

    @Override
    public synchronized void run() {
        increaseCount();
    }

    private void increaseCount() {
        //假设省略了其他操作的代码。
        //……………………
        synchronized (this) {
            for (int i = 0; i < 5; i++) {
                System.out.println(Thread.currentThread().getName() + ":" + count++);
                try {
                    Thread.sleep(1000);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
            }
        }
    }

    public static void main(String[] args) throws InterruptedException {
        SyncTest syncTest1 = new SyncTest();
        SyncTest syncTest2 = new SyncTest();
        Thread thread1 = new Thread(syncTest1, "thread1");
        Thread thread2 = new Thread(syncTest2, "thread2");
        thread1.start();
        thread2.start();
    }
    /**
     * 输出结果
     thread1:0
     thread2:0
     thread1:1
     thread2:2
     thread2:4
     thread1:3
     thread2:5
     thread1:5
     thread2:7
     thread1:6
     */}
  从输出结果看出，这个demo并没有保证线程安全，因为我们指定锁为this，指的就是调用这个方法的实例对象。这里我们new了两个不同的实例对象syncTest1，syncTest2，所以有两个锁，thread1与thread2分别进入自己传入的对象锁的线程执行increaseCount方法，做成线程不安全。如果把demo4的成员变量注释放开，并将mBytes传入synchronized后面的括号中，也是线程不安全的结果。这里之所以加上mBytes这个对象是为了说明synchronized后面的括号中是可以指定任意对象充当锁的，而零长度的byte数组对象创建起来将比任何对象都经济。当然，如果要使用这个经济实惠的锁并保证线程安全，那就不能new出多个不同实例对象出来啦。如果你非要想new两个不同对象出来，又想保证线程同步的话，那么synchronized后面的括号中可以填入SyncTest.class，表示这个类对象作为锁，自然就能保证线程同步啦。使用方法为：
synchronized(xxxx.class){
  //todo}
4．总结

修饰普通方法 一个对象中的加锁方法只允许一个线程访问。但要注意这种情况下锁的是访问该方法的实例对象， 如果多个线程不同对象访问该方法，则无法保证同步。


修饰静态方法 由于静态方法是类方法， 所以这种情况下锁的是包含这个方法的类，也就是类对象；这样如果多个线程不同对象访问该静态方法，也是可以保证同步的。


修饰代码块 其中普通代码块 如Synchronized（obj） 这里的obj 可以为类中的一个属性、也可以是当前的对象，它的同步效果和修饰普通方法一样；Synchronized方法 （obj.class）静态代码块它的同步效果和修饰静态方法类似。



作者：c_ychao
链接：https://www.jianshu.com/p/27f5935cafd8
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

Synchronized与lock区别？

并发编程的锁机制：synchronized和lock
并发编程中，锁是经常需要用到的，今天我们一起来看下Java中的锁机制：synchronized和lock。

锁的种类
锁的种类挺多，包括：自旋锁、自旋锁的其他种类、阻塞锁、可重入锁、读写锁、互斥锁、悲观锁、乐观锁、公平锁、可重入锁等等，其余就不列出了。我们这边重点看如下几种：可重入锁、读写锁、可中断锁、公平锁。
1.1 可重入锁
如果锁具备可重入性，则称作为可重入锁。synchronized和ReentrantLock都是可重入锁，可重入性在我看来实际上表明了锁的分配机制：基于线程的分配，而不是基于方法调用的分配。举比如说，当一个线程执行到method1 的synchronized方法时，而在method1中会调用另外一个synchronized方法method2，此时该线程不必重新去申请锁，而是可以直接执行方法method2。

1.2 读写锁
读写锁将对一个资源的访问分成了2个锁，如文件，一个读锁和一个写锁。正因为有了读写锁，才使得多个线程之间的读操作不会发生冲突。ReadWriteLock就是读写锁，它是一个接口，ReentrantReadWriteLock实现了这个接口。可以通过readLock()获取读锁，通过writeLock()获取写锁。

1.3 可中断锁
可中断锁，即可以中断的锁。在Java中，synchronized就不是可中断锁，而Lock是可中断锁。 如果某一线程A正在执行锁中的代码，另一线程B正在等待获取该锁，可能由于等待时间过长，线程B不想等待了，想先处理其他事情，我们可以让它中断自己或者在别的线程中中断它，这种就是可中断锁。

Lock接口中的lockInterruptibly()方法就体现了Lock的可中断性。

1.4 公平锁
公平锁即尽量以请求锁的顺序来获取锁。同时有多个线程在等待一个锁，当这个锁被释放时，等待时间最久的线程（最先请求的线程）会获得该锁，这种就是公平锁。

非公平锁即无法保证锁的获取是按照请求锁的顺序进行的，这样就可能导致某个或者一些线程永远获取不到锁。

synchronized是非公平锁，它无法保证等待的线程获取锁的顺序。对于ReentrantLock和ReentrantReadWriteLock，默认情况下是非公平锁，但是可以设置为公平锁。

synchronized和lock的用法
2.1 synchronized
synchronized是Java的关键字，当它用来修饰一个方法或者一个代码块的时候，能够保证在同一时刻最多只有一个线程执行该段代码。简单总结如下四种用法。
2.1.1 代码块
对某一代码块使用，synchronized后跟括号，括号里是变量，一次只有一个线程进入该代码块。

public int synMethod(int m){
    synchronized(m) {
     //...
    }
 }
1
2
3
4
5
2.1.2 方法声明时
方法声明时使用，放在范围操作符之后,返回类型声明之前。即一次只能有一个线程进入该方法，其他线程要想在此时调用该方法，只能排队等候。

public synchronized void synMethod() {
   //...
}
1
2
3
2.1.3 synchronized后面括号里是对象
synchronized后面括号里是一对象，此时线程获得的是对象锁。

public void test() {
  synchronized (this) {
      //...
  }
}
1
2
3
4
5
2.1.4 synchronized后面括号里是类
synchronized后面括号里是类，如果线程进入，则线程在该类中所有操作不能进行，包括静态变量和静态方法，对于含有静态方法和静态变量的代码块的同步，通常使用这种方式。

2.2 Lock
Lock接口主要相关的类和接口如下。

Lock
Lock
ReadWriteLock是读写锁接口，其实现类为ReetrantReadWriteLock。ReetrantLock实现了Lock接口。

2.2.1 Lock
Lock中有如下方法：

public interface Lock {
	void lockInterruptibly() throws InterruptedException;  
	boolean tryLock();  
	boolean tryLock(long time, TimeUnit unit) throws InterruptedException;  
	void unlock();  
	Condition newCondition();
}
1
2
3
4
5
6
7
lock：用来获取锁，如果锁被其他线程获取，处于等待状态。如果采用Lock，必须主动去释放锁，并且在发生异常时，不会自动释放锁。因此一般来说，使用Lock必须在try{}catch{}块中进行，并且将释放锁的操作放在finally块中进行，以保证锁一定被被释放，防止死锁的发生。
lockInterruptibly：通过这个方法去获取锁时，如果线程正在等待获取锁，则这个线程能够响应中断，即中断线程的等待状态。
tryLock：tryLock方法是有返回值的，它表示用来尝试获取锁，如果获取成功，则返回true，如果获取失败（即锁已被其他线程获取），则返回false，也就说这个方法无论如何都会立即返回。在拿不到锁时不会一直在那等待。
tryLock（long，TimeUnit）：与tryLock类似，只不过是有等待时间，在等待时间内获取到锁返回true，超时返回false。
unlock：释放锁，一定要在finally块中释放
2.2.2 ReetrantLock
实现了Lock接口，可重入锁，内部定义了公平锁与非公平锁。默认为非公平锁：

public ReentrantLock() {  
  sync = new NonfairSync();  
} 
1
2
3
可以手动设置为公平锁：

public ReentrantLock(boolean fair) {  
  sync = fair ? new FairSync() : new NonfairSync();  
}  
1
2
3
2.2.3 ReadWriteLock

public interface ReadWriteLock {  
    Lock readLock();       //获取读锁  
    Lock writeLock();      //获取写锁  
}  
1
2
3
4
一个用来获取读锁，一个用来获取写锁。也就是说将文件的读写操作分开，分成2个锁来分配给线程，从而使得多个线程可以同时进行读操作。ReentrantReadWirteLock实现了ReadWirteLock接口，并未实现Lock接口。 不过要注意的是：

如果有一个线程已经占用了读锁，则此时其他线程如果要申请写锁，则申请写锁的线程会一直等待释放读锁。

如果有一个线程已经占用了写锁，则此时其他线程如果申请写锁或者读锁，则申请的线程会一直等待释放写锁。

2.2.4 ReetrantReadWriteLock
ReetrantReadWriteLock同样支持公平性选择，支持重进入，锁降级。

public class RWLock {
    static Map<String, Object> map = new HashMap<String, Object>();
    static ReentrantReadWriteLock rwLock = new ReentrantReadWriteLock();
    static Lock r = rwLock.readLock();
    static Lock w = rwLock.writeLock();
    //读
    public static final Object get(String key){
        r.lock();
        try {
            return map.get(key);
        } finally {
            r.unlock();
        }
    }
    //写
    public static final Object put(String key, Object value){
        w.lock();
        try {
            return map.put(key, value);
        } finally {
            w.unlock();
        }
    }
}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
只需在读操作时获取读锁，写操作时获取写锁。当写锁被获取时，后续的读写操作都会被阻塞，写锁释放后，所有操作继续执行。

3. 两种锁的比较
3.1 synchronized和lock的区别
①　Lock是一个接口，而synchronized是Java中的关键字，synchronized是内置的语言实现；
②　synchronized在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；而Lock在发生异常时，如果没有主动通过unLock()去释放锁，则很可能造成死锁现象，因此使用Lock时需要在finally块中释放锁；
③　Lock可以让等待锁的线程响应中断，而synchronized却不行，使用synchronized时，等待的线程会一直等待下去，不能够响应中断；
④　通过Lock可以知道有没有成功获取锁，而synchronized却无法办到。
⑤　Lock可以提高多个线程进行读操作的效率。（可以通过readwritelock实现读写分离）
⑥　性能上来说，在资源竞争不激烈的情形下，Lock性能稍微比synchronized差点（编译程序通常会尽可能的进行优化synchronized）。但是当同步非常激烈的时候，synchronized的性能一下子能下降好几十倍。而ReentrantLock确还能维持常态。
3.2 性能比较
下面对synchronized与Lock进行性能测试，分别开启10个线程，每个线程计数到1000000，统计两种锁同步所花费的时间。网上也能找到这样的例子。

public class TestAtomicIntegerLock {

    private static int synValue;

    public static void main(String[] args) {
        int threadNum = 10;
        int maxValue = 1000000;
        testSync(threadNum, maxValue);
        testLocck(threadNum, maxValue);
    }
	//test synchronized
    public static void testSync(int threadNum, int maxValue) {
        Thread[] t = new Thread[threadNum];
        Long begin = System.nanoTime();
        for (int i = 0; i < threadNum; i++) {
            Lock locks = new ReentrantLock();
            synValue = 0;
            t[i] = new Thread(() -> {

                for (int j = 0; j < maxValue; j++) {
                    locks.lock();
                    try {
                        synValue++;
                    } finally {
                        locks.unlock();
                    }
                }

            });
        }
        for (int i = 0; i < threadNum; i++) {
            t[i].start();
        }
        //main线程等待前面开启的所有线程结束
        for (int i = 0; i < threadNum; i++) {
            try {
                t[i].join();
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }
        System.out.println("使用lock所花费的时间为：" + (System.nanoTime() - begin));
    }
	// test Lock
    public static void testLocck(int threadNum, int maxValue) {
        int[] lock = new int[0];
        Long begin = System.nanoTime();
        Thread[] t = new Thread[threadNum];
        for (int i = 0; i < threadNum; i++) {
            synValue = 0;
            t[i] = new Thread(() -> {
                for (int j = 0; j < maxValue; j++) {
                    synchronized(lock) {
                        ++synValue;
                    }
                }
            });
        }
        for (int i = 0; i < threadNum; i++) {
            t[i].start();
        }
        //main线程等待前面开启的所有线程结束
        for (int i = 0; i < threadNum; i++) {
            try {
                t[i].join();
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }

        System.out.println("使用synchronized所花费的时间为：" + (System.nanoTime() - begin));
    }

}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
测试结果的差异还是比较明显的，Lock的性能明显高于synchronized。本次测试基于JDK1.8。

使用lock所花费的时间为：436667997
使用synchronized所花费的时间为：616882878
1
2
JDK1.5中，synchronized是性能低效的。因为这是一个重量级操作，它对性能最大的影响是阻塞的是实现，挂起线程和恢复线程的操作都需要转入内核态中完成，这些操作给系统的并发性带来了很大的压力。相比之下使用Java提供的Lock对象，性能更高一些。多线程环境下，synchronized的吞吐量下降的非常严重，而ReentrankLock则能基本保持在同一个比较稳定的水平上。

到了JDK1.6，发生了变化，对synchronize加入了很多优化措施，有自适应自旋，锁消除，锁粗化，轻量级锁，偏向锁等等。导致在JDK1.6上synchronize的性能并不比Lock差。官方也表示，他们也更支持synchronize，在未来的版本中还有优化余地，所以还是提倡在synchronized能实现需求的情况下，优先考虑使用synchronized来进行同步。

4. 总结
本文主要对并发编程中的锁机制synchronized和lock，进行详解。synchronized是基于JVM实现的，内置锁，Java中的每一个对象都可以作为锁。对于同步方法，锁是当前实例对象。对于静态同步方法，锁是当前对象的Class对象。对于同步方法块，锁是Synchonized括号里配置的对象。Lock是基于在语言层面实现的锁，Lock锁可以被中断，支持定时锁。Lock可以提高多个线程进行读操作的效率。通过对比得知，Lock的效率是明显高于synchronized关键字的，一般对于数据结构设计或者框架的设计都倾向于使用Lock而非Synchronized。
lock类型
Lock类型
 
 
一、公平锁/非公平锁
 
公平锁是指多个线程按照申请锁的顺序来获取锁。
非公平锁是指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。有可能，会造成优先级反转或者饥饿现象。
对于ReentrantLock而言，通过构造函数指定该锁是否是公平锁，默认是非公平锁。非公平锁的优点在于吞吐量比公平锁大。
对于Synchronized而言，也是一种非公平锁。由于其并不像ReentrantLock是通过AQS的来实现线程调度，所以并没有任何办法使其变成公平锁。
二、可重入锁
 
可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁。
说的有点抽象，下面会有一个代码的示例。
对于Java ReentrantLock而言, 他的名字就可以看出是一个可重入锁，其名字是Re entrant Lock重新进入锁。
对于Synchronized而言,也是一个可重入锁。可重入锁的一个好处是可一定程度避免死锁。

 

synchronized void setA() throws Exception{


 

 


 

    Thread.sleep(1000);


 

 


 

    setB();


 

 


 

}


 

 


 

synchronized void setB() throws Exception{


 

 


 

    Thread.sleep(1000);


 

 


 

}

三、独享锁/共享锁
独享锁是指该锁一次只能被一个线程所持有。
共享锁是指该锁可被多个线程所持有。
对于Java ReentrantLock而言，其是独享锁。但是对于Lock的另一个实现类ReadWriteLock，其读锁是共享锁，其写锁是独享锁。
读锁的共享锁可保证并发读是非常高效的，读写，写读 ，写写的过程是互斥的。
独享锁与共享锁也是通过AQS来实现的，通过实现不同的方法，来实现独享或者共享。
对于Synchronized而言，当然是独享锁。
四、互斥锁/读写锁
上面讲的独享锁/共享锁就是一种广义的说法，互斥锁/读写锁就是具体的实现。
互斥锁在Java中的具体实现就是ReentrantLock
读写锁在Java中的具体实现就是ReadWriteLock
五、乐观锁/悲观锁
乐观锁与悲观锁不是指具体的什么类型的锁，而是指看待并发同步的角度。
悲观锁认为对于同一个数据的并发操作，一定是会发生修改的，哪怕没有修改，也会认为修改。因此对于同一个数据的并发操作，悲观锁采取加锁的形式。悲观的认为，不加锁的并发操作一定会出问题。
乐观锁则认为对于同一个数据的并发操作，是不会发生修改的。在更新数据的时候，会采用尝试更新，不断重新的方式更新数据。乐观的认为，不加锁的并发操作是没有事情的。
从上面的描述我们可以看出，悲观锁适合写操作非常多的场景，乐观锁适合读操作非常多的场景，不加锁会带来大量的性能提升。
悲观锁在Java中的使用，就是利用各种锁。
乐观锁在Java中的使用，是无锁编程，常常采用的是CAS算法，典型的例子就是原子类，通过CAS自旋实现原子操作的更新。
六、分段锁
分段锁其实是一种锁的设计，并不是具体的一种锁，对于ConcurrentHashMap而言，其并发的实现就是通过分段锁的形式来实现高效的并发操作。
我们以ConcurrentHashMap来说一下分段锁的含义以及设计思想，ConcurrentHashMap中的分段锁称为Segment，它即类似于HashMap(JDK7与JDK8中HashMap的实现)的结构，即内部拥有一个Entry数组，数组中的每个元素又是一个链表;同时又是一个ReentrantLock(Segment继承了ReentrantLock)。
当需要put元素的时候，并不是对整个hashmap进行加锁，而是先通过hashcode来知道他要放在那一个分段中，然后对这个分段进行加锁，所以当多线程put的时候，只要不是放在一个分段中，就实现了真正的并行的插入。
但是，在统计size的时候，可就是获取hashmap全局信息的时候，就需要获取所有的分段锁才能统计。
分段锁的设计目的是细化锁的粒度，当操作不需要更新整个数组的时候，就仅仅针对数组中的一项进行加锁操作。
 
七、偏向锁/轻量级锁/重量级锁
 
这三种锁是指锁的状态，并且是针对Synchronized。在Java 5通过引入锁升级的机制来实现高效Synchronized。这三种锁的状态是通过对象监视器在对象头中的字段来表明的。
偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁。降低获取锁的代价。
轻量级锁是指当锁是偏向锁的时候，被另一个线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，提高性能。
重量级锁是指当锁为轻量级锁的时候，另一个线程虽然是自旋，但自旋不会一直持续下去，当自旋一定次数的时候，还没有获取到锁，就会进入阻塞，该锁膨胀为重量级锁。重量级锁会让其他申请的线程进入阻塞，性能降低。
八、自旋锁
 
在Java中，自旋锁是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁，这样的好处是减少线程上下文切换的消耗，缺点是循环会消耗CPU。
线程自旋和适应性自旋 

我们知道，java线程其实是映射在内核之上的，线程的挂起和恢复会极大的影响开销。
并且jdk官方人员发现，很多线程在等待锁的时候，在很短的一段时间就获得了锁，所以它们在线程等待的时候，并不需要把线程挂起，而是让他无目的的循环，一般设置10次。
这样就避免了线程切换的开销，极大的提升了性能。 

而适应性自旋，是赋予了自旋一种学习能力，它并不固定自旋10次一下。
他可以根据它前面线程的自旋情况，从而调整它的自旋，甚至是不经过自旋而直接挂起。

volatile原理
1、引言

在多线程并发编程中synchronized和Volatile都扮演着重要的角色，Volatile是轻量级的synchronized，它在多处理器开发中保证了共享变量的“可见性”。可见性的意思是当一个线程修改一个共享变量时，另外一个线程能读到这个修改的值。它在某些情况下比synchronized的开销更小，下面我们将深入分析Voliate的实现原理。

2、Volatile定义

java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一致的更新，线程应该确保通过排他锁单独获得这个变量。Java语言提供了volatile，在某些情况下比锁更加方便。如果一个字段被声明成volatile，java线程内存模型确保所有线程看到这个变量的值是一致的。volatile可以保证线程可见性且提供了一定的有序性，但是无法保证原子性。在JVM底层volatile是采用“内存屏障”来实现的。

即一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义：

（1）保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。保证可见性、不保证原子性。
（2）禁止进行指令重排序。

关于这两层语义的详细情况，如果不是很理解，可以看博文：并发编程——原子性，可见性和有序性

3、为什么要使用Volatile

Volatile变量修饰符如果使用恰当的话，它比synchronized的使用和执行成本会更低，因为它不会引起线程上下文的切换和调度。

4、Voliate保证可见性

先看一段代码，假如线程1先执行，线程2后执行：

//线程1
boolean stop = false;
while(!stop){
    doSomething();
}

//线程2
stop = true;
1
2
3
4
5
6
7
8
这段代码是很典型的一段代码，很多人在中断线程时可能都会采用这种标记办法。但是事实上，这段代码会完全运行正确么？即一定会将线程中断么？不一定，也许在大多数时候，这个代码能够把线程中断，但是也有可能会导致无法中断线程（虽然这个可能性很小，但是只要一旦发生这种情况就会造成死循环了）。下面解释一下这段代码为何有可能导致无法中断线程。

前面文章 Java内存模型 中我们说过，每个线程在运行过程中都有自己的工作内存，那么线程1在运行的时候，会将stop变量的值拷贝一份放在自己的工作内存当中。那么当线程2更改了stop变量的值之后，但是还没来得及写入主存当中，线程2转去做其他事情了，那么线程1由于不知道线程2对stop变量的更改，因此还会一直循环下去。

但是用volatile修饰之后就变得不一样了：

第一：使用volatile关键字会强制将修改的值立即写入主存；

第二：使用volatile关键字的话，当线程2进行修改时，会导致线程1的工作内存中缓存变量stop的缓存行无效（反映到硬件层的话，就是CPU的L1或者L2缓存中对应的缓存行无效）；

第三：由于线程1的工作内存中缓存变量stop的缓存行无效，所以线程1再次读取变量stop的值时会去主存读取。

那么在线程2修改stop值时（当然这里包括2个操作，修改线程2工作内存中的值，然后将修改后的值写入内存），会使得线程1的工作内存中缓存变量stop的缓存行无效，然后线程1读取时，发现自己的缓存行无效，它会等待缓存行对应的主存地址被更新之后，然后去对应的主存读取最新的值。那么线程1读取到的就是最新的正确的值。
5、volatile不能保证原子性

Volatile不保证对变量的操作是原子性，下面看一个例子：

public class Test {
    public volatile int inc = 0;

    public void increase() {
        inc++;
    }

    public static void main(String[] args) {
        final Test test = new Test();
        for(int i=0;i<10;i++){
            new Thread(){
                public void run() {
                    for(int j=0;j<1000;j++)
                        test.increase();
                };
            }.start();
        }

        while(Thread.activeCount()>1)  //保证前面的线程都执行完
            Thread.yield();
        System.out.println(test.inc);
    }
}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
上面程序的输出有人认为是10000。但是事实上运行它会发现每次运行结果都不一致，都是一个小于10000的数字。可能有人就会有疑问，不对啊，上面是对变量inc进行自增操作，由于volatile保证了可见性，那么在每个线程中对inc自增完之后，在其他线程中都能看到修改后的值啊，所以有10个线程分别进行了1000次操作，那么最终inc的值应该是1000*10=10000。

这里面就有一个误区了，volatile关键字能保证可见性没有错，但是上面的程序错在没能保证原子性。可见性只能保证每次读取的是最新的值，但是volatile没办法保证对变量的操作的原子性。在前面已经提到过，自增操作是不具备原子性的，它包括读取变量的原始值、进行加1操作、写入工作内存。那么就是说自增操作的三个子操作可能会分割开执行，就有可能导致下面这种情况出现：

（1）假如某个时刻变量inc的值为10，线程1对变量进行自增操作，线程1先读取了变量inc的原始值，然后线程1被阻塞了；

（2）然后线程2对变量进行自增操作，线程2也去读取变量inc的原始值，由于线程1只是对变量inc进行读取操作，而没有对变量进行修改操作，所以不会导致线程2的工作内存中缓存变量inc的缓存行无效，所以线程2会直接去主存读取inc的值，发现inc的值时10，然后进行加1操作，并把11写入工作内存，最后写入主存。

（3）然后线程1接着进行加1操作，由于已经读取了inc的值，注意此时在线程1的工作内存中inc的值仍然为10，所以线程1对inc进行加1操作后inc的值为11，然后将11写入工作内存，最后写入主存。

（4）那么两个线程分别进行了一次自增操作后，inc只增加了1。

解释到这里，可能有朋友会有疑问，不对啊，前面不是保证一个变量在修改volatile变量时，会让缓存行无效吗？然后其他线程去读就会读到新的值，对，这个没错。这个就是上面的happens-before规则中的volatile变量规则，但是要注意，线程1对变量进行读取操作之后，被阻塞了的话，并没有对inc值进行修改。然后虽然volatile能保证线程2对变量inc的值读取是从内存中读取的，但是线程1没有进行修改，所以线程2根本就不会看到修改的值。

根源就在这里，自增操作不是原子性操作，而且volatile也无法保证对变量的任何操作都是原子性的。把上面的代码改成以下任何一种都可以达到效果：

5.1 采用synchronized

public class Test {
    public  int inc = 0;

    public synchronized void increase() {
        inc++;
    }

    public static void main(String[] args) {
        final Test test = new Test();
        for(int i=0;i<10;i++){
            new Thread(){
                public void run() {
                    for(int j=0;j<1000;j++)
                        test.increase();
                };
            }.start();
        }

        while(Thread.activeCount()>1)  //保证前面的线程都执行完
            Thread.yield();
        System.out.println(test.inc);
    }
}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
5.2 采用Lock

public class Test {
    public  int inc = 0;
    Lock lock = new ReentrantLock();

    public  void increase() {
        lock.lock();
        try {
            inc++;
        } finally{
            lock.unlock();
        }
    }

    public static void main(String[] args) {
        final Test test = new Test();
        for(int i=0;i<10;i++){
            new Thread(){
                public void run() {
                    for(int j=0;j<1000;j++)
                        test.increase();
                };
            }.start();
        }

        while(Thread.activeCount()>1)  //保证前面的线程都执行完
            Thread.yield();
        System.out.println(test.inc);
    }
}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
5.3 采用AtomicInteger

public class Test {
    public  AtomicInteger inc = new AtomicInteger();

    public  void increase() {
        inc.getAndIncrement();
    }

    public static void main(String[] args) {
        final Test test = new Test();
        for(int i=0;i<10;i++){
            new Thread(){
                public void run() {
                    for(int j=0;j<1000;j++)
                        test.increase();
                };
            }.start();
        }

        while(Thread.activeCount()>1)  //保证前面的线程都执行完
            Thread.yield();
        System.out.println(test.inc);
    }
}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
在java 1.5的java.util.concurrent.atomic包下提供了一些原子操作类，即对基本数据类型的 自增（加1操作），自减（减1操作）、以及加法操作（加一个数），减法操作（减一个数）进行了封装，保证这些操作是原子性操作。

6、volatile一定程度保证有序性

在前面提到volatile关键字能禁止指令重排序，所以volatile能在一定程度上保证有序性。volatile关键字禁止指令重排序有两层意思：

（1）当程序执行到volatile变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见；在其后面的操作肯定还没有进行；

（2）在进行指令优化时，不能将在对volatile变量访问的语句放在其后面执行，也不能把volatile变量后面的语句放到其前面执行。

可能上面说的比较绕，举个简单的例子：

//x、y为非volatile变量
//flag为volatile变量

x = 2;        //语句1
y = 0;        //语句2
flag = true;  //语句3
x = 4;         //语句4
y = -1;       //语句5
1
2
3
4
5
6
7
8
由于flag变量为volatile变量，那么在进行指令重排序的过程的时候，不会将语句3放到语句1、语句2前面，也不会讲语句3放到语句4、语句5后面。但是要注意语句1和语句2的顺序、语句4和语句5的顺序是不作任何保证的。并且volatile关键字能保证，执行到语句3时，语句1和语句2必定是执行完毕了的，且语句1和语句2的执行结果对语句3、语句4、语句5是可见的。那么我们回到前面举的一个例子：

//线程1:
context = loadContext();   //语句1
inited = true;             //语句2

//线程2:
while(!inited ){
  sleep()
}
doSomethingwithconfig(context);
1
2
3
4
5
6
7
8
9
前面举这个例子的时候，提到有可能语句2会在语句1之前执行，那么就可能导致context还没被初始化，而线程2中就使用未初始化的context去进行操作，导致程序出错。这里如果用volatile关键字对inited变量进行修饰，就不会出现这种问题了，因为当执行到语句2时，必定能保证context已经初始化完毕。

7、volatile的实现机制和原理

7.1 实现机制

前面讲述了源于volatile关键字的一些使用，下面我们来探讨一下volatile到底如何保证可见性和禁止指令重排序的。在x86处理器下通过工具获取JIT编译器生成的汇编指令来看看对Volatile进行写操作CPU会做什么事情。

Java代码: instance = new Singleton();//instance是volatile变量
汇编代码:  0x01a3de1d: movb $0x0,0x1104800(%esi);0x01a3de24: lock addl $0x0,(%esp);
1
2
观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令。lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能：

（1）它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成；

（2）它会强制将对缓存的修改操作立即写入主内存；

（3）如果是写操作，它会导致其他CPU中对应的缓存行无效。

7.2 实现原理

7.2.1 可见性

处理器为了提高处理速度，不直接和内存进行通讯，而是将系统内存的数据独到内部缓存后再进行操作，但操作完后不知什么时候会写到内存。

如果对声明了volatile变量进行写操作时，JVM会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写会到系统内存。这一步确保了如果有其他线程对声明了volatile变量进行修改，则立即更新主内存中数据。

但这时候其他处理器的缓存还是旧的，所以在多处理器环境下，为了保证各个处理器缓存一致，每个处理会通过嗅探在总线上传播的数据来检查 自己的缓存是否过期，当处理器发现自己缓存行对应的内存地址被修改了，就会将当前处理器的缓存行设置成无效状态，当处理器要对这个数据进行修改操作时，会强制重新从系统内存把数据读到处理器缓存里。 这一步确保了其他线程获得的声明了volatile变量都是从主内存中获取最新的。

7.2.2 有序性

Lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成。

8、使用volatile关键字的场景

synchronized关键字是防止多个线程同时执行一段代码，那么就会很影响程序执行效率，而volatile关键字在某些情况下性能要优于synchronized，但是要注意volatile关键字是无法替代synchronized关键字的，因为volatile关键字无法保证操作的原子性。通常来说，使用volatile必须具备以下2个条件：

（1）对变量的写操作不依赖于当前值
（2）该变量没有包含在具有其他变量的不变式中

实际上，这些条件表明，可以被写入volatile变量的这些有效值独立于任何程序的状态，包括变量的当前状态。即实际就是上面的2个条件需要保证操作是原子性操作，才能保证使用volatile关键字的程序在并发时能够正确执行。下面列举Java中使用volatile的几个场景。

8.1 状态标记量

volatile boolean inited = false;
//线程1:
context = loadContext();  
inited = true;            

//线程2:
while(!inited ){
    sleep();
}
doSomethingwithconfig(context);
1
2
3
4
5
6
7
8
9
10
8.2 double check（单例模式）

class Singleton{
    private volatile static Singleton instance = null;

    private Singleton() {

    }

    public static Singleton getInstance() {
        if(instance == null) {
            synchronized (Singleton.class) {
                if(instance == null)
                    instance = new Singleton();
            }
        }
        return instance;
    }
}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
这里为什么要使用volatile修饰instance？主要在于instance = new Singleton()这句，这并非是一个原子操作，事实上在JVM中这句话大概做了下面3件事情:

（1）给instance分配内存

（2）调用Singleton的构造函数来初始化成员变量

（3）将instance对象指向分配的内存空间（执行完这步instance就为非null了）。

但是在JVM的即时编译器中存在指令重排序的优化。也就是说上面的第二步和第三步的顺序是不能保证的，最终的执行顺序可能是 1-2-3 也可能是 1-3-2。如果是后者，则在3执行完毕、2未执行之前，被线程二抢占了，这时instance已经是非null了（但却没有初始化），所以线程二会直接返回instance，然后使用，然后顺理成章地报错。

9、Volatile的使用优化

在使用Volatile变量时，用一种追加字节的方式来优化队列出队和入队的性能。追加字节能优化性能？这种方式看起来很神奇，但如果深入理解处理器架构就能理解其中的奥秘。让我们先来看看LinkedTransferQueue这个类，它使用一个内部类类型来定义队列的头队列（Head）和尾节点（tail），而这个内部类PaddedAtomicReference相对于父类AtomicReference只做了一件事情，就将共享变量追加到64字节。我们可以来计算下，一个对象的引用占4个字节，它追加了15个变量共占60个字节，再加上父类的Value变量，一共64个字节。

/** head of the queue */
private transient final PaddedAtomicReference<QNode> head;

/** tail of the queue */
private transient final PaddedAtomicReference<QNode> tail;

static final class PaddedAtomicReference <T> extends AtomicReference <T> {

    // enough padding for 64bytes with 4byte refs
    Object p0, p1, p2, p3, p4, p5, p6, p7, p8, p9, pa, pb, pc, pd, pe;

    PaddedAtomicReference(T r) {
        super(r);
    }
}

public class AtomicReference <V> implements java.io.Serializable {

    private volatile V value;
  //省略其他代码
}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
为什么追加64字节能够提高并发编程的效率呢？ 因为对于英特尔酷睿i7，酷睿， Atom和NetBurst， Core Solo和Pentium M处理器的L1，L2或L3缓存的高速缓存行是64个字节宽，不支持部分填充缓存行，这意味着如果队列的头节点和尾节点都不足64字节的话，处理器会将它们都读到同一个高速缓存行中，在多处理器下每个处理器都会缓存同样的头尾节点，当一个处理器试图修改头接点时会将整个缓存行锁定，那么在缓存一致性机制的作用下，会导致其他处理器不能访问自己高速缓存中的尾节点，而队列的入队和出队操作是需要不停修改头接点和尾节点，所以在多处理器的情况下将会严重影响到队列的入队和出队效率。Doug lea使用追加到64字节的方式来填满高速缓冲区的缓存行，避免头接点和尾节点加载到同一个缓存行，使得头尾节点在修改时不会互相锁定。

那么是不是在使用Volatile变量时都应该追加到64字节呢？不是的。在两种场景下不应该使用这种方式。

（1）缓存行非64字节宽的处理器，如P6系列和奔腾处理器，它们的L1和L2高速缓存行是32个字节宽。
（2）共享变量不会被频繁的写。因为使用追加字节的方式需要处理器读取更多的字节到高速缓冲区，这本身就会带来一定的性能消耗，共享变量如果不被频繁写的话，锁的几率也非常小，就没必要通过追加字节的方式来避免相互锁定
————————————————
Java中常用的锁机制有那些？
一、公平锁/非公平锁

公平锁是指多个线程按照申请锁的顺序来获取锁。

非公平锁是指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。有可能，会造成优先级反转或者饥饿现象。

对于Java ReentrantLock而言，通过构造函数指定该锁是否是公平锁，默认是非公平锁。非公平锁的优点在于吞吐量比公平锁大。

对于Synchronized而言，也是一种非公平锁。由于其并不像ReentrantLock是通过AQS的来实现线程调度，所以并没有任何办法使其变成公平锁。

二、可重入锁

可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁。说的有点抽象，下面会有一个代码的示例。

对于Java ReentrantLock而言, 他的名字就可以看出是一个可重入锁，其名字是Re entrant Lock重新进入锁。

对于Synchronized而言,也是一个可重入锁。可重入锁的一个好处是可一定程度避免死锁。

synchronized void setA() throws Exception{

    Thread.sleep(1000);

    setB();

}

synchronized void setB() throws Exception{

    Thread.sleep(1000);

}
1
2
3
4
5
6
7
8
9
10
11
12
13
上面的代码就是一个可重入锁的一个特点，如果不是可重入锁的话，setB可能不会被当前线程执行，可能造成死锁。

三、独享锁/共享锁

独享锁是指该锁一次只能被一个线程所持有。

共享锁是指该锁可被多个线程所持有。

对于Java ReentrantLock而言，其是独享锁。但是对于Lock的另一个实现类ReadWriteLock，其读锁是共享锁，其写锁是独享锁。

读锁的共享锁可保证并发读是非常高效的，读写，写读 ，写写的过程是互斥的。

独享锁与共享锁也是通过AQS来实现的，通过实现不同的方法，来实现独享或者共享。

对于Synchronized而言，当然是独享锁。

四、互斥锁/读写锁

上面讲的独享锁/共享锁就是一种广义的说法，互斥锁/读写锁就是具体的实现。

互斥锁在Java中的具体实现就是ReentrantLock

读写锁在Java中的具体实现就是ReadWriteLock
五、乐观锁/悲观锁

乐观锁与悲观锁不是指具体的什么类型的锁，而是指看待并发同步的角度。

悲观锁认为对于同一个数据的并发操作，一定是会发生修改的，哪怕没有修改，也会认为修改。因此对于同一个数据的并发操作，悲观锁采取加锁的形式。悲观的认为，不加锁的并发操作一定会出问题。

乐观锁则认为对于同一个数据的并发操作，是不会发生修改的。在更新数据的时候，会采用尝试更新，不断重新的方式更新数据。乐观的认为，不加锁的并发操作是没有事情的。

从上面的描述我们可以看出，悲观锁适合写操作非常多的场景，乐观锁适合读操作非常多的场景，不加锁会带来大量的性能提升。

悲观锁在Java中的使用，就是利用各种锁。

乐观锁在Java中的使用，是无锁编程，常常采用的是CAS算法，典型的例子就是原子类，通过CAS自旋实现原子操作的更新。

六、分段锁

分段锁其实是一种锁的设计，并不是具体的一种锁，对于ConcurrentHashMap而言，其并发的实现就是通过分段锁的形式来实现高效的并发操作。

我们以ConcurrentHashMap来说一下分段锁的含义以及设计思想，ConcurrentHashMap中的分段锁称为Segment，它即类似于HashMap(JDK7与JDK8中HashMap的实现)的结构，即内部拥有一个Entry数组，数组中的每个元素又是一个链表;同时又是一个ReentrantLock(Segment继承了ReentrantLock)。

当需要put元素的时候，并不是对整个hashmap进行加锁，而是先通过hashcode来知道他要放在那一个分段中，然后对这个分段进行加锁，所以当多线程put的时候，只要不是放在一个分段中，就实现了真正的并行的插入。

但是，在统计size的时候，可就是获取hashmap全局信息的时候，就需要获取所有的分段锁才能统计。

分段锁的设计目的是细化锁的粒度，当操作不需要更新整个数组的时候，就仅仅针对数组中的一项进行加锁操作。

七、偏向锁/轻量级锁/重量级锁

这三种锁是指锁的状态，并且是针对Synchronized。在Java 5通过引入锁升级的机制来实现高效Synchronized。这三种锁的状态是通过对象监视器在对象头中的字段来表明的。

偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁。降低获取锁的代价。

轻量级锁是指当锁是偏向锁的时候，被另一个线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，提高性能。

重量级锁是指当锁为轻量级锁的时候，另一个线程虽然是自旋，但自旋不会一直持续下去，当自旋一定次数的时候，还没有获取到锁，就会进入阻塞，该锁膨胀为重量级锁。重量级锁会让其他申请的线程进入阻塞，性能降低。

八、自旋锁

在Java中，自旋锁是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁，这样的好处是减少线程上下文切换的消耗，缺点是循环会消耗CPU。

典型的自旋锁实现的例子，可以参考自旋锁的实现
设计原则与设计模式
设计原则有哪些？
1.遵循单一职责原则

一个类只专注于做一件事；
高内聚，低耦合；


2.开放-封闭原则

对拓展开放，对修改关闭（尽可能不动原有代码进行拓展）；
高内聚，低耦合；
为达到目的，需对系统进行抽象化设计（关键）；
UML举例：


3.里氏代换原则（LSP）

子类必须能够替换它们的基类型，基类与子类可互换，客户端没有察觉情况下；
低耦合；
很容易实现同一父类下的各个子类的互换，而客户端可以毫不察觉；
UML实例：


士兵只能使用WeaponGun，而无法使用ToyGun杀敌，如果使用ToyGun将会出错。这就是里氏替换原则。


4.依赖倒置原则

这个是开闭原则的基础，具体内容：面向接口编程，依赖于抽象而不依赖于具体。写代码时用到具体类时，不与具体类交互，而与具体类的上层接口交互。

5.接口隔离原则（Interface Segregation Principle）

使用多个专一功能的接口比使用一个总接口总要好，但不能过渡；
一个接口不能过于臃肿；
高内聚
会使一个软件系统功能拓展时，修改的压力不会影响到别的对象那去；
UML实例：

Model只要满足好身材这个条件，就有可能成为模特，漂亮女孩满足好身材和好。AngelaBaby即有好身材又好。


6.迪米特法则（Law of Demeter）

又叫最少知识原则；
对象与对象之间应该使用尽可能少的方法来关联，避免千丝万缕的关系；
低耦合；
类知道其他类应尽量少；
类可以访问其他类的方法或者属性也应尽量少；


如果Lily和Jack自己做hamburg那么就会与Vegetable、Meat、Bread产生千丝万缕关系。上图，则由KFC来做，这样Lily和Jack就只与Hamburg有关关联，与材料就没有关联了。


项目中用了哪些设计模式？
1.单例模式。
系统里有一个对象只允许产生一次。单例模式保证一个类仅有一个实例，并提供一个访问的他的全局访问点。
应用：数据库连接池。因为数据库连接池是一种数据库资源，使用数据库连接池的主要是为了节省打开或者关闭数据库连接所造成的效率损耗，这种效率上的损耗还是非常昂贵的，使用单例设计模式可以大大降低这种损耗。
多线程中线程池的设计一般也采用单例设计模式，这是由于线程池要方便对池中的线程进行控制。
Web应用的配置对象的读取，一般也应用单例模式，这个是由于配置文件是共享的资源。
网站中访问计数器，一般也是采用单例设计模式实现的，否则难以同步。

2.利用策略模式，优化if else
需求
这里虚拟一个业务需求，让大家容易理解。假设有一个订单系统，里面的一个功能是根据订单的不同类型作出不同的处理。
订单实体：

service接口：

传统实现

根据订单类型写一堆的if else：

策略模式实现
利用策略模式，只需要两行即可实现业务逻辑：

可以看到上面的方法中注入了HandlerContext，这是一个处理器上下文，用来保存不同的业务处理器，具体在下文会讲解。我们从中获取一个抽象的处理器AbstractHandler，调用其方法实现业务逻辑。
现在可以了解到，我们主要的业务逻辑是在处理器中实现的，因此有多少个订单类型，就对应有多少个处理器。以后需求变化，增加了订单类型，只需要添加相应的处理器就可以，上述OrderServiceV2Impl完全不需改动。
我们先看看业务处理器的写法：





首先每个处理器都必须添加到spring容器中，因此需要加上@Component注解，其次需要加上一个自定义注解@HandlerType，用于标识该处理器对应哪个订单类型，最后就是继承AbstractHandler，实现自己的业务逻辑。
自定义注解 @HandlerType：

抽象处理器 AbstractHandler：

自定义注解和抽象处理器都很简单，那么如何将处理器注册到spring容器中呢？
具体思路是：
1、扫描指定包中标有@HandlerType的类；
2、将注解中的类型值作为key，对应的类作为value，保存在Map中；
3、以上面的map作为构造函数参数，初始化HandlerContext，将其注册到spring容器中；
我们将核心的功能封装在HandlerProcessor类中，完成上面的功能。
HandlerProcessor：

ClassScanner：扫描工具类源码
HandlerProcessor需要实现BeanFactoryPostProcessor，在spring处理bean前，将自定义的bean注册到容器中。
核心工作已经完成，现在看看HandlerContext如何获取对应的处理器：
HandlerContext：

BeanTool：获取bean工具类
#getInstance 方法根据类型获取对应的class，然后根据class类型获取注册到spring中的bean。
最后请注意一点，HandlerProcessor和BeanTool必须能被扫描到，或者通过@Bean的方式显式的注册，才能在项目启动时发挥作用。
总结
利用策略模式可以简化繁杂的if else代码，方便维护，而利用自定义注解和自注册的方式，可以方便应对需求的变更。本文只是提供一个大致的思路，还有很多细节可以灵活变化，例如使用枚举类型、或者静态常量，作为订单的类型，相信你能想到更多更好的方法。
示例代码：
https://github.com/ciphermagic/java-learn/tree/master/sandbox/src/main/java/com/cipher/handler_demo

3.模版方法模式
模版方法模式属于行为型模式。模版方法模式的意图是在一个方法里实现一个算法，并推迟定义算法中的某些步骤，从而让其他类重新定义他们。

一个例子
在项目中经常会用到RedisTemplate，restTemplate，JdbcTemplate之类的对象。因为在操作资源的时候，通常会涉及到打开连接，使用资源，关闭连接的过程。但是这几个步骤又是很通用的，所以开发者抽象出来一个模版来定义一个算法骨架，然后通过子类的方式来定义各种资源具体的操作。

4.代理模式
说起java动态代理，在我刚开始学java时对这项技术也是十分困惑，明明可以直接调通的对象方法为什么还要使用动态代理？随着学习的不断深入和工作经验的积累，慢慢的体会并理解了java动态代理机制。昨天再给公司新同事做技术培训时有同学就对动态代理产生了疑问，我这里梳理一遍一并记录一下，方便大家查看对自己也是加深记忆。
(1)什么是代理？
大道理上讲代理是一种软件设计模式，目的地希望能做到代码重用。具体上讲，代理这种设计模式是通过不直接访问被代理对象的方式，而访问被代理对象的方法。这个就好比 商户---->明星经纪人(代理)---->明星这种模式。我们可以不通过直接与明星对话的情况下，而通过明星经纪人(代理)与其产生间接对话。

(2)什么情况下使用代理？
(1)设计模式中有一个设计原则是开闭原则，是说对修改关闭对扩展开放，我们在工作中有时会接手很多前人的代码，里面代码逻辑让人摸不着头脑，这时就很难去下手修改代码，那么这时我们就可以通过代理对类进行增强。
(2)我们在使用RPC框架的时候，框架本身并不能提前知道各个业务方要调用哪些接口的哪些方法 。那么这个时候，就可用通过动态代理的方式来建立一个中间人给客户端使用，也方便框架进行搭建逻辑，某种程度上也是客户端代码和框架松耦合的一种表现。
(3)Spring的AOP机制就是采用动态代理的机制来实现切面编程。

(3)静态代理和动态代理应用场景
我们根据加载被代理类的时机不同，将代理分为静态代理和动态代理。如果我们在代码编译时就确定了被代理的类是哪一个，那么就可以直接使用静态代理；如果不能确定，那么可以使用类的动态加载机制，在代码运行期间加载被代理的类这就是动态代理，比如RPC框架和Spring AOP机制。

(4)静态代理
我们先创建一个接口，遗憾的是java api代理机制求被代理类必须要实现某个接口，对于静态代理方式代理类也要实现和被代理类相同的接口；对于动态代理代理类则不需要显示的实现被代理类所实现的接口。
[java] view plain copy
/** 
 * 顶层接口 
 * @author yujie.wang 
 * 
 */  
public interface Person {  
    public void sayHello(String content, int age);  
    public void sayGoodBye(boolean seeAgin, double time);  
}  

[java] view plain copy
/** 
 * 需要被代理的类 实现了一个接口Person 
 * @author yujie.wang 
 * 
 */  
public class Student implements Person{  
  
    @Override  
    public void sayHello(String content, int age) {  
        // TODO Auto-generated method stub  
        System.out.println("student say hello" + content + " "+ age);  
    }  
  
    @Override  
    public void sayGoodBye(boolean seeAgin, double time) {  
        // TODO Auto-generated method stub  
        System.out.println("student sayGoodBye " + time + " "+ seeAgin);  
    }  
  
}  

[java] view plain copy
/** 
 * 静态代理，这个代理类也必须要实现和被代理类相同的Person接口 
 * @author yujie.wang 
 * 
 */  
public class ProxyTest implements Person{  
      
    private Person o;  
      
    public ProxyTest(Person o){  
        this.o = o;  
    }  
  
    public static void main(String[] args) {  
        // TODO Auto-generated method stub  
        //s为被代理的对象，某些情况下 我们不希望修改已有的代码，我们采用代理来间接访问  
        Student s = new Student();  
        //创建代理类对象  
        ProxyTest proxy = new ProxyTest(s);  
        //调用代理类对象的方法  
        proxy.sayHello("welcome to java", 20);  
        System.out.println("******");  
        //调用代理类对象的方法  
        proxy.sayGoodBye(true, 100);  
  
    }  
  
    @Override  
    public void sayHello(String content, int age) {  
        // TODO Auto-generated method stub  
        System.out.println("ProxyTest sayHello begin");  
        //在代理类的方法中 间接访问被代理对象的方法  
        o.sayHello(content, age);  
        System.out.println("ProxyTest sayHello end");  
    }  
  
    @Override  
    public void sayGoodBye(boolean seeAgin, double time) {  
        // TODO Auto-generated method stub  
        System.out.println("ProxyTest sayHello begin");  
        //在代理类的方法中 间接访问被代理对象的方法  
        o.sayGoodBye(seeAgin, time);  
        System.out.println("ProxyTest sayHello end");  
    }  
  
}  

测试代码输出：
[java] view plain copy
ProxyTest sayHello begin  
student say hellowelcome to java 20  
ProxyTest sayHello end  
******  
ProxyTest sayHello begin  
student sayGoodBye 100.0 true  
ProxyTest sayHello end  

静态代理看起来是比较简单的，没有什么问题只不过是在代理类中引入了被代理类的对象而已。
那么接下来我们看看动态代理。
(5)动态代理（反射+代理）
我们先直接上动态代理的代码，之后再分析代码的行为，上面的Person接口和Student被代理类保持不变。
[java] view plain copy
/** 
 * 动态代理,动态代理类不要显示的实现被代理类所实现的接口 
 * @author yujie.wang 
 * 
 */  
public class MyInvocationHandler implements InvocationHandler{  
      
    private Object object;  
      
    public MyInvocationHandler(Object object){  
        this.object = object;  
    }  
  
    @Override  
    public Object invoke(Object proxy, Method method, Object[] args)  
            throws Throwable {  
        // TODO Auto-generated method stub  
        System.out.println("MyInvocationHandler invoke begin");  
        System.out.println("proxy: "+ proxy.getClass().getName());  
        System.out.println("method: "+ method.getName());  
        for(Object o : args){  
            System.out.println("arg: "+ o);  
        }  
        //通过反射调用 被代理类的方法  
        method.invoke(object, args);  
        System.out.println("MyInvocationHandler invoke end");  
        return null;  
    }  
      
    public static void main(String [] args){  
        //创建需要被代理的类  
        Student s = new Student();  
        //这一句是生成代理类的class文件，前提是你需要在工程根目录下创建com/sun/proxy目录，不然会报找不到路径的io异常  
        System.getProperties().put("sun.misc.ProxyGenerator.saveGeneratedFiles","true");  
        //获得加载被代理类的 类加载器  
        ClassLoader loader = Thread.currentThread().getContextClassLoader();  
        //指明被代理类实现的接口  
        Class<?>[] interfaces = s.getClass().getInterfaces();  
        // 创建被代理类的委托类,之后想要调用被代理类的方法时，都会委托给这个类的invoke(Object proxy, Method method, Object[] args)方法  
        MyInvocationHandler h = new MyInvocationHandler(s);  
        //生成代理类  
        Person proxy = (Person)Proxy.newProxyInstance(loader, interfaces, h);  
        //通过代理类调用 被代理类的方法  
        proxy.sayHello("yujie.wang", 20);  
        proxy.sayGoodBye(true, 100);  
        System.out.println("end");  
    }  
  
}  
运行测试代码输出如下结果：
[java] view plain copy
MyInvocationHandler invoke begin  
proxy: com.sun.proxy.$Proxy0  
method: sayHello  
arg: yujie.wang  
arg: 20  
student say helloyujie.wang 20  
MyInvocationHandler invoke end  
MyInvocationHandler invoke begin  
proxy: com.sun.proxy.$Proxy0  
method: sayGoodBye  
arg: true  
arg: 100.0  
student sayGoodBye 100.0 true  
MyInvocationHandler invoke end  
end  


仔细分析上面的动态代理实现代码，我们看到这里涉及到java反射包下的一个接口InvocationHandler和一个类Proxy。
[java] view plain copy
package java.lang.reflect;  
public interface InvocationHandler {  
    public Object invoke(Object proxy, Method method, Object[] args)  
    throws Throwable;  
}  
这个接口只有一个invoke方法，我们在通过代理类调用被代理类的方法时，最终都会委托给这个invoke方法执行，
[java] view plain copy
//通过代理类调用 被代理类的方法  
proxy.sayHello("yujie.wang", 20);  
proxy.sayGoodBye(true, 100);  

所以我们就可以在这个invoke方法中对被代理类进行增强或做一些其他操作。
Proxy类的public static Object newProxyInstance(ClassLoader loader,Class<?>[] interfaces,InvocationHandler h)方法内部通过拼接字节码的方式来创建代理类，后面我会反编译出它所创建的代理类看看内容。
我们看这个方法的三个参数：
ClassLoader loader：指定一个动态加载代理类的类加载器
Class<?>[] interfaces：指明被代理类实现的接口，之后我们通过拼接字节码生成的类才能知道调用哪些方法。
InvocationHandler h：这是一个方法委托类，我们通过代理调用被代理类的方法时，就可以将方法名和方法参数都委托给这个委托类。
你看我们现在有了类加载器、类实现的接口、要调用方法的方法名和参数，那么我们就可以做很多事情了。
(6)动态代理在RPC中的应用
动态代理技术
动态代理涉及到了两种技术：1，反射机制；2，代理机制。这两种技术的详细解释请自行谷歌或百度。
有几篇文章大家可以看看：
http://m.blog.csdn.net/hejingyuan6/article/details/36203505
http://www.cnblogs.com/xiaoluo501395377/p/3383130.html
动态代理类 ————>client
委托类(即真实类)——>server
接口———————>指动态代理需要调用的服务
代理类和委托类都必须实现相同的接口，因为代理真正调用的还是委托类的方法。

client(动态代理类)先动态生成一个对象，表示一个client对象。然后这个client对象调用接口的方法A，方法A一般就是我们需要的服务，然后通过某种机制，方法A的真正调用会发生在server端，server将调用完方法A后，会得到相应的结果，将结果返回给client。那么client和server的一次交互就完成了。整个过程下来，client端申请调用，而server端真正执行调用，最后server端将调用产生的结果返回给client。这样有什么好处呢？客户端一般就是申请服务，如计算服务，而真正的计算的过程会交由服务端进行（理论上服务端的计算资源可以无限大——如集群模式）。典型的hadoop的底层通信框架RPC原理就是如此。

我们先通过一个例子来讲动态代理的具体实现，然后再讲到RPC协议。
例子摘抄于“深入理解java虚拟机——JVM高级特性与最佳实践（第2版）”的9.2.3节，讲的非常透彻。如下：




我们来分解这个例子，看它是怎么实现动态代理的。它首先做了这么几件事
1，定义公共的接口IHello，它里面的方法sayHello()就是client将来要调用的服务。
2，定义server端的类，即Hello，它实现了公共接口IHello，并实现了里面的sayHello方法，将来会通过server端类Hello的对象来调用sayHello()方法；
3，DynamicProxy。注意，它并不是动态代理对象，切勿混淆。它最重要的是实现了InvocationHandler的接口。我们待会来说DynamicProxy的作用；
4，Proxy.newProxyInstance( )。真正生成动态代理对象，也就是前面提到的client端的对象。这个方法是通过反射机制，动态生成代理对象，接下来会讲这个动态代理对象所对应的类是$Proxy0。

再看反编译生成的代码，它主要讲的是生成client的动态代理对象。我们看看newProxyInstance()这一步到底发生了什么：
1，在JVM中生成代理类￥Proxy0，然后会根据这个￥Proxy0对象动态生成一个对象，即动态代理对象。
2，￥Proxy0代表的是client端。它实现了两个最主要的功能：其一，在static块中，该代理对象为传入接口的每一个方法及Object类的方法都通过使用反射技术，生成了Method对象，如sayHello()方法的Method对象是m3。其二，实现了公共接口IHello，并实现了里面的sayHello方法，即client与server都实现了公共的接口。由于￥Proxy0保存着DynamicProxy对象的引用（newProxyInstance的第三个参数），这样在main函数中调用时，即hello.sayHello()，会跳转到DynaicProxy的invoke方法执行；
3，解释一下this.h.invoke(this,m3,null)这句代码，”this.h”就是父类Proxy中保存的InvocationHandler实例变量，即newProxyInstance的传进来的第三个参数，而第一个参数this指的是动态代理对象￥Proxy0。
4，执行完第3步，就会跳转到DynamicProxy的invoke方法中，这时发生真正的调用method.invoke(originalObj，arg)。这里的originalObj就是指Server端的实例对象。可以看到最终落脚点是实例对象originalObj发生的调用。这是理解的核心点，一定要牢牢把握！！！

其实关键是要抓住三个对象及他们之间的关系：client端的￥Proxy0，server端的orignalObj，以及(实现了InvocationHandler方法的)DynamicProxy对象。他们的关系是，首先server端的orignalObj绑定到DynamicProxy对象上，或者说DynamicProxy对象保存着orignalObj的引用；然后￥Proxy0的对象又保存着DynamicProxy的引用。因此，在发生方法调用的时候，跳转的顺序是从￥Proxy0 ——>DynamicProxy——>orignalObj。
过程如下：
hello.sayHello()==￥Proxy0Obj.sayHello()———————————————>
this.h.invoke(this,m3,null)== InvocationHandler.invoke(￥Proxy,m3,null)——>
method.invoke(orignalObj, args)。
这样就实现了在client请求的服务（方法调用），实际却是通过server端的对象方法调用完成的。

我还想谈谈我对DynamicProxy的理解，它存在的意义到底是什么？
先给出我的理解，DynamicProxy这个类的作用是起到解耦的作用，使client不用关心server的对象长什么样子，server也不用关心client是什么。接下来进行简要的分析。
如果没有DynamicProxy，那么对于newProxyInstance，传的第三个参数必然是服务端的对象orignalObj ，因为我们最终是要实现通过server端来实现真正的接口服务。比如对于sayHello()调用，那可能会变成：
有DynamicProxy时，是：

sayHello()
{
    this.h.invoke(this, m3, args);
}
1
2
3
4
等价于

sayHello()
{
    dynamicProxyObj.invoke($Proxy0, m3, args);
}
1
2
3
4
没有DynamicProxy时，是：

SayHello()
{
    m3.(orignalObj, args);
}
1
2
3
4
发现，没有DynamicProxy也能实现client发起接口服务的申请，server端实现真正调用。但是这就让client完全依赖于server端的对象了，没有server对象，client对象就没法存在，耦合性太强。而有了DynamicProxy，两者就完全解耦了。所以说DynamicProxy的定位就是给client和server端解耦的。

RPC
那RPC是怎么回事呢？我尝试谈一下我对它的理解，可能不一定准确，欢迎大家拍砖。
首先说RPC的协议。RPC的协议有很多种，如TaskUmbilicalProtocol，JobSubmissionProtocol等等。每个协议都是一系列的方法的组合，实际上就相当于前面例子中的sayHello()方法。client和server都需要实现这些协议中的这些方法。
RPC的主要技术包括前面讲过的的动态代理机制、NIO技术、网络编程技术等。我们这里只关注RPC的动态代理机制的实现逻辑，其他不讲。
RPC的动态代理机制与前面讲的有什么区别呢？有，最大的区别是前面所述的client和server都在同一个地址空间，invoke的调用发生在本地。而RPC的client和server是隔离的，两者并不在同一机器上。
我们再来回顾一遍动态代理的逻辑：client对象要绑定一个server对象，client发起一个方法调用（或称为服务调用）后，server会执行真正的方法体，在server端得到结果后将其返回给client端，中间会插一个DynamicProxy来给两者解耦。这样一次调用完成。
我们会发现，最重要的是绑定的这个动作。即动态生成的这个client对象要找到一个server对象，这个对象必须跟clinet的对象实现了相同的接口，然后绑定它。
那么具体怎么实现呢？我们能想到的就是在client端指定server对象的地址，但此时client是不可能知道server端的地址的。实际的解决方案是，client端在invoke方法中，将函数的调用信息（函数名，参数列表等）打包成可以序列化的Invocation对象，指定server端的socket地址，然后申请建立socket连接。建立好连接后，会将Invocation对象通过网络发送给server端。对于server端，它会将本地创建的server对象绑定到client端过来的请求上，绑定完之后，就可以通过java的反射机制完成函数调用。重点强调一下可能的理解误区，从client发送出去的Invocation对象只是函数的调用信息（函数名，参数列表），不是client的对象，真正的client动态对象是在server端通过反射生成的。

下面server端的动作是我自己猜测的，还没有看相关源码，大家看看就好。
发生RPC调用时，结合NIO的模型，我们知道Reader线程是用来处理和client的数据交换的。因此，要满足一个client对应一个server端的对象，我猜测可能的一种实现方式如下，即每个ReaderTheader都会有自己的server对象，模型大体上长这个样子：


当然这种模型也有问题，就是serverObj对象太多，每创建一个对象都是要消耗资源的。因此改进的方式是serverObj搞成静态的，一个clientObj对应一个serverObj。如下：


这样clientObj在申请方法调用的时候，只需要指明要调用服务端的哪个方法即可。如果申请的调用太多，就将其塞到一个队列中，然后server端再进行后续的消费。实质上这后续的动作就属于NIO方面的知识。这里就不展开讲了，hadoop源码还没细看，这里就当瞎猜了

就到这里吧，以后应该还会继续补充细节的。。。

(7)反编译Proxy.newProxyInstance所创建的代理类
//这一句是生成代理类的class文件，前提是你需要在工程根目录下创建com/sun/proxy目录，不然会报找不到路径的io异常
System.getProperties().put("sun.misc.ProxyGenerator.saveGeneratedFiles","true");
我们在代码中加入上述代码，代码就会保存生成的代理类，名称为$Proxy0.class

通过jd-gui反编译代码如下，其中注释是我加上去的：
[java] view plain copy
package com.sun.proxy;  
  
import com.yujie.proxy.dynamic.Person;  
import java.lang.reflect.InvocationHandler;  
import java.lang.reflect.Method;  
import java.lang.reflect.Proxy;  
import java.lang.reflect.UndeclaredThrowableException;  
/** 
*代理类也实现了Person接口，看起来和静态代理的方式也会一样的  
*同时代理类也继承了Proxy类 
*/   
public final class $Proxy0 extends Proxy implements Person{  
  private static Method m4;  
  private static Method m1;  
  private static Method m0;  
  private static Method m3;  
  private static Method m2;  
    
  public $Proxy0(InvocationHandler paramInvocationHandler)  
    throws   
  {  
    super(paramInvocationHandler);  
  }  
   //实现了Person接口的方法，这就是我们调用这个方法Proxy.newProxyInstance必须提供第二个参数的作用   
  public final void sayGoodBye(boolean paramBoolean, double paramDouble)  
    throws   
  {  
    try  
    {  
    // 我们看到通过调用代理类的方法时，最终方法都会委托给InvocationHandler实现类的invoke方法  
    // m4为代理类通过反射获得的Method  
      this.h.invoke(this, m4, new Object[] { Boolean.valueOf(paramBoolean), Double.valueOf(paramDouble) });  
      return;  
    }  
    catch (Error|RuntimeException localError)  
    {  
      throw localError;  
    }  
    catch (Throwable localThrowable)  
    {  
      throw new UndeclaredThrowableException(localThrowable);  
    }  
  }  
    
  public final boolean equals(Object paramObject)  
    throws   
  {  
    try  
    {  
      return ((Boolean)this.h.invoke(this, m1, new Object[] { paramObject })).booleanValue();  
    }  
    catch (Error|RuntimeException localError)  
    {  
      throw localError;  
    }  
    catch (Throwable localThrowable)  
    {  
      throw new UndeclaredThrowableException(localThrowable);  
    }  
  }  
    
  public final int hashCode()  
    throws   
  {  
    try  
    {  
      return ((Integer)this.h.invoke(this, m0, null)).intValue();  
    }  
    catch (Error|RuntimeException localError)  
    {  
      throw localError;  
    }  
    catch (Throwable localThrowable)  
    {  
      throw new UndeclaredThrowableException(localThrowable);  
    }  
  }  
  //实现了Person接口的方法，这就是我们调用这个方法Proxy.newProxyInstance必须提供第二个参数的作用   
  public final void sayHello(String paramString, int paramInt)  
    throws   
  {  
    try  
    {  
      // 我们看到通过调用代理类的方法时，最终方法都会委托给InvocationHandler实现类的invoke方法  
      // m4为代理类通过反射获得的Method  
      this.h.invoke(this, m3, new Object[] { paramString, Integer.valueOf(paramInt) });  
      return;  
    }  
    catch (Error|RuntimeException localError)  
    {  
      throw localError;  
    }  
    catch (Throwable localThrowable)  
    {  
      throw new UndeclaredThrowableException(localThrowable);  
    }  
  }  
    
  public final String toString()  
    throws   
  {  
    try  
    {  
      return (String)this.h.invoke(this, m2, null);  
    }  
    catch (Error|RuntimeException localError)  
    {  
      throw localError;  
    }  
    catch (Throwable localThrowable)  
    {  
      throw new UndeclaredThrowableException(localThrowable);  
    }  
  }  
    
  static  
  {  
    try  
    {//代理类通过反射 获得的接口方法Method  
      m4 = Class.forName("com.yujie.proxy.dynamic.Person").getMethod("sayGoodBye", new Class[] { Boolean.TYPE, Double.TYPE });  
      m1 = Class.forName("java.lang.Object").getMethod("equals", new Class[] { Class.forName("java.lang.Object") });  
      m0 = Class.forName("java.lang.Object").getMethod("hashCode", new Class[0]);  
      //代理类通过反射 获得的接口方法Method  
      m3 = Class.forName("com.yujie.proxy.dynamic.Person").getMethod("sayHello", new Class[] { Class.forName("java.lang.String"), Integer.TYPE });  
      m2 = Class.forName("java.lang.Object").getMethod("toString", new Class[0]);  
      return;  
    }  
    catch (NoSuchMethodException localNoSuchMethodException)  
    {  
      throw new NoSuchMethodError(localNoSuchMethodException.getMessage());  
    }  
    catch (ClassNotFoundException localClassNotFoundException)  
    {  
      throw new NoClassDefFoundError(localClassNotFoundException.getMessage());  
    }  
  }  
}  
(8)Jdk动态代理原理？
动态生成的代理类有如下特性：
1.继承了Proxy类，实现了代理的接口，由于java不能多继承，这里已经继承了Proxy类了，不能再继承其他的类，所以JDK的动态代理不支持对实现类的代理，只支持接口的代理。
2.提供了一个使用InvocationHandler作为参数的构造方法。
3.生成静态代码块来初始化接口中方法的Method对象，以及Object类的equals、hashCode、toString方法。
4.重写了Object类的equals、hashCode、toString，它们都只是简单的调用了InvocationHandler的invoke方法，即可以对其进行特殊的操作，也就是说JDK的动态代理还可以代理上述三个方法。
5.代理类实现代理接口的sayHello方法中，只是简单的调用了InvocationHandler的invoke方法，我们可以在invoke方法中进行一些特殊操作，甚至不调用实现的方法，直接返回。
(9)cglib动态代理原理？

a.代理类会获得所有在父类继承来的方法，并且会有MethodProxy与之对应，比如 Method CGLIB$setPerson$0$Method、MethodProxy CGLIB$setPerson$0$Proxy;
b.//代理方法（methodProxy.invokeSuper会调用）
c.//被代理方法(methodProxy.invoke会调用，这就是为什么在拦截器中调用methodProxy.invoke会死循环，一直在调用拦截器)
d.//调用拦截器         var10000.intercept(this, CGLIB$setPerson$0$Method, CGLIB$emptyArgs, CGLIB$setPerson$0$Proxy);
e.调用过程：代理对象调用this.setPerson方法->调用拦截器->methodProxy.invokeSuper->CGLIB$setPerson$0->被代理对象setPerson方法
f.拦截器MethodInterceptor中就是由MethodProxy的invokeSuper方法调用代理方法的，MethodProxy非常关键，我们分析一下它具体做了什么。
g.上面代码调用过程就是获取到代理类对应的FastClass，并执行了代理方法。还记得之前生成三个class文件吗？PersonService$$EnhancerByCGLIB$$eaaaed75$$FastClassByCGLIB$$355cb7ea.class就是代理类的FastClass，
PersonService$$FastClassByCGLIB$$a076b035.class就是被代理类的FastClass。

(10)Jdk动态代理与ciglib动态代理区别？
  
  JDK的动态代理机制只能代理实现了接口的类，而不能实现接口的类就不能实现JDK的动态代理，必须以接口的方式来实现，这样就要求目标类必须是继承接口的

  cglib一般是针对类来实现代理的，他的原理是对指定的目标类生成一个子类，并覆盖其中方法实现增强，但因为采用的是继承，所以不能对final修饰的类进行代理。


最后我们总结一下JDK动态代理和Gglib动态代理的区别：
1.JDK动态代理是实现了被代理对象的接口，Cglib是继承了被代理对象。
2.JDK和Cglib都是在运行期生成字节码，JDK是直接写Class字节码，Cglib使用ASM框架写Class字节码，Cglib代理实现更复杂，生成代理类比JDK效率低。
3.JDK调用代理方法，是通过反射机制调用，Cglib是通过FastClass机制直接调用方法，Cglib执行效率更高。

FastClass机制，它的原理简单来说就是：为代理类和被代理类各生成一个Class，这个Class会为代理类或被代理类的方法分配一个index(int类型)。
这个index当做一个入参，FastClass就可以直接定位要调用的方法直接进行调用，这样省去了反射调用，所以调用效率比JDK动态代理通过反射调用高。

(11)静态代理与动态代理区别
使用静态代理的优点：

使真实角色的业务更加简洁，没有多余的公共类业务，真实角色只负责本身的业务。
公共类业务全部由代理完成，使真实角色和代理类分工合作。
当公共类业务发生改变只需要改变代理类而不用去改变真实角色的代码。有利于程序的维护。
缺点：

每个真实角色都需要相应的代理类---类的数量变多。
编写代理类的数量多开发效率降低了。

动态代理
动态代理与静态代理的角色是一样的。
动态代理的代理类是动态生成的
动态代理有静态代理的好处，没有静态代理那样庞大的工作量
动态代理分为两类一类基于接口的动态代理一类基于类的动态代理
①　基于接口动态代理————jdk动态代理
②　基于类的动态代理————cglib


(12)

总结一下

jdk的代理让我们在不直接访问某些对象的情况下，通过代理机制也可以访问被代理对象的方法，这种技术可以应用在很多地方比如RPC框架，Spring AOP机制，但是我们看到jdk的代理机制必须要求被代理类实现某个方法，这样在生成代理类的时候才能知道重新那些方法。这样一个没有实现任何接口的类就无法通过jdk的代理机制进行代理，当然解决方法是使用cglib的代理机制进行代理。

5.工厂模式
工厂模式是一种在工程中广泛应用的设计模式，对代码的解耦合起到了很大的作用。实际上，我们可以将Spring理解成封装了我们工程中大量重复代码的一种工具，上一节说了，Spring中最为重要的组件就是IOC，而IOC中非常重要的部分就是应用了工厂模式的代码。而工厂模式依赖于Java的反射机制，所以，我们从反射机制讲起，一步步了解Spring的Bean工厂。
Java中的反射机制
我们在此处并不会详细介绍反射机制，只会讲述一些简单的内容，详细的东西我会整合成一篇文章供大家参考。
反射机制是指程序在运行的过程中，对于任意的一个类，我们都能够获取它的所有属性、方法，对于任何一个对象，都能调用它的任意方法和属性。
你还记得我们使用JDBC的流程吗？

//1.加载驱动程序
Class.forName("com.mysql.jdbc.Driver");
//2.获得数据库链接
Connection conn=DriverManager.getConnection(URL, USER, PASSWORD);
……
……
1
2
3
4
5
6
Class.forName()方法就是在程序运行的过程中，动态的将Driver这个类加加载到JVM（Java虚拟机）中并初始化，然后我们就能够直接调用这个类及其方法。实际上最简单的工厂模式就是用的这个方法，在JVM运行期直接通过forName方法创建某个类。了解了这些我们就开始工厂模式的讲解。

工厂模式
工厂模式提供了一种绝佳的创建对象的方法。在工厂模式中，我们并不会直接使用new来创建一个对象，而是使用一个共同的接口类来指定其实现类，这就大大降低了系统的耦合性——我们无需改变每个调用此接口的类，而直接改变实现此接口的类即可完成软件的更新迭代。直接来看一下下面这个工厂模式的代码。

import java.util.ResourceBundle;

/**
 * 使用此工厂类创建bean实例
 */
public class BeanFactory {
    //加载配置文件
    private static ResourceBundle bundle;
    static {
        bundle = ResourceBundle.getBundle("instance");
    }

    //根据指定的key,读取配置文件的全路径，创建对象
    public static <T>T getInstance(String key,Class<T> clazz){
        String className = bundle.getString(key);

        try {
            return (T)Class.forName(className).newInstance();
        }catch (Exception e){
            throw new RuntimeException();
        }
    }
}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
Class.forName(className).newInstance()方法就会返回className对应的类，这样我们就能够使用了。
ResourceBundle这个类是用来读取classpath中文件，这个文件需要放在resources文件夹或java包的根路径下，文件名必须是*.properties这个配置文件类型。这样读取到的文件流就形成了键值对的存储形式。配置文件的示例如下（使用键值对的方式配置）：

#service instance
foodTypeService=service.impl.FoodTypeService
dinnerTableService=service.impl.DinnerTableService
1
2
3
注意，配置文件中配置的需是接口的实现类，因为在工厂创建Bean的时候我们是使用接口来接收实现类的，这样才能够实现解耦合。笨着方法想，我们不能够直接new一个接口，对不对。
调用的时候如此调用：

protected IDinnerTableService dinnerTableService = BeanFactory.getInstance("dinnerTableService",IDinnerTableService.class);
1
我们传入key和DinnerTableService的class，通过这种方法就能够得到一个对象。

Spring中的工厂模式
工厂模式的思想正好契合SpringIOC的设计思想:某一接口的具体实现类的选择控制权从调用类中移除，转而交给第三方决定，即借由Spring的Bean配置来实现控制，这同样也是工厂模式的思想。
在Spring中有两个最基本的工厂，BeanFactory和ApplicationContext。BeanFactory是Spring框架的基础设施，面向的是Spring本身，也就是用于创建Spring扩展的其他内容，如Spring Security、Spring JDBC等，而ApplicationContext这个工厂是面向开发者的，也就是应用上下文——配置文件等，开发者能够使用这个工厂实现自己的功能。
关于Spring中的工厂模式，我们在后续的文章中会继续讲解，欢迎关注。
————————————————
版权声明：本文为CSDN博主「Roobtyan」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/yanmiao0715/article/details/82804949
6.装饰者模式
在我们的项目中遇到这样一个问题：我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。我们以往在spring和hibernate框架中总是配置一个数据源，因而sessionFactory的dataSource属性总是指向这个数据源并且恒定不变，所有DAO在使用sessionFactory的时候都是通过这个数据源访问数据库。但是现在，由于项目的需要，我们的DAO在访问sessionFactory的时候都不得不在多个数据源中不断切换，问题就出现了：如何让sessionFactory在执行数据持久化的时候，根据客户的需求能够动态切换不同的数据源？我们能不能在spring的框架下通过少量修改得到解决？是否有什么设计模式可以利用呢？

[b][size=xx-small]问题的分析[/size][/b]

我首先想到在spring的applicationContext中配置所有的dataSource。这些dataSource可能是各种不同类型的，比如不同的数据库：Oracle、SQL Server、MySQL等，也可能是不同的数据源：比如apache 提供的org.apache.commons.dbcp.BasicDataSource、spring提供的org.springframework.jndi.JndiObjectFactoryBean等。然后sessionFactory根据客户的每次请求，将dataSource属性设置成不同的数据源，以到达切换数据源的目的。

但是，我很快发现一个问题：当多用户同时并发访问数据库的时候会出现资源争用的问题。这都是“单例模式”惹的祸。众所周知，我们在使用spring框架的时候，在beanFactory中注册的bean基本上都是采用单例模式，即spring在启动的时候，这些bean就装载到内存中，并且每个bean在整个项目中只存在一个对象。正因为只存在一个对象，对象的所有属性，更准确说是实例变量，表现得就如同是个静态变量（实际上“静态”与“单例”往往是非常相似的两个东西，我们常常用“静态”来实现“单例”）。拿我们的问题来说，sessionFactory在整个项目中只有一个对象，它的实例变量dataSource也就只有一个，就如同一个静态变量一般。如果不同的用户都不断地去修改dataSource的值，必然会出现多用户争用一个变量的问题，对系统产生隐患。

通过以上的分析，解决多数据源访问问题的关键，就集中在sessionFactory在执行数据持久化的时候，能够通过某段代码去根据客户的需要动态切换数据源，并解决资源争用的问题。

[b][size=xx-small]问题的解决[/size][/b]
[b][size=xx-small](一) 采用Decorator设计模式[/size][/b]
要解决这个问题，我的思路锁定在了这个dataSource上了。如果sessionFactory指向的dataSource可以根据客户的需求去连接客户所需要的真正的数据源，即提供动态切换数据源的功能，那么问题就解决了。那么我们怎么做呢？去修改那些我们要使用的dataSource源码吗？这显然不是一个好的方案，我们希望我们的修改与原dataSource代码是分离的。根据以上的分析，使用GoF设计模式中的Decorator模式（装饰者模式）应当是我们可以选择的最佳方案。

什么是“Decorator模式”？简单点儿说就是当我们需要修改原有的功能，但我们又不愿直接去修改原有的代码时，设计一个Decorator套在原有代码外面。当我们使用Decorator的时候与原类完全一样，当Decorator的某些功能却已经修改为了我们需要修改的功能。Decorator模式的结构如图。
[img]http://dl.iteye.com/upload/attachment/345895/acd67ca7-c20e-3faf-98fb-1bdb21dc6015.jpg[/img]
我们本来需要修改图中所有具体的Component类的一些功能，但却并不是去直接修改它们的代码，而是在它们的外面增加一个Decorator。Decorator与具体的Component类都是继承的AbstractComponent，因此它长得和具体的Component类一样，也就是说我们在使用Decorator的时候就如同在使用ConcreteComponentA或者ConcreteComponentB一样，甚至那些使用ConcreteComponentA或者ConcreteComponentB的客户程序都不知道它们用的类已经改为了Decorator，但是Decorator已经对具体的Component类的部分方法进行了修改，执行这些方法的结果已经不同了。
[b][size=xx-small](二) 设计MultiDataSource类[/size][/b]
现在回到我们的问题，我们需要对dataSource的功能进行变更，但又不希望修改dataSource中的任何代码。我这里指的dataSource是所有实现javax.sql.DataSource接口的类，我们常用的包括apache 提供的org.apache.commons.dbcp.BasicDataSource、spring提供的org.springframework.jndi.JndiObjectFactoryBean等，这些类我们不可能修改它们本身，更不可能对它们一个个地修改以实现动态分配数据源的功能，同时，我们又希望使用dataSource的sessionFactory根本就感觉不到这样的变化。Decorator模式就正是解决这个问题的设计模式。

首先写一个Decorator类，我取名叫MultiDataSource，通过它来动态切换数据源。同时在配置文件中将sessionFactory的dataSource属性由原来的某个具体的dataSource改为MultiDataSource。如图：
[img]http://dl.iteye.com/upload/attachment/345898/1c1ec4aa-bc63-3cab-adb5-d5d1cc52caba.jpg[/img]
对比原Decorator模式，AbstractComponent是一个抽象类，但在这里我们可以将这个抽象类用接口来代替，即DataSource接口，而ConcreteComponent就是那些DataSource的实现类，如BasicDataSource、JndiObjectFactoryBean等。MultiDataSource封装了具体的dataSource，并实现了数据源动态切换：

java 代码


public class MultiDataSource implements DataSource {   
   private DataSource dataSource = null;   
   public MultiDataSource(DataSource dataSource){   
       this.dataSource = dataSource;   
   }   
       /* (non-Javadoc)  
        * @see javax.sql.DataSource#getConnection()  
        */  
   public Connection getConnection() throws SQLException {   
        return getDataSource().getConnection();   
   }   
     //其它DataSource接口应当实现的方法   

   public DataSource getDataSource(){   
      return this.dataSource;   
   }   
  }   
  public void setDataSource(DataSource dataSource) {   
      this.dataSource = dataSource;   
  }   
}   


客户在发出请求的时候，将dataSourceName放到request中，然后把request中的数据源名通过调用new MultiDataSource(dataSource)时可以告诉MultiDataSource客户需要的数据源，就可以实现动态切换数据源了。但细心的朋友会发现这在单例的情况下就是问题的，因为MultiDataSource在系统中只有一个对象，它的实例变量dataSource也只有一个，就如同一个静态变量一般。正因为如此，单例模式让许多设计模式都不得不需要更改，这将在我的《“单例”更改了我们的设计模式》中详细讨论。那么，我们在单例模式下如何设计呢？

[b][size=xx-small](三) 单例模式下的MultiDataSource[/size][/b]
在单例模式下，由于我们在每次调用MultiDataSource的方法的时候，dataSource都可能是不同的，所以我们不能将dataSource放在实例变量dataSource中，最简单的方式就是在方法getDataSource()中增加参数，告诉MultiDataSource我到底调用的是哪个dataSource：

public DataSource getDataSource(String dataSourceName){   
		log.debug("dataSourceName:"+dataSourceName);   
		try{   
			if(dataSourceName==null||dataSourceName.equals("")){   
				return this.dataSource;   
            }   
			return (DataSource)this.applicationContext.getBean(dataSourceName);   
		}catch(NoSuchBeanDefinitionException ex){   
			throw new DaoException("There is not the dataSource 
		}   
	} 

值得一提的是，我需要的数据源已经都在spring的配置文件中注册，dataSourceName就是其对应的id。
xml 代码

 <bean id="dataSource1"  
     class="org.apache.commons.dbcp.BasicDataSource">  
     <property name="driverClassName">  
         <value>oracle.jdbc.driver.OracleDrivervalue>  
     property> 
     ......   
 bean>  
 <bean id="dataSource2"  
     class="org.apache.commons.dbcp.BasicDataSource">  
     <property name="driverClassName">  
         <value>oracle.jdbc.driver.OracleDrivervalue> 
     property>   
     ......   
 bean> 

为了得到spring的ApplicationContext，MultiDataSource类必须实现接口org.springframework.context.ApplicationContextAware，并且实现方法：
java 代码

private ApplicationContext applicationContext = null;   
     public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {   
         this.applicationContext = applicationContext;   
     } 

如此这样，我就可以通过this.applicationContext.getBean(dataSourceName)得到dataSource了。
[b][size=xx-small](四) 通过线程传递dataSourceName[/size][/b]
查看以上设计，MultiDataSource依然无法运行，因为用户在发出请求时，他需要连接什么数据库，其数据源名是放在request中的，要将request中的数据源名传给MultiDataSource，需要经过BUS和DAO，也就是说为了把数据源名传给MultiDataSource，BUS和DAO的所有方法都要增加dataSourceName的参数，这是我们不愿看到的。写一个类，通过线程的方式跳过BUS和DAO直接传递给MultiDataSource是一个不错的设计：

[img]http://dl.iteye.com/upload/attachment/345905/b6d72569-7258-30ba-aaff-6394557eedb5.jpg[/img]
java 代码

    public class SpObserver {   
        private static ThreadLocal local = new ThreadLocal();   
        public static void putSp(String sp) {   
            local.set(sp);   
        }   
        public static String getSp() {   
            return (String)local.get();   
        }   
    }   

做一个filter，每次客户发出请求的时候就调用SpObserver.petSp(dataSourceName)，将request中的dataSourceName传递给SpObserver对象。最后修改MultiDataSource的方法getDataSource()：
java 代码

 public DataSource getDataSource(){   
      String sp = SpObserver.getSp();   
      return getDataSource(sp);   
 }  

完整的MultiDataSource代码在附件中。
[b][size=xx-small](五) 动态添加数据源[/size][/b]
通过以上方案，我们解决了动态分配数据源的问题，但你可能提出疑问：方案中的数据源都是配置在spring的ApplicationContext中，如果我在程序运行过程中动态添加数据源怎么办？这确实是一个问题，而且在我们的项目中也确实遇到。spring的ApplicationContext是在项目启动的时候加载的。加载以后，我们如何动态地加载新的bean到ApplicationContext中呢？我想到如果用spring自己的方法解决这个问题就好了。所幸的是，在查看spring的源代码后，我找到了这样的代码，编写了DynamicLoadBean类，只要调用loadBean()方法，就可以将某个或某几个配置文件中的bean加载到ApplicationContext中（见附件）。不通过配置文件直接加载对象，在spring的源码中也有，感兴趣的朋友可以自己研究。
[b][size=xx-small](六) 在spring中配置[/size][/b]
在完成了所有这些设计以后，我最后再唠叨一句。我们应当在spring中做如下配置：

 <bean id="dynamicLoadBean" class="com.htxx.service.dao.DynamicLoadBean">bean>  
 <bean id="dataSource" class="com.htxx.service.dao.MultiDataSource">  
         <property name="dataSource">  
             <ref bean="dataSource1" />  
         property>  
     bean>  
     <bean id="sessionFactory" class="org.springframework.orm.hibernate3.LocalSessionFactoryBean">  
         <property name="dataSource">  
             <ref bean="dataSource" />  
         property>  
         ......   
     bean>

其中dataSource属性实际上更准确地说应当是defaultDataSource，即spring启动时以及在客户没有指定数据源时应当指定的默认数据源。

[b][size=xx-small]该方案的优势[/size][/b]
以上方案与其它方案相比，它有哪些优势呢？

首先，这个方案完全是在spring的框架下解决的，数据源依然配置在spring的配置文件中，sessionFactory依然去配置它的dataSource属性，它甚至都不知道dataSource的改变。唯一不同的是在真正的dataSource与sessionFactory之间增加了一个MultiDataSource。

其次，实现简单，易于维护。这个方案虽然我说了这么多东西，其实都是分析，真正需要我们写的代码就只有MultiDataSource、SpObserver两个类。MultiDataSource类真正要写的只有getDataSource()和getDataSource(sp)两个方法，而SpObserver类更简单了。实现越简单，出错的可能就越小，维护性就越高。

最后，这个方案可以使单数据源与多数据源兼容。这个方案完全不影响BUS和DAO的编写。如果我们的项目在开始之初是单数据源的情况下开发，随着项目的进行，需要变更为多数据源，则只需要修改spring配置，并少量修改MVC层以便在请求中写入需要的数据源名，变更就完成了。如果我们的项目希望改回单数据源，则只需要简单修改配置文件。这样，为我们的项目将增加更多的弹性。

[b][size=x-small]特别说明：实例中的DynamicLoadBean在web环境下运行会出错，需要将类中AbstractApplicationContext改为 org.springframework.context.ConfigurableApplicationContext。[/size][/b]

[url]http://www.iteye.com/topic/91667[/url]
————————————————
版权声明：本文为CSDN博主「mib_2012」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/mib_2012/article/details/83762091
7.观察者模式
观察者模式实际应用场景-----Spring事件机制
以下伪代码是一个保存订单的功能，并会发送短信消息：

/**
* Author heling on 2019/1/9
*/
@Service
public class OrderServiceImpl implements OrderService {
 
    @Override
    public void saveOrder() {
        //1.创建订单
        System.out.println("订单创建成功");
        //2.发送短信
        System.out.println("恭喜您订单创建成功！----by sms");
    }
}
现有新需求：需要加一个微信通知的功能，代码如下：

/**
* Author heling on 2019/1/9
*/
@Service
public class OrderServiceImpl implements OrderService {
 
    @Override
    public void saveOrder() {
        //1.创建订单
        System.out.println("订单创建成功");
        //2.发送短信
        System.out.println("恭喜您订单创建成功！----by sms");
        //新需求：微信通知
        // 3.发送微信
        System.out.println("恭喜您订单创建成功！----by wechat");
    }
}
存在问题：每次创建订单需要加新功能（如新的通知方式），则要修改原有的类，难以维护。

违背设计模式的原则

1.单一职责：订单保存功能，杂糅了消息通知这些功能

2.开闭原则：对拓展开放，对修改关闭

 

优化方案：使用观察者模式，使创建订单和消息通知进行分离，低耦合。可以选择消息队列，spring事件机制等，本文选择Spring事件机制。

 

改造开始：

1.创建事件

package com.pengshu.magicwallet.admin.test;
 
import org.springframework.context.ApplicationEvent;
 
import java.util.List;
 
/**
* Author heling on 2019/1/9
* 订单创建活动事件
*/
public class OrderCreateEvent extends ApplicationEvent {
 
    private String name;
 
    //消息参数
    private List<String> contentList;
 
    public OrderCreateEvent(Object source, String name, List<String> contentList) {
        super(source);
        this.name = name;
        this.contentList = contentList;
    }
 
    public String getName() {
        return name;
    }
    public void setName(String name) {
        this.name = name;
    }
    public List<String> getContentList() {
        return contentList;
    }
    public void setContentList(List<String> contentList) {
        this.contentList = contentList;
    }
}
2.监听器

package com.pengshu.magicwallet.admin.test;
 
import org.springframework.context.ApplicationListener;
import org.springframework.stereotype.Component;
 
/**
* Author heling on 2019/1/9
* 短信监听器
* ApplicationListener是无序的
*/
@Component
public class SmsListener implements ApplicationListener<OrderCreateEvent> {
 
    @Override
    public void onApplicationEvent(OrderCreateEvent event) {
        //发送短信
        System.out.println(event.getContentList().get(0) + ",您的订单:" + event.getContentList().get(1) + "创建成功! ----by sms");
 
    }
}
package com.pengshu.magicwallet.admin.test;
 
import org.springframework.context.ApplicationListener;
import org.springframework.stereotype.Component;
 
/**
* Author heling on 2019/1/9
* 微信监听器
*/
@Component
public class WechatListener implements ApplicationListener<OrderCreateEvent> {
 
    @Override
    public void onApplicationEvent(OrderCreateEvent event) {
        //发送微信
        System.out.println(event.getContentList().get(0) + ",您的订单:" + event.getContentList().get(1) + "创建成功! ----by wechat");
 
    }
}
3.事件发布

package com.pengshu.magicwallet.admin.test;
 
import org.springframework.context.ApplicationContext;
import org.springframework.stereotype.Service;
 
import javax.annotation.Resource;
import java.util.ArrayList;
 
/**
* Author heling on 2019/1/9
*/
@Service
public class OrderServiceImpl implements OrderService {
 
    @Resource
    private ApplicationContext applicationContext;
 
    @Autowired
    private ApplicationEventPublisher applicationEventPublisher;
 
    @Override
    public void saveOrder() {
        //1.创建订单
        System.out.println("订单创建成功");
        //2.发布事件
        ArrayList<String> contentList = new ArrayList<>();
        contentList.add("heling");
        contentList.add("123456789");
        OrderCreateEvent orderCreateEvent = new OrderCreateEvent(this, "订单创建", contentList);
        applicationContext.publishEvent(orderCreateEvent);//ApplicationContext是我们的事件容器上层，我们发布事件，也可以通过此容器完成发布
        //applicationEventPublisher.publishEvent(orderCreateEvent);//也可以
        System.out.println("finished!");
    }
}
打印结果：

订单创建成功

heling,您的订单:123456789创建成功! ----by sms

heling,您的订单:123456789创建成功! ----by wechat

finished!

如何异步执行监听器？

1.springboot开启事件异步设置

package com.pengshu.magicwallet;
 
import org.mybatis.spring.annotation.MapperScan;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.PropertySource;
import org.springframework.scheduling.annotation.EnableAsync;
import org.springframework.transaction.annotation.EnableTransactionManagement;
 
@SpringBootApplication
@MapperScan("com.pengshu.magicwallet.mapper")
@PropertySource("classpath:authority.properties")
@EnableTransactionManagement
@EnableAsync //开启spring事件异步设置，加@Async注解
public class MagicWalletAdminApplication {
    public static void main(String[] args) {
        SpringApplication.run(MagicWalletAdminApplication.class, args);
    }
}
 

2.监听器类或方法添加@Async注解

打印结果：

订单创建成功

finished!

heling,您的订单:123456789创建成功! ----by sms

heling,您的订单:123456789创建成功! ----by wechat

如何制定监听器执行顺序？

package com.pengshu.magicwallet.admin.test;
 
import org.springframework.context.ApplicationEvent;
import org.springframework.context.event.SmartApplicationListener;
import org.springframework.stereotype.Component;
 
/**
* Author heling on 2019/1/9
* 微信监听器
* SmartApplicationListener可以设置顺序等
*/
@Component
public class WechatListener implements SmartApplicationListener {
 
    //设置监听优先级
    @Override
    public int getOrder() {
        return 1;
    }
 
    //监听器智能所在之一，能够根据事件类型动态监听
    @Override
    public boolean supportsEventType(Class<? extends ApplicationEvent> aClass) {
        return aClass == OrderCreateEvent.class;
    }
 
    //监听器智能所在之二，能够根据事件发布者类型动态监听
    @Override
    public boolean supportsSourceType(Class<?> aClass) {
        return aClass == OrderServiceImpl.class;
    }
 
    @Override
    public void onApplicationEvent(ApplicationEvent applicationEvent) {
        OrderCreateEvent event = (OrderCreateEvent) applicationEvent;
        //发送微信
        System.out.println(event.getContentList().get(0) + ",您的订单:" + event.getContentList().get(1) + "创建成功! ----by wechat");
 
    }
 
//    @Override
//    @Async
//    public void onApplicationEvent(OrderCreateEvent event) {
//
//        //发送微信
//        System.out.println(event.getContentList().get(0) + ",您的订单:" + event.getContentList().get(1) + "创建成功! ----by wechat");
//
//    }
}
package com.pengshu.magicwallet.admin.test;
 
import org.springframework.context.ApplicationEvent;
import org.springframework.context.event.SmartApplicationListener;
import org.springframework.stereotype.Component;
 
/**
* Author heling on 2019/1/9
* 短信监听器
*/
@Component
public class SmsListener implements SmartApplicationListener {
 
    @Override
    public int getOrder() {
        return 2;
    }
 
    @Override
    public boolean supportsEventType(Class<? extends ApplicationEvent> aClass) {
        return aClass == OrderCreateEvent.class;
    }
 
    @Override
    public boolean supportsSourceType(Class<?> aClass) {
        return aClass == OrderServiceImpl.class;
    }
 
    @Override
    public void onApplicationEvent(ApplicationEvent applicationEvent) {
        OrderCreateEvent event = (OrderCreateEvent) applicationEvent;
        //发送短信
        System.out.println(event.getContentList().get(0) + ",您的订单:" + event.getContentList().get(1) + "创建成功! ----by sms");
 
    }
 
//    @Override
//    @Async
//    public void onApplicationEvent(OrderCreateEvent event) {
//
//        //发送短信
//        System.out.println(event.getContentList().get(0) + ",您的订单:" + event.getContentList().get(1) + "创建成功! ----by sms");
//
//    }
}
 

打印结果：

订单创建成功

heling,您的订单:123456789创建成功! ----by wechat

heling,您的订单:123456789创建成功! ----by sms

finished!

 

在实现了SmartApplicationListener的监听器中，我们通过重写GetOrder方法来修改不同监听器的顺序，优先级越小，则越先被调用。通过配置不同的优先级，且让监听器之间阻塞调用。我们就能实现流水线式的有序事件调用，这在实际应用场景中还是蛮有意义的
————————————————
版权声明：本文为CSDN博主「超人不会飞2018」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/weixin_39035120/article/details/86225377


数据库
数据库设计的重要性和设计原则
说起数据库设计，相信大家都明白怎么回事，但说起数据库设计的重要性，我想大家也只是停留在概念上而已，到底如何重要？怎么重要呢？今天就将我至今为止的理解向大家阐述下。

一个不良的数据库设计，必然会造成很多问题，轻则增减字段，重则系统无法运行。我先来说说数据库设计不合理的表现吧：

1. 与需求不符

因为这个原因造成的改动量往往是最大。如果进入编码阶段的话，很可能会直接让你崩溃掉。

2. 性能低下

含有大数据量的表之间的关联过多；没有合理的字段设计来用于查询而造成的SQL查询语句很复杂；对于大数据量的表没有采用有效的手段去处理；滥用视图等。

3. 数据完整性丧失

含有主外键关系的表之间关联字段的设计方式不合理，造成更新与删除操作后程序容易出错或不完善；使用了已经删除或丢失掉的数据。

4. 可扩展性性太差

表设计的与业务绑定的太紧密、单一，造成表的可拓展性、可修改性太差，无法新需求的要求。

5. 非必要数据冗余量太大

没用的垃圾数据存储过多，不仅占用资源，还影响查询效率。

6. 不利于计算或统计

缺少必要的联系性或统计性字段或用于计算统计的字段分散于多个表中，造成计算统计的步骤繁琐，甚至无法计算统计。

7. 没有详尽的数据记录信息

缺少必要的字段，造成无法跟踪数据变化、用户操作，也无法进行数据分析。

8. 表之间的耦合性太大

多张表之间关联的过于紧密，造成一张表发生变化而影响到其他表。

9. 字段设计考虑不周

字段长度过短或字段类型过于明确，造成可发挥、可拓展的空间太小。

大多数的程序员对于软件开发的出发点认识不是很明确，总是认为实现功能才是重要的，在简单了解完基本需求后就急忙进入编码阶段，对于数据库设计思考的比较少、比较简单，大多设计都只停留在表面上，这往往是要命的，会为系统留下很多隐患。要么是写代码开发过程中才发现问题，要么就是系统上线运转后没多久就出现问题，还有可能给后期维护增加了很多工作量。如果到了那个时候再想修改数据库设计或进行优化等同于推翻重来。

数据库是整个软件应用的根基，是软件设计的起点，它起着决定性的质变作用，因此我们必须对数据库设计高度重视起来，培养设计良好数据库的习惯，是一个优秀的软件设计师所必须具备的基本素质条件！

那么我们要做到什么程度才是对的呢？下面就说说数据库设计的原则

1.  数据库设计最起码要占用整个项目开发的40%以上的时间

数据库是需求的直观反应和表现，因此设计时必须要切实符合用户的需求，要多次与用户沟通交流来细化需求，将需求中的要求和每一次的变化都要一一体现在数据库的设计当中。如果需求不明确，就要分析不确定的因素，设计表时就要事先预留出可变通的字段，正所谓“有备无患”。

2.  数据库设计不仅仅停留于页面demo的表面

页面内容所需要的字段，在数据库设计中只是一部分，还有系统运转、模块交互、中转数据、表之间的联系等等所需要的字段，因此数据库设计绝对不是简单的基本数据存储，还有逻辑数据存储。

3.  数据库设计完成后，项目80%的设计开发在你脑海中就已经完成了

每个字段的设计都是有他必要的意义的，你在设计每一个字段的同时，就应该已经想清楚程序中如何去运用这些字段，多张表的联系在程序中是如何体现的。换句话说，你完成数据库设计后，程序中所有的实现思路和实现方式在你的脑海中就已经考虑过了。如果达不到这种程度，那当进入编码阶段后，才发现要运用的技术或实现的方式数据库无法支持，这时再改动数据库就会很麻烦，会造成一系列不可预测的问题。

4.  数据库设计时就要考虑到效率和优化问题

一开始就要分析哪些表会存储较多的数据量，对于数据量较大的表的设计往往是粗粒度的，也会冗余一些必要的字段，已达到尽量用最少的表、最弱的表关系去存储海量的数据。并且在设计表时，一般都会对主键建立聚集索引，含有大数据量的表更是要建立索引以提供查询性能。对于含有计算、数据交互、统计这类需求时，还要考虑是否有必要采用存储过程。

5.  添加必要的（冗余）字段

像“创建时间”、“修改时间”、“备注”、“操作用户IP”和一些用于其他需求（如统计）的字段等，在每张表中必须都要有，不是说只有系统中用到的数据才会存到数据库中，一些冗余字段是为了便于日后维护、分析、拓展而添加的，这点是非常重要的，比如黑客攻击，篡改了数据，我们便就可以根据修改时间和操作用户IP来查找定位。

6.  设计合理的表关联

若多张表之间的关系复杂，建议采用第三张映射表来关联维护两张表之间的关系，以降低表之间的直接耦合度。若多张表涉及到大数据量的问题，表结构尽量简单，关联也要尽可能避免。

7.  设计表时不加主外键等约束性关联，系统编码阶段完成后再添加约束性关联

这样做的目的是有利于团队并行开发，减少编码时所遇到的问题，表之间的关系靠程序来控制。编码完成后再加关联并进行测试。不过也有一些公司的做法是干脆就不加表关联。

8.  选择合适的主键生成策略

主键生成策略大致可分：int自增长类型（identity、sequence）、手动增长类型（建立单独一张表来维护）、手动维护类型（如userId）、字符串类型（uuid、guid）。int型的优点是使用简单、效率高，但多表之间数据合并时就很容易出现问题，手动增长类型和字符串类型能很好解决多表数据合并的问题，但同样也都有缺点：前者的缺点是增加了一次数据库访问来获取主键，并且又多维护一张主键表，增加了复杂度；而后者是非常占用存储空间，且表关联查询的效率低下，索引的效率也不高，跟int类型正好相反。

终上所述，我们可见数据库设计在整个软件开发的起到的举足轻重的作用，尤其是我说的设计原则的第一点，数据库与需求是相辅相成的，我经常把软件开发比作汽车制造。汽车制造会经过图纸设计，模型制作，样车制造，小批量试生产，最后是批量生产等步骤。整个过程环环相扣，后一过程是建立在前一过程正确的前提基础之上的。如果在图纸设计阶段发现了一个纰漏，我们可以重新进行图纸设计，如果到了样车制造阶段发现这个错误，那么我们就要把从图纸设计到样车制造的阶段重来，越到后面发现设计上的问题，所付出的代价越大，修改的难度也越大。

数据库设计难度其实要比单纯的技术实现的难很多，他充分体现了一个人的全局设计能力和掌控能力，所以在今后的项目中大家一定要着重培养这方面的能力，这里我将我的经验分享给了大家，希望能对大家有所帮助。
三范式与反范式
一、 三范式
主键： 创建表时可以不设置主键 ， 但是没有设置主键的表 ， 底层会认为所有的键都是主键 ，所以在创建时使用了所有的字段创建索引 ， 在查询时索引的存在几乎没有意义 。
复合主键： 两个或两个以上的字段作为评价一条数据记录的唯一性标志 。
第一范式：强调列的原子性 ， 即：列不能分成几列
只要是关系型数据库 ， 就自然的遵循第一范式


第二范式：
①　首先满足第一范式
②　必须有主键
③　没有包含在主键中的列必须完全依赖于主键， 而不能只依赖主键的一部分

第三范式：
a.首先满足第一范式
b.也属于第二范式的一种情况
c.任何非主键字段不能依赖于其他非主键字段
三范式是在数据库初期使用（时间换取空间） ， 能外键关联就外键关联 ， 能不冗余数据设计 ， 就不冗余。
但是现在的系统对性能要求高， 对存储要求低（空间换时间）
二、 反范式
但是现在的系统对性能要求高， 对存储要求低（空间换时间） ， 所以出现了一套反范式
反范式： 只要违反了第二范式和第三范式 ， 就能做到空间换时间 ， 获的最大的效率 。
数据库优化
对mysql优化时一个综合性的技术，主要包括
a: 表的设计合理化(符合3NF)
b: 添加适当索引(index) [四种: 普通索引、主键索引、唯一索引unique、全文索引]
c: 分表技术(水平分割、垂直分割)
d: 读写[写: update/delete/add]分离
e: 存储过程 [模块化编程，可以提高速度]
f: 对mysql配置优化 [配置最大并发数my.ini, 调整缓存大小 ]
g: mysql服务器硬件升级
h: 定时的去清除不需要的数据,定时进行碎片整理(MyISAM)
Mysql千万级优化？
很多人第一反应是各种切分；我给的顺序是:
第一优化你的sql和索引；
第二加缓存，memcached,redis；
第三以上都做了后，还是慢，就做主从复制或主主复制，读写分离，可以在应用层做，效率高，也可以用三方工具，第三方工具推荐360的atlas,其它的要么效率不高，要么没人维护；
第四如果以上都做了还是慢，不要想着去做切分，mysql自带分区表，先试试这个，对你的应用是透明的，无需更改代码,但是sql语句是需要针对分区表做优化的，sql条件中要带上分区条件的列，从而使查询定位到少量的分区上，否则就会扫描全部分区，另外分区表还有一些坑，在这里就不多说了；
第五如果以上都做了，那就先做垂直拆分，其实就是根据你模块的耦合度，将一个大的系统分为多个小的系统，也就是分布式系统；
第六才是水平切分，针对数据量大的表，这一步最麻烦，最能考验技术水平，要选择一个合理的sharding key,为了有好的查询效率，表结构也要改动，做一定的冗余，应用也要改，sql中尽量带sharding key，将数据定位到限定的表上去查，而不是扫描全部的表；mysql数据库一般都是按照这个步骤去演化的，成本也是由低到高；有人也许要说第一步优化sql和索引这还用说吗？的确，大家都知道，但是很多情况下，这一步做的并不到位，甚至有的只做了根据sql去建索引，根本没对sql优化（中枪了没？），除了最简单的增删改查外，想实现一个查询，可以写出很多种查询语句，不同的语句，根据你选择的引擎、表中数据的分布情况、索引情况、数据库优化策略、查询中的锁策略等因素，最终查询的效率相差很大；优化要从整体去考虑，有时你优化一条语句后，其它查询反而效率被降低了，所以要取一个平衡点；即使精通mysql的话，除了纯技术面优化，还要根据业务面去优化sql语句，这样才能达到最优效果；你敢说你的sql和索引已经是最优了吗?再说一下不同引擎的优化，myisam读的效果好，写的效率差，这和它数据存储格式，索引的指针和锁的策略有关的，它的数据是顺序存储的（innodb数据存储方式是聚簇索引），他的索引btree上的节点是一个指向数据物理位置的指针，所以查找起来很快，（innodb索引节点存的则是数据的主键，所以需要根据主键二次查找）；myisam锁是表锁，只有读读之间是并发的，写写之间和读写之间（读和插入之间是可以并发的，去设置concurrent_insert参数，定期执行表优化操作，更新操作就没有办法了）是串行的，所以写起来慢，并且默认的写优先级比读优先级高，高到写操作来了后，可以马上插入到读操作前面去，如果批量写，会导致读请求饿死，所以要设置读写优先级或设置多少写操作后执行读操作的策略;myisam不要使用查询时间太长的sql，如果策略使用不当，也会导致写饿死，所以尽量去拆分查询效率低的sql,innodb一般都是行锁，这个一般指的是sql用到索引的时候，行锁是加在索引上的，不是加在数据记录上的，如果sql没有用到索引，仍然会锁定表,mysql的读写之间是可以并发的，普通的select是不需要锁的，当查询的记录遇到锁时，用的是一致性的非锁定快照读，也就是根据数据库隔离级别策略，会去读被锁定行的快照，其它更新或加锁读语句用的是当前读，读取原始行；因为普通读与写不冲突，所以innodb不会出现读写饿死的情况，又因为在使用索引的时候用的是行锁，锁的粒度小，竞争相同锁的情况就少，就增加了并发处理，所以并发读写的效率还是很优秀的，问题在于索引查询后的根据主键的二次查找导致效率低；ps:很奇怪，为什innodb的索引叶子节点存的是主键而不是像mysism一样存数据的物理地址指针吗？如果存的是物理地址指针不就不需要二次查找了吗，这也是我开始的疑惑，根据mysism和innodb数据存储方式的差异去想，你就会明白了，我就不费口舌了！所以innodb为了避免二次查找可以使用索引覆盖技术，无法使用索引覆盖的，再延伸一下就是基于索引覆盖实现延迟关联；不知道什么是索引覆盖的，建议你无论如何都要弄清楚它是怎么回事！尽你所能去优化你的sql吧！说它成本低，却又是一项费时费力的活，需要在技术与业务都熟悉的情况下，用心去优化才能做到最优，优化后的效果也是立竿见影的！
慢查询如何优化？
答: 建索引


Sql优化
1.对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。

2.应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如：
select id from t where num is null
可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：
select id from t where num=0

3.应尽量避免在 where 子句中使用!=或<>操作符，否则将引擎放弃使用索引而进行全表扫描。

4.应尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如：
select id from t where num=10 or num=20
可以这样查询：
select id from t where num=10
union all
select id from t where num=20

5.in 和 not in 也要慎用，否则会导致全表扫描，如：
select id from t where num in(1,2,3)
对于连续的数值，能用 between 就不要用 in 了：
select id from t where num between 1 and 3

6.下面的查询也将导致全表扫描：
select id from t where name like ‘%abc%’

7.应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如：
select id from t where num/2=100
应改为:
select id from t where num=100*2

8.应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如：
select id from t where substring(name,1,3)=‘abc’–name以abc开头的id
应改为:
select id from t where name like ‘abc%’

9.不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。

10.在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，
否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。

11.不要写一些没有意义的查询，如需要生成一个空表结构：
select col1,col2 into #t from t where 1=0
这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样：
create table #t(…)

12.很多时候用 exists 代替 in 是一个好的选择：
select num from a where num in(select num from b)
用下面的语句替换：
select num from a where exists(select 1 from b where num=a.num)

13.并不是所有索引对查询都有效，SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，
如一表中有字段sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。

14.索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，
因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。
一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。

15.尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。
这是因为引擎在处理查询和连接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。

16.尽可能的使用 varchar 代替 char ，因为首先变长字段存储空间小，可以节省存储空间，
其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。

17.任何地方都不要使用 select * from t ，用具体的字段列表代替“*”，不要返回用不到的任何字段。

18.避免频繁创建和删除临时表，以减少系统表资源的消耗。

19.临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件，最好使用导出表。

20.在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，
以提高速度；如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。

21.如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。

22.尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。

23.使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。

24.与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。
在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。
25.尽量避免大事务操作，提高系统并发能力。26.尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。
————————————————
单表优化
除非单表数据未来会一直不断上涨，否则不要一开始就考虑拆分，拆分会带来逻辑、部署、运维的各种复杂度，一般以整型值为主的表在千万级以下，字符串为主的表在五百万以下是没有太大问题的。而事实上很多时候MySQL单表的性能依然有不少优化空间，甚至能正常支撑千万级以上的数据量：

字段

①　尽量使用TINYINT、SMALLINT、MEDIUM_INT作为整数类型而非INT，如果非负则加上UNSIGNED
②　VARCHAR的长度只分配真正需要的空间
③　使用枚举或整数代替字符串类型
④　尽量使用TIMESTAMP而非DATETIME，
⑤　单表不要有太多字段，建议在20以内
⑥　避免使用NULL字段，很难查询优化且占用额外索引空间
⑦　用整型来存IP
索引

⑧　索引并不是越多越好，要根据查询有针对性的创建，考虑在WHERE和ORDER
⑨　BY命令上涉及的列建立索引，可根据EXPLAIN来查看是否用了索引还是全表扫描
⑩　应尽量避免在WHERE子句中对字段进行NULL值判断，否则将导致引擎放弃使用索引而进行全表扫描
⑪　值分布很稀少的字段不适合建索引，例如"性别"这种只有两三个值的字段
⑫　字符字段只建前缀索引
⑬　字符字段最好不要做主键
⑭　不用外键，由程序保证约束
⑮　尽量不用UNIQUE，由程序保证约束
⑯　使用多列索引时主意顺序和查询条件保持一致，同时删除不必要的单列索引
查询SQL

可通过开启慢查询日志来找出较慢的SQL
不做列运算：SELECT id WHERE age + 1 =
10，任何对列的操作都将导致表扫描，它包括数据库教程函数、计算表达式等等，查询时要尽可能将操作移至等号右边
sql语句尽可能简单：一条sql只能在一个cpu运算；大语句拆小语句，减少锁时间；一条大sql可以堵死整个库
不用SELECT *
OR改写成IN：OR的效率是n级别，IN的效率是log(n)级别，in的个数建议控制在200以内
不用函数和触发器，在应用程序实现
避免%xxx式查询
少用JOIN
使用同类型进行比较，比如用’123’和’123’比，123和123比
尽量避免在WHERE子句中使用!=或<>操作符，否则将引擎放弃使用索引而进行全表扫描
对于连续数值，使用BETWEEN不用IN：SELECT id FROM t WHERE num BETWEEN 1 AND 5
列表数据不要拿全表，要使用LIMIT来分页，每页数量也不要太大
引擎
目前广泛使用的是MyISAM和InnoDB两种引擎：

MyISAM
MyISAM引擎是MySQL 5.1及之前版本的默认引擎，它的特点是：

不支持行锁，读取时对需要读到的所有表加锁，写入时则对表加排它锁
不支持事务
不支持外键
不支持崩溃后的安全恢复
在表有读取查询的同时，支持往表中插入新纪录
支持BLOB和TEXT的前500个字符索引，支持全文索引
支持延迟更新索引，极大提升写入性能
对于不会进行修改的表，支持压缩表，极大减少磁盘空间占用
InnoDB
InnoDB在MySQL 5.5后成为默认索引，它的特点是：

支持行锁，采用MVCC来支持高并发
支持事务
支持外键
支持崩溃后的安全恢复
不支持全文索引
总体来讲，MyISAM适合SELECT密集型的表，而InnoDB适合INSERT和UPDATE密集型的表

系统调优参数
可以使用下面几个工具来做基准测试：

sysbench：一个模块化，跨平台以及多线程的性能测试工具
iibench-mysql：基于 Java 的 MySQL/Percona/MariaDB 索引进行插入性能测试工具
tpcc-mysql：Percona开发的TPC-C测试工具
具体的调优参数内容较多，具体可参考官方文档，这里介绍一些比较重要的参数：

back_log：back_log值指出在MySQL暂时停止回答新请求之前的短时间内多少个请求可以被存在堆栈中。也就是说，如果MySql的连接数据达到max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即back_log，如果等待连接的数量超过back_log，将不被授予连接资源。可以从默认的50升至500
wait_timeout：数据库连接闲置时间，闲置连接会占用内存资源。可以从默认的8小时减到半小时
max_user_connection: 最大连接数，默认为0无上限，最好设一个合理上限
thread_concurrency：并发线程数，设为CPU核数的两倍
skip_name_resolve：禁止对外部连接进行DNS解析，消除DNS解析时间，但需要所有远程主机用IP访问
key_buffer_size：索引块的缓存大小，增加会提升索引处理速度，对MyISAM表性能影响最大。对于内存4G左右，可设为256M或384M，通过查询show
status like ‘key_read%’，保证key_reads / key_read_requests在0.1%以下最好
innodb_buffer_pool_size：缓存数据块和索引块，对InnoDB表性能影响最大。通过查询show status like
‘Innodb_buffer_pool_read%’，保证 (Innodb_buffer_pool_read_requests –
Innodb_buffer_pool_reads) / Innodb_buffer_pool_read_requests越高越好
innodb_additional_mem_pool_size：InnoDB存储引擎用来存放数据字典信息以及一些内部数据结构的内存空间大小，当数据库对象非常多的时候，适当调整该参数的大小以确保所有数据都能存放在内存中提高访问效率，当过小的时候，MySQL会记录Warning信息到数据库的错误日志中，这时就需要该调整这个参数大小
innodb_log_buffer_size：InnoDB存储引擎的事务日志所使用的缓冲区，一般来说不建议超过32MB
query_cache_size：缓存MySQL中的ResultSet，也就是一条SQL语句执行的结果集，所以仅仅只能针对select语句。当某个表的数据有任何任何变化，都会导致所有引用了该表的select语句在Query
Cache中的缓存数据失效。所以，当我们的数据变化非常频繁的情况下，使用Query
Cache可能会得不偿失。根据命中率(Qcache_hits/(Qcache_hits+Qcache_inserts)*100))进行调整，一般不建议太大，256MB可能已经差不多了，大型的配置型静态数据可适当调大.
可以通过命令show status like 'Qcache_%'查看目前系统Query catch使用大小
read_buffer_size：MySql读入缓冲区大小。对表进行顺序扫描的请求将分配一个读入缓冲区，MySql会为它分配一段内存缓冲区。如果对表的顺序扫描请求非常频繁，可以通过增加该变量值以及内存缓冲区大小提高其性能
sort_buffer_size：MySql执行排序使用的缓冲大小。如果想要增加ORDER
BY的速度，首先看是否可以让MySQL使用索引而不是额外的排序阶段。如果不能，可以尝试增加sort_buffer_size变量的大小
read_rnd_buffer_size：MySql的随机读缓冲区大小。当按任意顺序读取行时(例如，按照排序顺序)，将分配一个随机读缓存区。进行排序查询时，MySql会首先扫描一遍该缓冲，以避免磁盘搜索，提高查询速度，如果需要排序大量数据，可适当调高该值。但MySql会为每个客户连接发放该缓冲空间，所以应尽量适当设置该值，以避免内存开销过大。
record_buffer：每个进行一个顺序扫描的线程为其扫描的每张表分配这个大小的一个缓冲区。如果你做很多顺序扫描，可能想要增加该值
thread_cache_size：保存当前没有与连接关联但是准备为后面新的连接服务的线程，可以快速响应连接的线程请求而无需创建新的
table_cache：类似于thread_cache_size，但用来缓存表文件，对InnoDB效果不大，主要用于MyISAM
升级硬件
Scale up，这个不多说了，根据MySQL是CPU密集型还是I/O密集型，通过提升CPU和内存、使用SSD，都能显著提升MySQL性能

读写分离
也是目前常用的优化，从库读主库写，一般不要采用双主或多主引入很多复杂性，尽量采用文中的其他方案来提高性能。同时目前很多拆分的解决方案同时也兼顾考虑了读写分离

缓存
缓存可以发生在这些层次：

MySQL内部：在系统调优参数介绍了相关设置
数据访问层：比如MyBatis针对SQL语句做缓存，而Hibernate可以精确到单个记录，这里缓存的对象主要是持久化对象Persistence
Object
应用服务层：这里可以通过编程手段对缓存做到更精准的控制和更多的实现策略，这里缓存的对象是数据传输对象Data Transfer
Object
Web层：针对web页面做缓存
浏览器客户端：用户端的缓存
可以根据实际情况在一个层次或多个层次结合加入缓存。这里重点介绍下服务层的缓存实现，目前主要有两种方式：

直写式（Write Through）：在数据写入数据库后，同时更新缓存，维持数据库与缓存的一致性。这也是当前大多数应用缓存框架如Spring
Cache的工作方式。这种实现非常简单，同步好，但效率一般。
回写式（Write Back）：当有数据要写入数据库时，只会更新缓存，然后异步批量的将缓存数据同步到数据库上。这种实现比较复杂，需要较多的应用逻辑，同时可能会产生数据库与缓存的不同步，但效率非常高。
表分区
MySQL在5.1版引入的分区是一种简单的水平拆分，用户需要在建表的时候加上分区参数，对应用是透明的无需修改代码

对用户来说，分区表是一个独立的逻辑表，但是底层由多个物理子表组成，实现分区的代码实际上是通过对一组底层表的对象封装，但对SQL层来说是一个完全封装底层的黑盒子。MySQL实现分区的方式也意味着索引也是按照分区的子表定义，没有全局索引



用户的SQL语句是需要针对分区表做优化，SQL条件中要带上分区条件的列，从而使查询定位到少量的分区上，否则就会扫描全部分区，可以通过EXPLAIN PARTITIONS来查看某条SQL语句会落在那些分区上，从而进行SQL优化，如下图5条记录落在两个分区上：

mysql> explain partitions select count(1) from user_partition where id in (1,2,3,4,5);
+----+-------------+----------------+------------+-------+---------------+---------+---------+------+------+--------------------------+
| id | select_type | table          | partitions | type  | possible_keys | key     | key_len | ref  | rows | Extra                    |
+----+-------------+----------------+------------+-------+---------------+---------+---------+------+------+--------------------------+
|  1 | SIMPLE      | user_partition | p1,p4      | range | PRIMARY       | PRIMARY | 8       | NULL |    5 | Using where; Using index |
+----+-------------+----------------+------------+-------+---------------+---------+---------+------+------+--------------------------+
1 row in set (0.00 sec)
1
2
3
4
5
6
7
分区的好处是：

可以让单表存储更多的数据
分区表的数据更容易维护，可以通过清楚整个分区批量删除大量数据，也可以增加新的分区来支持新插入的数据。另外，还可以对一个独立分区进行优化、检查、修复等操作
部分查询能够从查询条件确定只落在少数分区上，速度会很快
分区表的数据还可以分布在不同的物理设备上，从而搞笑利用多个硬件设备
可以使用分区表赖避免某些特殊瓶颈，例如InnoDB单个索引的互斥访问、ext3文件系统的inode锁竞争
可以备份和恢复单个分区
分区的限制和缺点

一个表最多只能有1024个分区
如果分区字段中有主键或者唯一索引的列，那么所有主键列和唯一索引列都必须包含进来
分区表无法使用外键约束
NULL值会使分区过滤无效
所有分区必须使用相同的存储引擎

分区的类型：

RANGE分区：基于属于一个给定连续区间的列值，把多行分配给分区
LIST分区：类似于按RANGE分区，区别在于LIST分区是基于列值匹配一个离散值集合中的某个值来进行选择
HASH分区：基于用户定义的表达式的返回值来进行选择的分区，该表达式使用将要插入到表中的这些行的列值进行计算。这个函数可以包含MySQL中有效的、产生非负整数值的任何表达式
KEY分区：类似于按HASH分区，区别在于KEY分区只支持计算一列或多列，且MySQL服务器提供其自身的哈希函数。必须有一列或多列包含整数值
分区适合的场景有：

最适合的场景数据的时间序列性比较强，则可以按时间来分区，如下所示：

CREATE TABLE members (
    firstname VARCHAR(25) NOT NULL,
    lastname VARCHAR(25) NOT NULL,
    username VARCHAR(16) NOT NULL,
    email VARCHAR(35),
    joined DATE NOT NULL
)
PARTITION BY RANGE( YEAR(joined) ) (
    PARTITION p0 VALUES LESS THAN (1960),
    PARTITION p1 VALUES LESS THAN (1970),
    PARTITION p2 VALUES LESS THAN (1980),
    PARTITION p3 VALUES LESS THAN (1990),
    PARTITION p4 VALUES LESS THAN MAXVALUE
);
1
2
3
4
5
6
7
8
9
10
11
12
13
14
查询时加上时间范围条件效率会非常高，同时对于不需要的历史数据能很容的批量删除。

如果数据有明显的热点，而且除了这部分数据，其他数据很少被访问到，那么可以将热点数据单独放在一个分区，让这个分区的数据能够有机会都缓存在内存中，查询时只访问一个很小的分区表，能够有效使用索引和缓存
另外MySQL有一种早期的简单的分区实现 - 合并表（merge table），限制较多且缺乏优化，不建议使用，应该用新的分区机制来替代

垂直拆分
垂直分库是根据数据库里面的数据表的相关性进行拆分，比如：一个数据库里面既存在用户数据，又存在订单数据，那么垂直拆分可以把用户数据放到用户库、把订单数据放到订单库。垂直分表是对数据表进行垂直拆分的一种方式，常见的是把一个多字段的大表按常用字段和非常用字段进行拆分，每个表里面的数据记录数一般情况下是相同的，只是字段不一样，使用主键关联

比如原始的用户表是：



垂直拆分后是：



垂直拆分的优点是：

可以使得行数据变小，一个数据块(Block)就能存放更多的数据，在查询时就会减少I/O次数(每次查询时读取的Block 就少)
可以达到最大化利用Cache的目的，具体在垂直拆分的时候可以将不常变的字段放一起，将经常改变的放一起
数据维护简单
缺点是：

主键出现冗余，需要管理冗余列
会引起表连接JOIN操作（增加CPU开销）可以通过在业务服务器上进行join来减少数据库压力
依然存在单表数据量过大的问题（需要水平拆分）
事务处理复杂
水平拆分
概述
水平拆分是通过某种策略将数据分片来存储，分库内分表和分库两部分，每片数据会分散到不同的MySQL表或库，达到分布式的效果，能够支持非常大的数据量。前面的表分区本质上也是一种特殊的库内分表

库内分表，仅仅是单纯的解决了单一表数据过大的问题，由于没有把表的数据分布到不同的机器上，因此对于减轻MySQL服务器的压力来说，并没有太大的作用，大家还是竞争同一个物理机上的IO、CPU、网络，这个就要通过分库来解决

前面垂直拆分的用户表如果进行水平拆分，结果是：



实际情况中往往会是垂直拆分和水平拆分的结合，即将Users_A_M和Users_N_Z再拆成Users和UserExtras，这样一共四张表

水平拆分的优点是:

不存在单库大数据和高并发的性能瓶颈
应用端改造较少
提高了系统的稳定性和负载能力
缺点是：

分片事务一致性难以解决
跨节点Join性能差，逻辑复杂
数据多次扩展难度跟维护量极大
分片原则

能不分就不分，参考单表优化
分片数量尽量少，分片尽量均匀分布在多个数据结点上，因为一个查询SQL跨分片越多，则总体性能越差，虽然要好于所有数据在一个分片的结果，只在必要的时候进行扩容，增加分片数量
分片规则需要慎重选择做好提前规划，分片规则的选择，需要考虑数据的增长模式，数据的访问模式，分片关联性问题，以及分片扩容问题，最近的分片策略为范围分片，枚举分片，一致性Hash分片，这几种分片都有利于扩容
尽量不要在一个事务中的SQL跨越多个分片，分布式事务一直是个不好处理的问题
查询条件尽量优化，尽量避免Select *
的方式，大量数据结果集下，会消耗大量带宽和CPU资源，查询尽量避免返回大量结果集，并且尽量为频繁使用的查询语句建立索引。
通过数据冗余和表分区赖降低跨库Join的可能
这里特别强调一下分片规则的选择问题，如果某个表的数据有明显的时间特征，比如订单、交易记录等，则他们通常比较合适用时间范围分片，因为具有时效性的数据，我们往往关注其近期的数据，查询条件中往往带有时间字段进行过滤，比较好的方案是，当前活跃的数据，采用跨度比较短的时间段进行分片，而历史性的数据，则采用比较长的跨度存储。

总体上来说，分片的选择是取决于最频繁的查询SQL的条件，因为不带任何Where语句的查询SQL，会遍历所有的分片，性能相对最差，因此这种SQL越多，对系统的影响越大，所以我们要尽量避免这种SQL的产生。

解决方案
由于水平拆分牵涉的逻辑比较复杂，当前也有了不少比较成熟的解决方案。这些方案分为两大类：客户端架构和代理架构。

客户端架构
通过修改数据访问层，如JDBC、Data Source、MyBatis，通过配置来管理多个数据源，直连数据库，并在模块内完成数据的分片整合，一般以Jar包的方式呈现

这是一个客户端架构的例子：



可以看到分片的实现是和应用服务器在一起的，通过修改Spring JDBC层来实现

客户端架构的优点是：

应用直连数据库，降低外围系统依赖所带来的宕机风险
集成成本低，无需额外运维的组件
缺点是：

限于只能在数据库访问层上做文章，扩展性一般，对于比较复杂的系统可能会力不从心
将分片逻辑的压力放在应用服务器上，造成额外风险
代理架构
通过独立的中间件来统一管理所有数据源和数据分片整合，后端数据库集群对前端应用程序透明，需要独立部署和运维代理组件

这是一个代理架构的例子：



代理组件为了分流和防止单点，一般以集群形式存在，同时可能需要Zookeeper之类的服务组件来管理

代理架构的优点是：

能够处理非常复杂的需求，不受数据库访问层原来实现的限制，扩展性强
对于应用服务器透明且没有增加任何额外负载
缺点是：

需部署和运维独立的代理中间件，成本高
应用需经过代理来连接数据库，网络上多了一跳，性能有损失且有额外风险
各方案比较



最好按大公司->社区->小公司->个人这样的出品方顺序来选择
选择口碑较好的，比如github星数、使用者数量质量和使用者反馈
开源的优先，往往项目有特殊需求可能需要改动源代码
按照上述思路，推荐以下选择：

客户端架构：ShardingJDBC
代理架构：MyCat或者Atlas

兼容MySQL且可水平扩展的数据库
目前也有一些开源数据库兼容MySQL协议，如：

TiDB
Cubrid
但其工业品质和MySQL尚有差距，且需要较大的运维投入，如果想将原始的MySQL迁移到可水平扩展的新数据库中，可以考虑一些云数据库：

阿里云PetaData
阿里云OceanBase
腾讯云DCDB
NoSQL
在MySQL上做Sharding是一种戴着镣铐的跳舞，事实上很多大表本身对MySQL这种RDBMS的需求并不大，并不要求ACID，可以考虑将这些表迁移到NoSQL，彻底解决水平扩展问题，例如：

日志类、监控类、统计类数据
非结构化或弱结构化数据
对事务要求不强，且无太多关联操作的数据
Mysql如何做高可用?
第一种：主从复制+读写分离
客户端通过Master对数据库进行写操作，slave端进行读操作，并可进行备份。Master出现问题后，可以手动将应用切换到slave端。

对于数据实时性要求不是特别严格的应用，只需要通过廉价的pc server来扩展Slave的数量，将读压力分散到多台Slave的机器上面，即可通过分散单台数据库服务器的读压力来解决数据库端的读性能瓶颈，毕竟在大多数数据库应用系统中的读压力要比写压力大的多。这在很大程度上解决了目前很多中小型网站的数据库压力瓶颈问题，甚至有些大型网站也在使用类似的方案解决数据库瓶颈问题。

第二种：Mysql Cluster

MySQL Cluster 由一组计算机构成，每台计算机上均运行着多种进程，包括 MySQL 服务器，NDB Cluster的数据节点，管理服务器，以及（可能）专门的数据访问程序。
由于MySQL Cluster架构复杂，部署费时（通常需要DBA几个小时的时间才能完成搭建），而依靠 MySQL Cluster Manager 只需一个命令即可完成，但 MySQL Cluster Manager 是收费的。并且业内资深人士认为NDB 不适合大多数业务场景，而且有安全问题。因此，使用的人数较少。

第三种：Heartbeat+双主从复制

heartbeat 是 Linux-HA 工程的一个组件,heartbeat 最核心的包括两个部分：心跳监测和资源接管。在指定的时间内未收到对方发送的报文，那么就认为对方失效，这时需启动资源接管模块来接管运 行在对方主机上的资源或者服务

第四种：HeartBeat+DRBD+Mysql


DRBD 是通过网络来实现块设备的数据镜像同步的一款开源 Cluster 软件，它自动完成网络中两个不同服务
器上的磁盘同步，相对于 binlog 日志同步，它是更底层的磁盘同步，理论上 DRDB 适合很多文件型系统的高可
用。

第五种：Lvs+keepalived+双主复制（常用）

Lvs 是一个虚拟的服务器集群系统，可以实现 LINUX 平台下的简单负载均衡。keepalived 是一个类似于
layer3, 4 & 5 交换机制的软件，主要用于主机与备机的故障转移，这是一种适用面很广的负载均衡和高可用方
案，最常用于 Web 系统。

参考：
https://blog.csdn.net/wzy0623/article/details/81015707


第六种:MariaDB Galera


MariaDB Galera Cluster 是一套在mysql innodb存储引擎上面实现multi-master及数据实时同步的系统架构，业务层面无需做读写分离工作，数据库读写压力都能按照既定的规则分发到 各个节点上去。在数据方面完全兼容 MariaDB 和 MySQL。
该架构主要有以下几种特性：
(1).同步复制 Synchronous replication
(2).Active-active multi-master 拓扑逻辑
(3).可对集群中任一节点进行数据读写
(4).自动成员控制，故障节点自动从集群中移除
(5).自动节点加入
(6).真正并行的复制，基于行级
(7).直接客户端连接，原生的 MySQL 接口
(8).每个节点都包含完整的数据副本
(9).多台数据库中数据同步由 wsrep 接口实现
其局限性体现在以下几点：
(1).目前的复制仅仅支持InnoDB存储引擎,任何写入其他引擎的表，包括mysql.*表将不会复制,但是DDL语句会被复制的,因此创建用户将会被复制,但是insert into mysql.user…将不会被复制的.
(2).DELETE操作不支持没有主键的表,没有主键的表在不同的节点顺序将不同,如果执行SELECT…LIMIT… 将出现不同的结果集.
(3).在多主环境下LOCK/UNLOCK TABLES不支持,以及锁函数GET_LOCK(), RELEASE_LOCK()…
(4).查询日志不能保存在表中。如果开启查询日志，只能保存到文件中。
(5).允许最大的事务大小由wsrep_max_ws_rows和wsrep_max_ws_size定义。任何大型操作将被拒绝。如大型的LOAD DATA操作。
(6).由于集群是乐观的并发控制，事务commit可能在该阶段中止。如果有两个事务向在集群中不同的节点向同一行写入并提交，失败的节点将中止。对 于集群级别的中止，集群返回死锁错误代码(Error: 1213 SQLSTATE: 40001 (ER_LOCK_DEADLOCK)).
(7).XA事务不支持，由于在提交上可能回滚。
(8).整个集群的写入吞吐量是由最弱的节点限制，如果有一个节点变得缓慢，那么整个集群将是缓慢的。为了稳定的高性能要求，所有的节点应使用统一的硬件。
(9).集群节点建议最少3个。
(10).如果DDL语句有问题将破坏集群。
Mysql存储原理？
MySQL 的常用引擎
1. InnoDB
InnoDB 的存储文件有两个，后缀名分别是 .frm 和 .idb，其中 .frm 是表的定义文件，而 idb 是数据文件。

InnoDB 中存在表锁和行锁，不过行锁是在命中索引的情况下才会起作用。

InnoDB 支持事务，且支持四种隔离级别（读未提交、读已提交、可重复读、串行化），默认的为可重复读；而在 Oracle 数据库中，只支持串行化级别和读已提交这两种级别，其中默认的为读已提交级别。

2. Myisam
Myisam 的存储文件有三个，后缀名分别是 .frm、.MYD、MYI，其中 .frm 是表的定义文件，.MYD 是数据文件，.MYI 是索引文件。

Myisam 只支持表锁，且不支持事务。Myisam 由于有单独的索引文件，在读取数据方面的性能很高 。

3. 存储结构
InnoDB 和 Myisam 都是用 B+Tree 来存储数据的。

MySQL 的数据、索引存储结构
1. 数据存储的原理（硬盘）
信息存储在硬盘里，硬盘是由很多的盘片组成，通过盘片表面的磁性物质来存储数据。

把盘片放在显微镜下放大，可以看到盘片表面是凹凸不平的，凸起的地方被磁化，代表数字 1，凹的地方没有被磁化，代表数字 0，因此硬盘可以通过二进制的形式来存储表示文字、图片等的信息。

硬盘有很多种，但是都是由盘片、磁头、盘片主轴、控制电机、磁头控制器、数据转换器、接口、缓存等几个部分组成。

所有的盘片都固定在一个旋转轴上，这个轴即盘片主轴。

所有的盘片之间是绝对平行的，在每个盘片的盘面上都有一个磁头，磁头与盘片之间的距离比头发丝的直径还小。

所有的磁头连在一个磁头控制器上，由磁头控制器负责各个磁头的运动，磁头可沿盘片的半径方向移动，实际上是斜切运动，每个磁头同一时刻必须是同轴的，即从正上方往下看，所有磁头任何时候都是重叠的。

由于技术的发展，目前已经有多磁头独立技术了，在此不考虑此种情况。

盘片以每分钟数千转到上万转的速度在高速运转，这样磁头就能对盘片上的指定位置进行数据的读写操作。

由于硬盘是高精密设备，尘埃是其大敌，所以必须完全密封。

2. 数据读写的原理
硬盘在逻辑上被划分为磁道、柱面以及扇区。

磁头靠近主轴接触的表面，即线速度最小的地方，是一个特殊的区域，它不存放任何数据，称为启停区或者着陆区，启停区外就是数据区。

在最外圈，离主轴最远的地方是 “0” 磁道，硬盘数据的存放就是从最外圈开始的。

在硬盘中还有一个叫 “0” 磁道检测器的构件，它是用来完成硬盘的初始定位。

盘面

硬盘的盘片一般用铝合金材料做基片，硬盘的每一个盘片都有上下两个盘面，一般每个盘面都会得到利用，都可以存储数据，成为有效盘面，也有极个别的硬盘盘面数为单数。

每一个这样的有效盘面都有一个盘面号，按顺序从上至下从 0 开始编号。

在硬盘系统中，盘面号又叫磁头号，因为每一个有效盘面都有一个对应的读写磁头，硬盘的盘片组在 2-14 片不等，通常有 2-3 个盘片。

磁道

磁盘在格式化时被划分成许多同心圆，这些同心圆轨迹叫做磁道。

磁道从外向内从 0 开始顺序编号，硬盘的每一个盘面有 300-1024 个磁道，新式大容量硬盘每面的磁道数更多，信息以脉冲串的形式记录在这些轨迹中，这些同心圆不是连续记录数据，而是被划分成一段段的圆弧。

这些圆弧的角速度一样，由于径向长度不一样，所以线速度也不一样，外圈的线速度较内圈的线速度大，即同样的转速度下，外圈在同样时间段里，划过的圆弧长度要比内圈划过的圆弧长度大。

每段圆弧叫做一个扇区，扇区从 1 开始编号，每个扇区中的数据作为一个单元同时读出或写入。

磁道是看不见的，只是盘面上以特殊形式磁化了的一些磁化区，在磁盘格式化时就已规划完毕。

柱面

所有盘面上的同一磁道构成一个圆柱，通常称作柱面。

每个圆柱上的磁头由上而下从 0 开始编号，数据的读 / 写按柱面进行，即磁头读 / 写数据时首先在同一柱面内从 0 磁头开始进行操作，依次向下在同一柱面的不同盘面即磁头上进行操作。

只有在同一柱面所有的磁头全部读 / 写完毕后磁头才转移到下一柱面（同心圆再往里的柱面），因为选取磁头只需要通过电子切换即可，而选取柱面则必须机械切换，电子切换相当快，比在机械上的磁头向邻近磁道移动快得多。

所以，数据的读 / 写按柱面进行，而不按盘面进行，也就是说，一个磁道写满数据后，就在同一柱面的下一个盘面来写，一个柱面写满后，才移到下一个扇区开始写数据，读数据也按照这种方式进行，这样就提高了硬盘的读 / 写效率。

扇区

操作系统以扇区形式将信息存储在硬盘上，每个扇区包括 512 个字节的数据和一些其他信息，一个扇区有两个主要部分：存储数据地点的标识符和存储数据的数据段。

标识符就是扇区头标，包括组成扇区三维地址的三个数字：盘面号，柱面号，扇区号（块号）。

数据段可分为数据和保护数据的纠错码（ECC）。在初始准备期间，计算机用 512 个虚拟信息字节（实际数据的存放地）和与这些虚拟信息字节相应的 ECC 数字填入这个部分。

3. 访盘请求完成过程
1）确定磁盘地址（柱面号，磁头号，扇区号），内存地址（源 / 目）：

当需要从磁盘读取数据的时候，系统会将数据的逻辑地址传递个磁盘，磁盘的控制电路按照寻址逻辑将逻辑地址翻译成物理地址，即确定要读的数据在哪个磁道，哪个扇区。

2）为了读取这个扇区的数据，需要将磁头放到这个扇区上方，为了实现这一点：

A. 首先必须找到柱面，即磁头需要移动对准相应磁道，这个过程叫做寻道，所耗费时间叫做寻道时间。

B. 然后目标扇区旋转到磁头下，即磁盘旋转将目标扇区旋转到磁头下，这个过程耗费的时间叫做旋转时间。

3）即一次访盘请求（读 / 写）完成过程由三个动作组成：

A. 寻道（时间）：磁头移动定位到指定磁道。

B. 旋转延迟（时间）：等待指定扇区从磁头下旋转经过。

C. 数据传输（时间）：数据在磁盘与内存之间的实际传输。

4. 磁盘的读写原理
系统将文件存储到磁盘上时，按柱面、磁头、扇区的方式进行，即最先是第 1 磁道的第一磁头下的所有扇区，然后是同一柱面的下一个磁头……

一个柱面存储满后就推进到下一个柱面，直到把文件内容全部写入磁盘。

系统也以相同的顺序读出数据，读出数据时通过告诉磁盘控制器要读出扇区所在柱面号、磁头号和扇区号（物理地址的三个组成部分）进行。

5. 减少 I/O 的预读原理
由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费的时间，磁盘的存取速度往往是主存的几百分之一。

因此，为了提高效率，要尽量减少磁盘的 I/O。

磁盘往往不是严格地按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。

这样做的理论依据是计算机科学中著名的局部性原理：

当一个数据被用到时，其附近的数据一般来说也会被马上使用。

程序运行期间所需要的数据通常比较集中。

由于磁盘顺序读取的效率很高（不需要寻道时间，只需要很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高 I/O 效率。

预读的长度一般为页（Page）的整数倍。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储分割为连续的大小相等的块。

每个存储块称为一页（在许多操作系统中，页的大小通常为 4k），主存和磁盘以页为单位交换数据，当程序要读取的数据不在主存中时，会触发一个缺页异常。

此时系统会向磁盘发出读盘信息，磁盘会找到数据的起始位置并向后连续读取一页或几页的数据载入内存中，然后异常返回，程序继续运行。

6. MySQL 的索引
索引是一种用来实现 MySQL 高效获取数据的数据结构。

我们通常所说的在某个字段上建索引，意思就是让 MySQL 对该字段以索引这种数据结构来存储，然后查找的时候就有对应的查找算法。

建索引的根本目的是为了查找的优化，特别是当数据很庞大的时候，一般的查找算法有顺序查找、折半查找、快速查找等。

但是每种查找算法都只能应用于特定的数据结构之上，例如顺序查找依赖于顺序结构，折半查找通过二叉查找树或红黑树实现二分搜索。因此在数据之外，数据库系统还维护着满足特定查找算法的数据结构。

这些数据结构以某种方式引用数据，这样就可以在这些数据结构上实现高级查找算法，这种数据结构就是索引。

7. MySQL 的 B+Tree
目前大多数数据库系统及文件系统都采用 B-Tree 或其变种 B+Tree 作为索引结构。

B+ 树索引是 B+ 树在数据库中的一种实现，是最常见也是数据库中使用最为频繁的一种索引。B+ 树中的 B 代表平衡，而不是二叉。

因为 B+ 树是从最早的平衡二叉树演化而来的。B+ 树是由二叉查找树、平衡二叉树（AVLTree）和平衡多路查找树（B-Tree）逐步优化而来。

二叉查找树：左子树的键值小于根的键值，右子树的键值大于根的键值。

AVL 树：平衡二叉树（AVL 树）在符合二叉查找树的条件下，还满足任何节点的两个子树的高度最大差为 1。

平衡多路查找树（B-Tree）：为磁盘等外存储设备设计的一种平衡查找树。

系统从磁盘读取数据到内存时是以磁盘块（block）为基本单位的，位于同一磁盘块中的数据会被一次性读取出来，而不是按需读取。

InnoDB 存储引擎使用页作为数据读取单位，页是其磁盘管理的最小单位，默认 page 大小是 16k。

系统的一个磁盘块的存储空间往往没有这么大，因此 InnoDB 每次申请磁盘空间时都会是若干地址连续磁盘块来达到页的大小 16KB。

InnDB 在把磁盘数据读入到磁盘时会以页为基本单位，在查询数据时如果一个页中的每条数据都能助于定位数据记录的位置，这将会减少磁盘 I/O 的次数，提高查询效率。

B-Tree 结构的数据可以让系统高效的找到数据所在的磁盘块。

为了描述 B-Tree，首先定义一条数据记录为一个二元组 [key, data]，key 为记录的键值，对于不同数据记录，key 是互不相同的；data 为数据记录除 key 外的数据。

那么 B-Tree 是满足下列条件的数据结构：

d 为大于 1 的一个正整数，称为 B-Tree 的度。
h 为一个正整数，称为 B-Tree 的高度。
每个非叶子节点由 n-1 个 key 和 n 个指针组成，其中 d<=n<=2d。
每个叶子节点最少包含一个 key 和两个指针，最多包含 2d-1 个 key 和 2d 个指针，叶节点的指针均为 null 。
所有叶节点具有相同的深度，等于树高 h。
key 和指针互相间隔，节点两端是指针。
一个节点中的 key 从左到右非递减排列。
所有节点组成树结构。
每个指针要么为 null，要么指向另外一个节点。
如果某个指针在节点 node 最左边且不为 null，则其指向节点的所有 key 小于 v(key1)，其中 v(key1) 为
node 的第一个 key 的值。
如果某个指针在节点 node 最右边且不为 null，则其指向节点的所有 key 大于 v(keym)，其中 v(keym) 为
node 的最后一个 key 的值。
如果某个指针在节点 node 的左右相邻 key 分别是 keyi 和 keyi+1 且不为 null，则其指向节点的所有 key
小于 v(keyi+1) 且大于 v(keyi)。
B-Tree 中的每个节点根据实际情况可以包含大量的关键字信息和分支，例：




每个节点占用一个盘块的磁盘空间，一个节点上有两个升序排序的关键字和三个指向子树根节点的指针，指针存储的是子节点所在磁盘块的地址。

两个关键词划分成的三个范围域对应三个指针指向的子树的数据的范围域。

以根节点为例，关键字为 17 和 35，P1 指针指向的子树的数据范围为小于 17，P2 指针指向的子树的数据范围为 17~35，P3 指针指向的子树的数据范围为大于 35。

模拟查找关键字 29 的过程：

根据根节点找到磁盘块 1，读入内存。【磁盘 I/O 操作第 1 次】
比较关键字 29 在区间（17,35），找到磁盘块 1 的指针 P2。
根据 P2 指针找到磁盘块 3，读入内存。【磁盘 I/O 操作第 2 次】
比较关键字 29 在区间（26,30），找到磁盘块 3 的指针 P2。
根据 P2 指针找到磁盘块 8，读入内存。【磁盘 I/O 操作第 3 次】
在磁盘块 8 中的关键字列表中找到关键字 29。
MySQL 的 InnoDB 存储引擎在设计时是将根节点常驻内存的，因此力求达到树的深度不超过 3，也就是说 I/O 不需要超过 3 次。

分析上面过程，发现需要 3 次磁盘 I/O 操作，和 3 次内存查找操作。由于内存中的关键字是一个有序表结构，可以利用二分法查找提高效率。

而 3 次磁盘 I/O 操作是影响整个 B-Tree 查找效率的决定因素。

B-Tree 相对于 AVLTree 缩减了节点个数，使每次磁盘 I/O 取到内存的数据都发挥了作用，从而提高了查询效率。

B+Tree 是在 B-Tree 基础上的一种优化，使其更适合实现外存储索引结构，InnoDB 存储引擎就是用 B+Tree 实现其索引结构。

在 B-Tree 中，每个节点中有 key，也有 data，而每一个页的存储空间是有限的，如果 data 数据较大时将会导致每个节点（即一个页）能存储的 key 的数量很小。

当存储的数据量很大时同样会导致 B-Tree 的深度较大，增大查询时的磁盘 I/O 次数，进而影响查询效率。

在 B+Tree 中，所有数据记录节点都是按照键值大小顺序存放在同一层的叶子节点上，而非叶子节点上只存储 key 值信息，这样可以大大加大每个节点存储的 key 值数量，降低 B+Tree 的高度。

B+Tree 在 B-Tree 的基础上有两点变化：

数据是存在叶子节点中的；
数据节点之间是有指针指向的。
由于 B+Tree 的非叶子节点只存储键值信息，假设每个磁盘块能存储 4 个键值及指针信息，则变成 B+Tree 后其结构如下图所示：




通常在 B+Tree 上有两个头指针，一个指向根节点，另一个指向关键字最小的叶子节点，而且所有叶子节点（即数据节点）之间是一种链式环结构。

因此可以对 B+Tree 进行两种查找运算：一种是对于主键的范围查找和分页查找，另一种是从根节点开始，进行随机查找。

8. Myisam 中的 B+Tree
Myisam 引擎也是采用的 B+Tree 结构来作为索引结构。

由于 Myisam 中的索引和数据分别存放在不同的文件，所以在索引树中的叶子节点中存的数据是该索引对应的数据记录的地址，由于数据与索引不在一起，所以 Myisam 是非聚簇索引。




9. InnoDB 中的 B+Tree
InnoDB 是以 ID 为索引的数据存储。

采用 InnoDB 引擎的数据存储文件有两个，一个定义文件，一个是数据文件。

InnoDB 通过 B+Tree 结构对 ID 建索引，然后在叶子节点中存储记录。




若建索引的字段不是主键 ID，则对该字段建索引，然后在叶子节点中存储的是该记录的主键，然后通过主键索引找到对应的记录。

MySQL 的相关优化
1. MySQL 性能优化：组成、表的设计
开启查询缓存。避免某些 SQL 函数直接在 SQL 语句中使用，从而导致 Mysql 缓存失效。

避免画蛇添足。目的是什么就取什么，例如某个逻辑是只需要判断是否存在女性，若是查到了一条即可，勿要全部都查一遍，此时要善用 limit。

建合适的索引。所以要建在合适的地方，合适的对象上。经常操作 / 比较 / 判断的字段应该建索引。

字段大小合宜。字段的取值是有限而且是固定的，这种情况下可以用 enum，IP 字段可以用 unsigned int 来存储。

表的设计。垂直分割表，使得固定表与变长表分割，从而降低表的复杂度和字段的数目。

2. SQL 语句优化：避免全表扫描
建索引：一般在 where 及 order by 中涉及到的列上建索引，尽量不要对可以重复的字段建索引。

尽量避免在 where 中使用 !（<>）或 or，也不要进行 null 值判断。

尽量避免在 where 中对字段进行函数操作、表达式操作。

尽量避免使用 like- %，在此种情况下可以进行全文检索。
Mysql数据库隔离级别？
一、数据库事务隔离级别

数据库事务的隔离级别有4个，由低到高依次为Read uncommitted 、Read committed 、Repeatable read 、Serializable ，这四个级别可以逐个解决脏读 、不可重复读 、幻读 这几类问题

注意：我们讨论隔离级别的场景，主要是在多个事务并发 的情况下，因此，接下来的讲解都围绕事务并发。

Read uncommitted 读未提交
公司发工资了，领导把5000元打到singo的账号上，但是该事务并未提交，而singo正好去查看账户，发现工资已经到账，是5000元整，非常高 兴。可是不幸的是，领导发现发给singo的工资金额不对，是2000元，于是迅速回滚了事务，修改金额后，将事务提交，最后singo实际的工资只有 2000元，singo空欢喜一场。



出现上述情况，即我们所说的脏读 ，两个并发的事务，“事务A：领导给singo发工资”、“事务B：singo查询工资账户”，事务B读取了事务A尚未提交的数据。

当隔离级别设置为Read uncommitted 时，就可能出现脏读，如何避免脏读，请看下一个隔离级别。

Read committed 读提交（oracle默认隔离级别）
singo拿着工资卡去消费，系统读取到卡里确实有2000元，而此时她的老婆也正好在网上转账，把singo工资卡的2000元转到另一账户，并在 singo之前提交了事务，当singo扣款时，系统检查到singo的工资卡已经没有钱，扣款失败，singo十分纳闷，明明卡里有钱，为 何…

出现上述情况，即我们所说的不可重复读 ，两个并发的事务，“事务A：singo消费”、“事务B：singo的老婆网上转账”，事务A事先读取了数据，事务B紧接了更新了数据，并提交了事务，而事务A再次读取该数据时，数据已经发生了改变。

当隔离级别设置为Read committed 时，避免了脏读，但是可能会造成不可重复读。

大多数数据库的默认级别就是Read committed，比如Sql Server , Oracle。如何解决不可重复读这一问题，请看下一个隔离级别。

Repeatable read 重复读（Mysql的默认隔离级别）
当隔离级别设置为Repeatable read 时，可以避免不可重复读。当singo拿着工资卡去消费时，一旦系统开始读取工资卡信息（即事务开始），singo的老婆就不可能对该记录进行修改，也就是singo的老婆不能在此时转账。

虽然Repeatable read避免了不可重复读，但还有可能出现幻读 。

singo的老婆工作在银行部门，她时常通过银行内部系统查看singo的信用卡消费记录。有一天，她正在查询到singo当月信用卡的总消费金额 （select sum(amount) from transaction where month = 本月）为80元，而singo此时正好在外面胡吃海塞后在收银台买单，消费1000元，即新增了一条1000元的消费记录（insert transaction … ），并提交了事务，随后singo的老婆将singo当月信用卡消费的明细打印到A4纸上，却发现消费总额为1080元，singo的老婆很诧异，以为出 现了幻觉，幻读就这样产生了。

注：Mysql的默认隔离级别就是Repeatable read。

Serializable 序列化
Serializable 是最高的事务隔离级别，同时代价也花费最高，性能很低，一般很少使用，在该级别下，事务顺序执行，不仅可以避免脏读、不可重复读，还避免了幻像读。

二、脏读、幻读、不可重复读

1.脏读：
脏读就是指当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。

2.不可重复读：
是指在一个事务内，多次读同一数据。在这个事务还没有结束时，另外一个事务也访问该同一数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改，那么第一个事务两次读到的的数据可能是不一样的。这样就发生了在一个事务内两次读到的数据是不一样的，因此称为是不可重复读。（即不能读到相同的数据内容）
例如，一个编辑人员两次读取同一文档，但在两次读取之间，作者重写了该文档。当编辑人员第二次读取文档时，文档已更改。原始读取不可重复。如果只有在作者全部完成编写后编辑人员才可以读取文档，则可以避免该问题。

3.幻读:
是指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象
发生了幻觉一样。
例如，一个编辑人员更改作者提交的文档，但当生产部门将其更改内容合并到该文档的主复本时，发现作者已将未编辑的新材料添加到该文档中。如果在编辑人员和生产部门完成对原始文档的处理之前，任何人都不能将新材料添加到文档中，则可以避免该问题。
什么是事务？
事务是访问数据库的一个操作序列，数据库应用系统通过事务集来完成对数据库的存取。事务的正确执行使得数据库从一种状态转换为另一种状态。


事务的特性？
1、原子性

即不可分割，事务要么全部被执行，要么全部不执行。如果事务的所有子事务全部提交成功，则所有的数据库操作被提交，数据库状态发生变化；如果有子事务失败，则其他子事务的数据库操作被回滚，即数据库回到事务执行前的状态，不会发生状态转换

2、一致性

事务的执行使得数据库从一种正确状态转换成另外一种正确状态

3、隔离性

在事务正确提交之前，不允许把事务对该数据的改变提供给任何其他事务，即在事务正确提交之前，它可能的结果不应该显示给其他事务

4、持久性

事务正确提交之后，其结果将永远保存在数据库之中，即使在事务提交之后有了其他故障，事务的处理结果也会得到保存
Mysql如何实现ACID？
1)原子性（Atomicity，或称不可分割性）
定义
原子性是指一个事务是一个不可分割的工作单位，其中的操作要么都做，要么都不做；如果事务中一个sql语句执行失败，则已执行的语句也必须回滚，数据库退回到事务前的状态。

实现原理：undo log
在说明原子性原理之前，首先介绍一下MySQL的事务日志。MySQL的日志有很多种，如二进制日志、错误日志、查询日志、慢查询日志等，此外InnoDB存储引擎还提供了两种事务日志：redo log(重做日志)和undo log(回滚日志)。其中redo log用于保证事务持久性；undo log则是事务原子性和隔离性实现的基础。

下面说回undo log。实现原子性的关键，是当事务回滚时能够撤销所有已经成功执行的sql语句。InnoDB实现回滚，靠的是undo log：当事务对数据库进行修改时，InnoDB会生成对应的undo log；如果事务执行失败或调用了rollback，导致事务需要回滚，便可以利用undo log中的信息将数据回滚到修改之前的样子。

undo log属于逻辑日志，它记录的是sql执行相关的信息。当发生回滚时，InnoDB会根据undo log的内容做与之前相反的工作：对于每个insert，回滚时会执行delete；对于每个delete，回滚时会执行insert；对于每个update，回滚时会执行一个相反的update，把数据改回去。

以update操作为例：当事务执行update时，其生成的undo log中会包含被修改行的主键(以便知道修改了哪些行)、修改了哪些列、这些列在修改前后的值等信息，回滚时便可以使用这些信息将数据还原到update之前的状态。
2)一致性（Consistency）
1. 基本概念
一致性是指事务执行结束后，数据库的完整性约束没有被破坏，事务执行的前后都是合法的数据状态。数据库的完整性约束包括但不限于：实体完整性（如行的主键存在且唯一）、列完整性（如字段的类型、大小、长度要符合要求）、外键约束、用户自定义完整性（如转账前后，两个账户余额的和应该不变）。

2. 实现
可以说，一致性是事务追求的最终目标：前面提到的原子性、持久性和隔离性，都是为了保证数据库状态的一致性。此外，除了数据库层面的保障，一致性的实现也需要应用层面进行保障。

实现一致性的措施包括：

保证原子性、持久性和隔离性，如果这些特性无法保证，事务的一致性也无法保证
数据库本身提供保障，例如不允许向整形列插入字符串值、字符串长度不能超过列的限制等
应用层面进行保障，例如如果转账操作只扣除转账者的余额，而没有增加接收者的余额，无论数据库实现的多么完美，也无法保证状态的一致
3)隔离性（Isolation）
1. 定义
与原子性、持久性侧重于研究事务本身不同，隔离性研究的是不同事务之间的相互影响。隔离性是指，事务内部的操作与其他事务是隔离的，并发执行的各个事务之间不能互相干扰。严格的隔离性，对应了事务隔离级别中的Serializable (可串行化)，但实际应用中出于性能方面的考虑很少会使用可串行化。

隔离性追求的是并发情形下事务之间互不干扰。简单起见，我们仅考虑最简单的读操作和写操作(暂时不考虑带锁读等特殊操作)，那么隔离性的探讨，主要可以分为两个方面：

(一个事务)写操作对(另一个事务)写操作的影响：锁机制保证隔离性
(一个事务)写操作对(另一个事务)读操作的影响：MVCC保证隔离性
2. 锁机制
首先来看两个事务的写操作之间的相互影响。隔离性要求同一时刻只能有一个事务对数据进行写操作，InnoDB通过锁机制来保证这一点。

锁机制的基本原理可以概括为：事务在修改数据之前，需要先获得相应的锁；获得锁之后，事务便可以修改数据；该事务操作期间，这部分数据是锁定的，其他事务如果需要修改数据，需要等待当前事务提交或回滚后释放锁。

行锁与表锁

按照粒度，锁可以分为表锁、行锁以及其他位于二者之间的锁。表锁在操作数据时会锁定整张表，并发性能较差；行锁则只锁定需要操作的数据，并发性能好。但是由于加锁本身需要消耗资源(获得锁、检查锁、释放锁等都需要消耗资源)，因此在锁定数据较多情况下使用表锁可以节省大量资源。MySQL中不同的存储引擎支持的锁是不一样的，例如MyIsam只支持表锁，而InnoDB同时支持表锁和行锁，且出于性能考虑，绝大多数情况下使用的都是行锁。
4)持久性（Durability）
定义
持久性是指事务一旦提交，它对数据库的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响。
2. 实现原理：redo log
redo log和undo log都属于InnoDB的事务日志。下面先聊一下redo log存在的背景。

InnoDB作为MySQL的存储引擎，数据是存放在磁盘中的，但如果每次读写数据都需要磁盘IO，效率会很低。为此，InnoDB提供了缓存(Buffer Pool)，Buffer Pool中包含了磁盘中部分数据页的映射，作为访问数据库的缓冲：当从数据库读取数据时，会首先从Buffer Pool中读取，如果Buffer Pool中没有，则从磁盘读取后放入Buffer Pool；当向数据库写入数据时，会首先写入Buffer Pool，Buffer Pool中修改的数据会定期刷新到磁盘中（这一过程称为刷脏）。

Buffer Pool的使用大大提高了读写数据的效率，但是也带了新的问题：如果MySQL宕机，而此时Buffer Pool中修改的数据还没有刷新到磁盘，就会导致数据的丢失，事务的持久性无法保证。

于是，redo log被引入来解决这个问题：当数据修改时，除了修改Buffer Pool中的数据，还会在redo log记录这次操作；当事务提交时，会调用fsync接口对redo log进行刷盘。如果MySQL宕机，重启时可以读取redo log中的数据，对数据库进行恢复。redo log采用的是WAL（Write-ahead logging，预写式日志），所有修改先写入日志，再更新到Buffer Pool，保证了数据不会因MySQL宕机而丢失，从而满足了持久性要求。

既然redo log也需要在事务提交时将日志写入磁盘，为什么它比直接将Buffer Pool中修改的数据写入磁盘(即刷脏)要快呢？主要有以下两方面的原因：

（1）刷脏是随机IO，因为每次修改的数据位置随机，但写redo log是追加操作，属于顺序IO。

（2）刷脏是以数据页（Page）为单位的，MySQL默认页大小是16KB，一个Page上一个小修改都要整页写入；而redo log中只包含真正需要写入的部分，无效IO大大减少。

redo log与binlog
我们知道，在MySQL中还存在binlog(二进制日志)也可以记录写操作并用于数据的恢复，但二者是有着根本的不同的：
（1）作用不同：redo log是用于crash recovery的，保证MySQL宕机也不会影响持久性；binlog是用于point-in-time recovery的，保证服务器可以基于时间点恢复数据，此外binlog还用于主从复制。

（2）层次不同：redo log是InnoDB存储引擎实现的，而binlog是MySQL的服务器层(可以参考文章前面对MySQL逻辑架构的介绍)实现的，同时支持InnoDB和其他存储引擎。

（3）内容不同：redo log是物理日志，内容基于磁盘的Page；binlog的内容是二进制的，根据binlog_format参数的不同，可能基于sql语句、基于数据本身或者二者的混合。

（4）写入时机不同：binlog在事务提交时写入；redo log的写入时机相对多元：

前面曾提到：当事务提交时会调用fsync对redo log进行刷盘；这是默认情况下的策略，修改innodb_flush_log_at_trx_commit参数可以改变该策略，但事务的持久性将无法保证。
除了事务提交时，还有其他刷盘时机：如master thread每秒刷盘一次redo log等，这样的好处是不一定要等到commit时刷盘，commit速度大大加快。

InnoDB锁机制
一、为什么要加锁
锁机制用于管理对共享资源的并发访问。
当多个用户并发地存取数据时，在数据库中就可能会产生多个事务同时操作同一行数据的情况，若对并发操作不加控制就可能会读取和存储不正确的数据，破坏数据的一致性。

一种典型的并发问题——丢失更新（其他锁问题及解决方法会在后面说到）：

注：RR默认隔离级别下，为更清晰体现时间先后，暂时忽略锁等待，不影响最终效果~

异常结果：商品S库存更新为120，但实际上针对商品S进行了两次入库操作，最终商品S库存应为100+10+20=130，但实际结果为120，首先提交的事务A的更新『丢失了』！！！所以就需要锁机制来保证这种情况不会发生。
二、InnoDB锁类型概述

简介（后面会分别详细说到）：

1、乐观锁与悲观锁是两种并发控制的思想，可用于解决丢失更新问题：
乐观锁会“乐观地”假定大概率不会发生并发更新冲突，访问、处理数据过程中不加锁，只在更新数据时再根据版本号或时间戳判断是否有冲突，有则处理，无则提交事务；

悲观锁会“悲观地”假定大概率会发生并发更新冲突，访问、处理数据前就加排他锁，在整个数据处理过程中锁定数据，事务提交或回滚后才释放锁；

2、InnoDB支持多种锁粒度，默认使用行锁，锁粒度最小，锁冲突发生的概率最低，支持的并发度也最高，但系统消耗成本也相对较高；
3、共享锁与排他锁是InnoDB实现的两种标准的行锁；
4、InnoDB有三种锁算法——记录锁、gap间隙锁、还有结合了记录锁与间隙锁的next-key锁，InnoDB对于行的查询加锁是使用的是next-key locking这种算法，一定程度上解决了幻读问题；
5、意向锁是为了支持多种粒度锁同时存在；（1.0版本不重点介绍，如有兴趣可参看知乎推荐回答https://www.zhihu.com/questio...）
三、行锁详解
InnoDB默认使用行锁，实现了两种标准的行锁——共享锁与排他锁；


注意：
1、除了显式加锁的情况，其他情况下的加锁与解锁都无需人工干预。
2、InnoDB所有的行锁算法都是基于索引实现的，锁定的也都是索引或索引区间（这一点会在第六章节『锁算法』中详细说到）；

共享锁与排它锁兼容性示例（使用默认的RR隔离级别，图中数字从小到大标识操作执行先后顺序）：



四、当前读与快照读
1、当前读：即加锁读，读取记录的最新版本，会加锁保证其他并发事务不能修改当前记录，直至获取锁的事务释放锁；

使用当前读的操作主要包括：显式加锁的读操作与插入/更新/删除等写操作，如下所示：

select * from table where ? lock in share mode;
select * from table where ? for update;
insert into table values (…);
update table set ? where ?;
delete from table where ?;
注：当Update SQL被发给MySQL后，MySQL Server会根据where条件，读取第一条满足条件的记录，然后InnoDB引擎会将第一条记录返回，并加锁，待MySQL Server收到这条加锁的记录之后，会再发起一个Update请求，更新这条记录。一条记录操作完成，再读取下一条记录，直至没有满足条件的记录为止。因此，Update操作内部，就包含了当前读。同理，Delete操作也一样。Insert操作会稍微有些不同，简单来说，就是Insert操作可能会触发Unique Key的冲突检查，也会进行一个当前读。
2、快照读：即不加锁读，读取记录的快照版本而非最新版本，通过MVCC实现；

InnoDB默认的RR事务隔离级别下，不显式加『lock in share mode』与『for update』的『select』操作都属于快照读，保证事务执行过程中只有第一次读之前提交的修改和自己的修改可见，其他的均不可见；
五、MVCC
MVCC『多版本并发控制』，与之对应的是『基于锁的并发控制』；
MVCC的最大好处：读不加任何锁，读写不冲突，对于读操作多于写操作的应用，极大的增加了系统的并发性能；

InnoDB默认的RR事务隔离级别下，不显式加『lock in share mode』与『for update』的『select』操作都属于快照读，使用MVCC，保证事务执行过程中只有第一次读之前提交的修改和自己的修改可见，其他的均不可见；

关于InnoDB MVCC的实现原理，在《高性能Mysql》一书中有一些说明，网络上也大多沿用这一套理论，但这套理论与InnoDB的实际实现还是有一定差距的，但不妨我们通过它初步理解MVCC的实现机制，所以我在此贴上此书中的说明；


六、锁算法
InnoDB主要实现了三种行锁算法：


InnoDB所有的行锁算法都是基于索引实现的，锁定的也都是索引或索引区间；

不同的事务隔离级别、不同的索引类型、是否为等值查询，使用的行锁算法也会有所不同；下面仅以InnoDB默认的RR隔离级别、等值查询为例，介绍几种行锁算法：


1、等值查询使用聚簇索引


注： InnoDB表是索引组织表，根据主键索引构造一棵B+树，叶子节点存放的是整张表的行记录数据，且按主键顺序存放；我这里做了一个表格模拟主键索引的叶子节点，使用主键索引查询，就会锁住相关主键索引，锁住了索引也就锁住了行记录，其他并发事务就无法修改此行数据，直至提交事务释放锁，保证了并发情况下数据的一致性；
2、等值查询使用唯一索引


注：辅助索引的叶子节点除了存放辅助索引值，也存放了对应主键索引值；锁定时会锁定辅助索引与主键索引；
3、等值查询使用辅助索引


注：Gap锁，锁定的是索引记录之间的间隙，是防止幻读的关键；如果没有上图中绿色标识的Gap Lock，其他并发事务在间隙中插入了一条记录如：『insert into stock (id,sku_id) values(2,103);』并提交，那么在此事务中重复执行上图中SQL，就会查询出并发事务新插入的记录，即出现幻读；（幻读是指在同一事务下，连续执行两次同样的SQL语句可能导致不同的结果，第二次的SQL语句可能返回之前不存在的行记录）加上Gap Lock后，并发事务插入新数据前会先检测间隙中是否已被加锁，防止幻读的出现；
更多锁示例可参看博客：https://yq.aliyun.com/article...
更多锁算法详解可参看何博士博客：http://hedengcheng.com/?p=771

七、锁问题
注：其实InnoDB默认的RR事务隔离级别已经为我们做了大多数的事，业务中更多需要关心『丢失更新』这种问题，通常使用乐观锁方式解决；我们在读操作时一般不会使用加锁读，但MVCC并不能完全解读幻读问题，其他并发事务是可以插入符合当前事务查询条件的数据，只是当前事务因为读快照数据无法查看到，这种情况下应该使用唯一索引等方式保证不会重复插入重复的业务数据，在此不再赘述~

乐观锁、悲观锁
什么是乐观锁、悲观锁？
乐观锁
总是假设最好的情况，每次去读数据的时候都认为别人不会修改，所以不会上锁， 但是在更新的时候会判断一下在此期间有没有其他线程更新该数据， 可以使用版本号机制和CAS算法实现。 乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。 在Java中java.util.concurrent.atomic包下面的原子变量类就是基于CAS实现的乐观锁。

 

悲观锁
总是假设最坏的情况，每次去读数据的时候都认为别人会修改，所以每次在读数据的时候都会上锁， 这样别人想读取数据就会阻塞直到它获取锁 （共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。 传统的关系型数据库里边就用到了很多悲观锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。 Java中synchronized和ReentrantLock等独占锁就是悲观锁思想的实现。
为什么要用乐观锁与悲观锁？
乐观锁和悲观锁是两种思想，用于解决并发场景下的数据竞争问题。
   当我们并发访问或更新数据库时，有可能会出现脏读（Dirty Read）、不可重复读（Unrepeatable Read）、幻读（Phantom Read）、更新丢失（Lost update）等数据不一致情况，为了解决这些问题，mysql引入了多种锁的概念。
乐观锁实现方式？
乐观锁的实现方式主要有两种：CAS机制和版本号机制
1、CAS（Compare And Swap）
CAS操作包括了3个操作数：
需要读写的内存位置(V)
进行比较的预期值(A)
拟写入的新值(B)
CAS操作逻辑如下：如果内存位置V的值等于预期的A值，则将该位置更新为新值B，否则不进行任何操作。许多CAS的操作是自旋的：如果操作不成功，会一直重试，直到操作成功为止。
这里引出一个新的问题，既然CAS包含了Compare和Swap两个操作，它又如何保证原子性呢？答案是：CAS是由CPU支持的原子操作，其原子性是在硬件层面进行保证的。
下面以Java中的自增操作(i++)为例，看一下悲观锁和CAS分别是如何保证线程安全的。我们知道，在Java中自增操作不是原子操作，它实际上包含三个独立的操作：
读取i值；
加1；
将新值写回i
因此，如果并发执行自增操作，可能导致计算结果的不准确。在下面的代码示例中：value1没有进行任何线程安全方面的保护，value2使用了乐观锁(CAS)，value3使用了悲观锁(synchronized)。
运行程序，使用1000个线程同时对value1、value2和value3进行自增操作，可以发现：value2和value3的值总是等于1000，而value1的值常常小于1000。
public class Test {

    //value1：线程不安全
    private static int value1 = 0;
    //value2：使用乐观锁
    private static AtomicInteger value2 = new AtomicInteger(0);
    //value3：使用悲观锁
    private static int value3 = 0;
    private static synchronized void increaseValue3(){
        value3++;
    }

    public static void main(String[] args) throws Exception {
        //开启1000个线程，并执行自增操作
        for(int i = 0; i < 1000; ++i){
            new Thread(new Runnable() {
                @Override
                public void run() {
                    try {
                        Thread.sleep(100);
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
                    value1++;
                    value2.getAndIncrement();
                    increaseValue3();
                }
            }).start();
        }
        //打印结果
        Thread.sleep(1000);
        System.out.println("线程不安全：" + value1);
        System.out.println("乐观锁(AtomicInteger)：" + value2);
        System.out.println("悲观锁(synchronized)：" + value3);
    }}
首先来介绍AtomicInteger。AtomicInteger是java.util.concurrent.atomic包提供的原子类，利用CPU提供的CAS操作来保证原子性；除了AtomicInteger外，还有AtomicBoolean、AtomicLong、AtomicReference等众多原子类。
下面看一下AtomicInteger的源码，了解下它的自增操作getAndIncrement()是如何实现的（源码以Java7为例，Java8有所不同，但思想类似）。
public class AtomicInteger extends Number implements java.io.Serializable {
    //存储整数值，volatile保证可视性
    private volatile int value;
    //Unsafe用于实现对底层资源的访问
    private static final Unsafe unsafe = Unsafe.getUnsafe();

    //valueOffset是value在内存中的偏移量
    private static final long valueOffset;
    //通过Unsafe获得valueOffset
    static {
        try {
            valueOffset = unsafe.objectFieldOffset(AtomicInteger.class.getDeclaredField("value"));
        } catch (Exception ex) { throw new Error(ex); }
    }

    public final boolean compareAndSet(int expect, int update) {
        return unsafe.compareAndSwapInt(this, valueOffset, expect, update);
    }

    public final int getAndIncrement() {
        for (;;) {
            int current = get();
            int next = current + 1;
            if (compareAndSet(current, next))
                return current;
        }
    }}
源码分析说明如下：

getAndIncrement()实现的自增操作是自旋CAS操作：在循环中进行compareAndSet，如果执行成功则退出，否则一直执行。


其中compareAndSet是CAS操作的核心，它是利用Unsafe对象实现的。


Unsafe又是何许人也呢？Unsafe是用来帮助Java访问操作系统底层资源的类（如可以分配内存、释放内存），通过Unsafe，Java具有了底层操作能力，可以提升运行效率；强大的底层资源操作能力也带来了安全隐患(类的名字Unsafe也在提醒我们这一点)，因此正常情况下用户无法使用。AtomicInteger在这里使用了Unsafe提供的CAS功能。


valueOffset可以理解为value在内存中的偏移量，对应了CAS三个操作数(V/A/B)中的V；偏移量的获得也是通过Unsafe实现的。


value域的volatile修饰符：Java并发编程要保证线程安全，需要保证原子性、可视性和有序性；CAS操作可以保证原子性，而volatile可以保证可视性和一定程度的有序性；在AtomicInteger中，volatile和CAS一起保证了线程安全性。关于volatile作用原理的说明涉及到Java内存模型(JMM)，这里不详细展开。

说完了AtomicInteger，再说synchronized。synchronized通过对代码块加锁来保证线程安全：在同一时刻，只能有一个线程可以执行代码块中的代码。synchronized是一个重量级的操作，不仅是因为加锁需要消耗额外的资源，还因为线程状态的切换会涉及操作系统核心态和用户态的转换；不过随着JVM对锁进行的一系列优化(如自旋锁、轻量级锁、锁粗化等)，synchronized的性能表现已经越来越好。
2、版本号机制
除了CAS，版本号机制也可以用来实现乐观锁。版本号机制的基本思路是在数据中增加一个字段version，表示该数据的版本号，每当数据被修改，版本号加1。当某个线程查询数据时，将该数据的版本号一起查出来；当该线程更新数据时，判断当前版本号与之前读取的版本号是否一致，如果一致才进行操作。
需要注意的是，这里使用了版本号作为判断数据变化的标记，实际上可以根据实际情况选用其他能够标记数据版本的字段，如时间戳等。
下面以“更新玩家金币数”为例（数据库为MySQL，其他数据库同理），看看悲观锁和版本号机制是如何应对并发问题的。
考虑这样一种场景：游戏系统需要更新玩家的金币数，更新后的金币数依赖于当前状态(如金币数、等级等)，因此更新前需要先查询玩家当前状态。
下面的实现方式，没有进行任何线程安全方面的保护。如果有其他线程在query和update之间更新了玩家的信息，会导致玩家金币数的不准确。
@Transactionalpublic void updateCoins(Integer playerId){
    //根据player_id查询玩家信息
    Player player = query("select coins, level from player where player_id = {0}", playerId);
    //根据玩家当前信息及其他信息，计算新的金币数
    Long newCoins = ……;
    //更新金币数
    update("update player set coins = {0} where player_id = {1}", newCoins, playerId);}
为了避免这个问题，悲观锁通过加锁解决这个问题，代码如下所示。在查询玩家信息时，使用select …… for update进行查询；该查询语句会为该玩家数据加上排它锁，直到事务提交或回滚时才会释放排它锁；在此期间，如果其他线程试图更新该玩家信息或者执行select for update，会被阻塞
@Transactionalpublic void updateCoins(Integer playerId){
    //根据player_id查询玩家信息（加排它锁）
    Player player = queryForUpdate("select coins, level from player where player_id = {0} for update", playerId);
    //根据玩家当前信息及其他信息，计算新的金币数
    Long newCoins = ……;
    //更新金币数
    update("update player set coins = {0} where player_id = {1}", newCoins, playerId);}
版本号机制则是另一种思路，它为玩家信息增加一个字段：version。在初次查询玩家信息时，同时查询出version信息；在执行update操作时，校验version是否发生了变化，如果version变化，则不进行更新。
@Transactionalpublic void updateCoins(Integer playerId){
    //根据player_id查询玩家信息，包含version信息
    Player player = query("select coins, level, version from player where player_id = {0}", playerId);
    //根据玩家当前信息及其他信息，计算新的金币数
    Long newCoins = ……;
    //更新金币数，条件中增加对version的校验
    update("update player set coins = {0} where player_id = {1} and version = {2}", newCoins, playerId, player.version);}

悲观锁处理流程
①　在对任意记录进行修改前，先尝试为该记录加上排他锁（exclusive locking）
②　如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常
③　如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了
④　其间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常

mysql悲观锁实现
在使用mysql悲观锁之前，我们需要关闭mysql数据库中的“自动提交”属性（set autocommit=0;），因为在mysql中默认使用的是"autocommit模式"。在默认模式下，执行完一个数据库更新操作后，mysql会立即将结果进行提交。

 

悲观锁使用的示例代码如下：

开始事务。begin;/begin work;/start transaction; (三者选一即可)
查询商品信息。select status from item where id=1 for update;
插入订单数据。insert into order (id,item_id) values (null,1);
修改商品状态。update item set status=2;
事务提交。commit;/commit work;（二选一即可）
select…for update
      上面的查询语句中，我们使用了select…for update的方式，这样就通过开启排他锁的方式实现了悲观锁。此时在item表中，id为1的那条数据就被我们锁定了，其它的事务必须等本次事务提交之后才能执行。这样我们可以保证当前的数据不会被其它事务修改。    

      使用select…for update会把数据给锁住，不过我们需要注意一些锁的级别，MySQL InnoDB默认Row-Level Lock，所以只有「明确」地指定主键/索引，MySQL 才会执行Row lock (只锁住被选取的数据) ，否则MySQL 将会执行Table Lock (将整个数据表单给锁住)。
乐观锁与悲观锁优缺点和适用场景？
应用场景：
当竞争不激烈 (出现并发冲突的概率小)时，乐观锁更有优势，因为悲观锁会锁住代码块或数据，其他线程无法同时访问，影响并发，而且加锁和释放锁都需要消耗额外的资源。

当竞争激烈(出现并发冲突的概率大)时，悲观锁更有优势，因为乐观锁在执行更新时频繁失败，需要不断重试，浪费CPU资源。

 

悲观锁优点
悲观并发控制实际上是“先取锁再访问”的保守策略，为数据处理的安全提供了保证。
悲观锁基于DB层面实现，对业务代码无入侵，使用方便
悲观锁缺点
悲观锁适用于可靠的持续性连接，诸如C/S应用。 对于Web应用的HTTP连接，先天不适用
锁的使用意味着性能的损耗，在高并发、锁定持续时间长的情况下，尤其严重。 Web应用的性能瓶颈多在数据库处，使用悲观锁，进一步收紧了瓶颈
非正常中止情况下的解锁机制，设计和实现起来很麻烦，成本还很高
不够严谨的设计下，可能产生莫名其妙的，不易被发现的死锁问题
悲观的缺陷是不论是页锁还是行锁，加锁的时间可能会很长，这样可能会长时间的限制其他用户的访问，也就是说悲观锁的并发访问性不好


下面是CAS一些不那么完美的地方（乐观锁的缺点）：

1、ABA问题
假设有两个线程——线程1和线程2，两个线程按照顺序进行以下操作：

线程1读取内存中数据为A；
线程2将该数据修改为B；
线程2将该数据修改为A；
线程1对数据进行CAS操作
在第(4)步中，由于内存中数据仍然为A，因此CAS操作成功，但实际上该数据已经被线程2修改过了。这就是ABA问题。

在AtomicInteger的例子中，ABA似乎没有什么危害。但是在某些场景下，ABA却会带来隐患，例如栈顶问题：一个栈的栈顶经过两次(或多次)变化又恢复了原值，但是栈可能已发生了变化。

对于ABA问题，比较有效的方案是引入版本号，内存中的值每发生一次变化，版本号都+1；在进行CAS操作时，不仅比较内存中的值，也会比较版本号，只有当二者都没有变化时，CAS才能执行成功。Java中的AtomicStampedReference类便是使用版本号来解决ABA问题的。

2、高竞争下的开销问题
在并发冲突概率大的高竞争环境下，如果CAS一直失败，会一直重试，CPU开销较大。针对这个问题的一个思路是引入退出机制，如重试次数超过一定阈值后失败退出。当然，更重要的是避免在高竞争环境下使用乐观锁。

3、功能限制
CAS的功能是比较受限的，例如CAS只能保证单个变量（或者说单个内存值）操作的原子性，这意味着：(1)原子性不一定能保证线程安全，例如在Java中需要与volatile配合来保证线程安全；(2)当涉及到多个变量(内存值)时，CAS也无能为力。

除此之外，CAS的实现需要硬件层面处理器的支持，在Java中普通用户无法直接使用，只能借助atomic包下的原子类使用，灵活性受到限制。

死锁
一、 什么是死锁

死锁是指两个或两个以上的进程在执行过程中,因争夺资源而造成的一种互相等待的现象,若无外力作用,它们都将无法推进下去.此时称系统处于死锁状态或系统产生了死锁,这些永远在互相等的进程称为死锁进程.

二、 死锁产生的四个必要条件

•互斥条件：指进程对所分配到的资源进行排它性使用，即在一段时间内某资源只由一个进程占用。如果此时还有其它进程请求资源，则请求者只能等待，直至占有资源的进程用毕释放

•请求和保持条件：指进程已经保持至少一个资源，但又提出了新的资源请求，而该资源已被其它进程占有，此时请求进程阻塞，但又对自己已获得的其它资源保持不放

•不剥夺条件：指进程已获得的资源，在未使用完之前，不能被剥夺，只能在使用完时由自己释放

•环路等待条件：指在发生死锁时，必然存在一个进程——资源的环形链，即进程集合{P0，P1，P2，···，Pn}中的P0正在等待一个P1占用的资源；P1正在等待P2占用的资源，……，Pn正在等待已被P0占用的资源

这四个条件是死锁的必要条件，只要系统发生死锁，这些条件必然成立，而只要上述条件之一不满足，就不会发生死锁。

三、 如何处理死锁

1) 锁模式

1.共享锁（S）

由读操作创建的锁，防止在读取数据的过程中，其它事务对数据进行更新；其它事务可以并发读取数据。共享锁可以加在表、页、索引键或者数据行上。在SQL SERVER默认隔离级别下数据读取完毕后就会释放共享锁，但可以通过锁提示或设置更高的事务隔离级别改变共享锁的释放时间。

2.独占锁(X)

对资源独占的锁，一个进程独占地锁定了请求的数据源，那么别的进程无法在此数据源上获得任何类型的锁。独占锁一致持有到事务结束。

3.更新锁(U)

更新锁实际上并不是一种独立的锁，而是共享锁与独占锁的混合。当SQL SERVER执行数据修改操作却首先需要搜索表以找到需要修改的资源时，会获得更新锁。

更新锁与共享锁兼容，但只有一个进程可以获取当前数据源上的更新锁，

其它进程无法获取该资源的更新锁或独占锁，更新锁的作用就好像一个序列化阀门（serialization gate），将后续申请独占锁的请求压入队列中。持有更新锁的进程能够将其转换成该资源上的独占锁。更新锁不足以用于更新数据—实际的数据修改仍需要用到独占锁。对于独占锁的序列化访问可以避免转换死锁的发生，更新锁会保留到事务结束或者当它们转换成独占锁时为止。

4. 意向锁(IX,IU,IS)

意向锁并不是独立的锁定模式，而是一种指出哪些资源已经被锁定的机制。

如果一个表页上存在独占锁，那么另一个进程就无法获得该表上的共享表锁，这种层次关系是用意向锁来实现的。进程要获得独占页锁、更新页锁或意向独占页锁，首先必须获得该表上的意向独占锁。同理，进程要获得共享行锁，必须首先获得该表的意向共享锁，以防止别的进程获得独占表锁。

5. 特殊锁模式(Sch_s,Sch_m,BU)

SQL SERVER提供3种额外的锁模式：架构稳定锁、架构修改锁、大容量更新锁。

6.转换锁(SIX,SIU,UIX)

转换锁不会由SQL SERVER 直接请求，而是从一种模式转换到另一种模式所造成的。SQL SERVER 2008支持3种类型的转换锁：SIX、SIU、UIX.其中最常见的是SIX锁，如果事务持有一个资源上的共享锁（S），然后又需要一个IX锁，此时就会出现SIX。

7.键范围锁

键范围锁是在可序列化隔离级别中锁定一定范围内数据的锁。保证在查询数据的键范围内不允许插入数据。

SQL SERVER 锁模式

缩写

锁模式

说明

S

Shared

允许其他进程读取但不能修改锁定的资源

X

Exclusive

防止别的进程读取或者修改锁定资源中的数据

U

Update

防止其它进程获取更新锁或独占锁；在搜索要修改的数据时使用

IS

Intent shared

表示该资源的一个组件被共享锁锁定了。只有在表或页级别才能获得这类锁

IU

Intent update

表示该资源的一个组件被更新锁锁定了。只有在表或页级别才能获得这类锁

IX

Intent exclusive

表示该资源的一个组件被独占锁锁定了。只有在表或页级别才能获得这类锁

SIX

Shared with intent exclusive

表示一个正持有共享锁的资源还有一个组件（一页或一行）被独占锁锁定了

SIU

Shared with intent Update

表示一个正持有共享锁的资源还有一个组件（一页或一行）被更新锁锁定了

UIX

Update with intent exclusive

表示一个正持有更新锁的资源还有一个组件（一页或一行）被独占锁锁定了

Sch-S

Schema stability

表示一个使用该表的查询正在被编译

Sch-M

Schema modification

表示表的结构正在被修改

BU

Bulk Update

在一个大容量复制操作将数据导入表中并且（手动或自动）应用了TABLOCK查

询提示时使用

2) 锁粒度

SQL SERVER 可以在表、页、行等级别锁定用户的数据资源即非系统资源（系统资源是用闩锁来保护的）。此外SQL SERVER 还可以锁定索引键和索引键范围。

通过sys.dm_tran_locks视图可以查看谁被锁定了（如行，键，页）、锁的模式以及特定资源的标志符。基于sys.dm_tran_locks视图创建如下视图用于查看锁定的资源以及锁模式（通过这个视图可以查看事务锁定的表、页、行以及加在数据资源上的锁类型）。

?
1
2
3
4
5
6
7
8
9
10
11
12
13
14
CREATEVIEW dblocks AS
SELECTrequest_session_id ASspid,
DB_NAME(resource_database_id)AS dbname,
CASEWHEN resource_type='object'
THENOBJECT_NAME(resource_associated_entity_id)
WHENresource_associated_entity_id=0 THEN'n/a'
ELSEOBJECT_NAME(p.object_id) ENDAS entity_name,
index_id,
resource_typeAS RESOURCE,
resource_descriptionAS DESCRIPTION,
request_modeAS mode,
request_statusAS STATUS
FROMsys.dm_tran_locks t LEFTJOIN sys.partitions p ON p.partition_id=t.resource_associated_entity_id
WHEREresource_database_id=DB_ID()
3) 如何跟踪死锁

通过选择sql server profiler 事件中的如下选项就可以跟踪到死锁产生的相关语句。

4) 死锁案例分析

在该案例中process65db88, process1d0045948为语句1的进程，process629dc8 为语句2的进程； 语句2获取了1689766页上的更新锁，在等待1686247页上的更新锁；而语句1则获取了1686247页上的更新锁在等待1689766页上的更新锁，两个语句等待的资源形成了一个环路，造成死锁。

5) 如何解决死锁

针对如上死锁案例，分析其对应语句执行计划如下：

通过执行计划可以看出，在查找需要更新的数据时使用的是索引扫描，比较耗费性能，这样就造成锁定资源时间过长，增加了语句并发执行时产生死锁的概率。

处理方式：

1． 在表上建立一个聚集索引。

2． 对语句更新的相关字段建立包含索引。

优化后该语句执行计划如下：

优化后的执行计划使用了索引查找，将大幅提升该查询语句的性能，降低了锁定资源的时间，同时也减少了锁定资源的范围，这样就降低了锁资源循环等待事件发生的概率，对于预防死锁的发生会有一定的作用。

死锁是无法完全避免的，但如果应用程序适当处理死锁，对涉及的任何用户及系统其余部分的影响可降至最低（适当处理是指发生错误1205时，应用程序重新提交批处理，第二次尝试大多能成功。一个进程被杀死，它的事务被取消，它的锁被释放，死锁中涉及到的另一个进程就可以完成它的工作并释放锁，所以就不具备产生另一个死锁的条件了。）

四、 如何预防死锁

阻止死锁的途径就是避免满足死锁条件的情况发生，为此我们在开发的过程中需要遵循如下原则：

1.尽量避免并发的执行涉及到修改数据的语句。

2.要求每一个事务一次就将所有要使用到的数据全部加锁，否则就不允许执行。

3.预先规定一个加锁顺序，所有的事务都必须按照这个顺序对数据执行封锁。如不同的过程在事务内部对对象的更新执行顺序应尽量保证一致。

4.每个事务的执行时间不可太长，对程序段的事务可考虑将其分割为几个事务。在事务中不要求输入，应该在事务之前得到输入，然后快速执行事务。

5.使用尽可能低的隔离级别。

6.数据存储空间离散法。该方法是指采用各种手段，将逻辑上在一个表中的数据分散的若干离散的空间上去，以便改善对表的访问性能。主要通过将大表按行或者列分解为若干小表，或者按照不同的用户群两种方法实现。

7.编写应用程序，让进程持有锁的时间尽可能短，这样其它进程就不必花太长的时间等待锁被释放。



补充：


死锁的概念：

如果一组进程中的每一个进程都在等待仅由该组进程中的其他进程才能引发的事件，那么改组进程是死锁的。

死锁的常见表现：

死锁不仅会发生多个进程中，也会发生在一个进程中。

（1）多进程死锁：有进程A，进程B，进程A拥有资源1，需要请求正在被进程B占有的资源2。而进程B拥有资源2，请求正在被进程A战友的资源1。两个进程都在等待对方释放资源后请求该资源，而相互僵持，陷入死锁。

（2）单进程死锁：进程A拥有资源1，而它又在请求资源1，而它所请求的资源1必须等待该资源使用完毕得到释放后才可被请求。这样，就陷入了自己的死锁。

产生死锁的原因：

（1）进程推进顺序不当造成死锁。

（2）竞争不可抢占性资源引起死锁。

（3）竞争可消耗性资源引起死锁。

死锁的四个必要条件（四个条件四者不可缺一）：

（1）互斥条件。某段时间内，一个资源一次只能被一个进程访问。

（2）请求和保持条件。进程A已经拥有至少一个资源，此时又去申请其他资源，而该资源又正在被进程使用，此时请求进程阻塞，但对自己已经获得的资源保持不放。

（3）不可抢占资源。进程已获得的资源在未使用完不能被抢占，只能在自己使用完时由自己释放。

（4）循环等待序列。存在一个循环等待序列P0P1P2……Pn，P0请求正在被进程P1占有的资源，P1请求正在被P2占有的资源……Pn正在请求被进程P0占有的资源。

解除死锁的两种方法：

（1）终止（或撤销）进程。终止（或撤销）系统中的一个或多个死锁进程，直至打破循环环路，使系统从死锁状态中解除出来。

（2）抢占资源。从一个或多个进程中抢占足够数量的资源，分配给死锁进程，以打破死锁状态。
索引
索引是什么？
索引是一种优化查询的数据结构


索引类型？
普通索引 。
是最基本的索引，它没有任何限制.

唯一索引
与前面的普通索引类似，不同的就是：索引列的值必须唯一，但允许有空值。如果是组合索引，则列值的组合必须唯一.

主键索引
是一种特殊的唯一索引，一个表只能有一个主键，不允许有空值。

组合索引
指多个字段上创建的索引，只有在查询条件中使用了创建一个字段，索引才会被使用。使用组合索引时遵循最左前缀。

全文索引
主要用来查找文本中的关键字，而不是直接与索引中的值相比较.fulltext索引跟其它索引大不相同，它更像是一个搜索引擎，而不是简单的，其中语句的参数匹配.fulltext索引配合匹配操作使用，而不是一般的where语句加像。它可以在create table，alter table，create index使用，不过目前只有char，varchar，text列上可以创建全文索引。值得一提的是，在数据量较大时候，现将数据放入一个没有全局索引的表中，然后再用CREATE index创建全文索引，要比先为一张表建立全文然后再将数据写入的速度快很多。
索引为什么快？
使用B+tree的数据结构，能够快速筛选出需要的记录，避免全表扫描


为什么B+Tree快？
1 ，B+Tree拥有B-Tree的优点,深度浅,数据块大
2 ，因为只在叶子结点存储数据,从而导致扫全表的能力强,因为叶子结点是顺序的,从而导致排序功能更强。
3 ，查询时间相对稳定,因为原因1:平衡二叉树,解决了查询不会受到结点分布的影响, 原因2:因为数据在叶子结点,导致每次查询的深度是一样的(相对于B-Tree)
索引的优点、缺点？
索引的优点

1.通过创建唯一索引，可以保证数据库每一行数据的唯一性
2.可以大大提高查询速度
3.可以加速表与表的连接
4.可以显著的减少查询中分组和排序的时间。

索引的缺点

1.创建索引和维护索引需要时间，而且数据量越大时间越长
2.创建索引需要占据磁盘的空间，如果有大量的索引，可能比数据文件更快达到最大文件尺寸
3.当对表中的数据进行增加，修改，删除的时候，索引也要同时进行维护，降低了数据的维护速度

如何建索引？
1.不是越多越好，因为如果建立过多的索引，保存的速度就会下降
2.常更新的表越少越好，因为在字段中做更新（插入）操作后，索引也会更新的，这样的话效率会大大降低
3.数据量小的表最好不要建立索引，因为小的表即使建立索引也不会有大的用处,还会增加额外的索引开销
4.不同的值比较多的列才需要建立索引
5.某种数据本身具备唯一性的时候，建立唯一性索引，可以保证定义的列的数据完整性，以提高查询熟度
6.频繁进行排序或分组的列(group by或者是order by)可以建立索引，提高搜索速度
7.经常用于查询条件的字段应该建立索引

不经常引用的列不要建立索引,因为不常用,即使建立了索引也没有多大意义
经常频繁更新的列不要建立索引,因为肯定会影响插入或更新的效率
索引并不是一劳永逸的,用的时间长了需要进行整理或者重建

如何理解索引最左前缀匹配？
在创建一个n列的索引时，需要遵循“最左前缀”原则。

创建表：create table abc(a varchar(32) not null, b varchar(32), c date, d varchar(32) );

创建普通索引：create index in_abc_acb on abc(a, c, b);

1、必须用到索引的第一个字段

select * from abc where  d='d' and b='b';  不会用到索引，必须要用到左边第一个字段；
1
2、对于索引的第一个字段，用like时左边必须是固定值，通配符只能出现在右边

select * from AAA where a like ‘1%’;会用到索引;而select * from abc where a like ‘%1’;不会用到索引。
1
3、遇到范围（>、<、between、like）查询就停止匹配，

select * from abc where a like '1%' and c=sysdate;  a 会用到索引，c 则不会
1
4、字段前加了函数（表达式）索引会被抑制，

select * from abc where trim(a) = 'a' ; a不会用到索引 
select * from abc where a = 'a' and c + 1 > sysdate;  a会用到索引 c不会用到索引
select * from abc where a = 'a' and c > sysdate - 2;  a会用到索引 c会用到索引
1
2
3
5、索引是从左到右匹配，in 和 = 可以乱序

select * from abc where b like 'b%' and c = sysdate and a='a' ；acb的索引都可以用到
1

Mysql索引失效有哪几种情况？
1.索引无法存储null值
　　
　　a.单列索引无法储null值，复合索引无法储全为null的值。

       b.查询时，采用is null条件时，不能利用到索引，只能全表扫描。

为什么索引列无法存储Null值？

a.索引是有序的。NULL值进入索引时，无法确定其应该放在哪里。（将索引列值进行建树，其中必然涉及到诸多的比较操作，null 值是不确定值无法比较，无法确定null出现在索引树的叶子节点位置。）

b.如果需要把空值存入索引，方法有二：其一，把NULL值转为一个特定的值，在WHERE中检索时，用该特定值查找。其二，建立一个复合索引。例如

create index ind_a on table(col1,1); 通过在复合索引中指定一个非空常量值，而使构成索引的列的组合中，不可能出现全空值。

2.不适合键值较少的列（不适合重复数据较多的列）
　　假如索引列TYPE有5个键值，如果有1万条数据，那么 WHERE TYPE = 1将访问表中的2000个数据块。

再加上访问索引块，一共要访问大于200个的数据块。

如果全表扫描，假设10条数据一个数据块，那么只需访问1000个数据块，既然全表扫描访问的数据块

少一些，肯定就不会利用索引了。

3.前导模糊查询不能利用索引(like '%XX’或者like ‘%XX%’)
　　假如有这样一列code的值为’AAA’,‘AAB’,‘BAA’,‘BAB’ ,如果where code like '%AB’条件，由于前面是

模糊的，所以不能利用索引的顺序，必须一个个去找，看是否满足条件。这样会导致全索引扫描或者全表扫

描。如果是这样的条件where code like 'A % '，就可以查找CODE中A开头的CODE的位置，当碰到B开头的

数据时，就可以停止查找了，因为后面的数据一定不满足要求。这样就可以利用索引了。

4.其他几种索引失效的情况
　　
1.如果条件中有or，即使其中有条件带索引也不会使用(这也是为什么尽量少用or的原因)

要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引

2.对于多列索引，不是使用的第一部分，则不会使用索引

3.like查询以%开头

4.如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引

5.如果mysql估计使用全表扫描要比使用索引快,则不使用索引

5.MySQL主要提供2种方式的索引：B-Tree索引，Hash索引
　　B树索引具有范围查找和前缀查找的能力，对于有N节点的B树，检索一条记录的复杂度为O(LogN)。相当于二分查找。

哈希索引只能做等于查找，但是无论多大的Hash表，查找复杂度都是O(1)。

显然，如果值的差异性大，并且以等值查找（=、 <、>、in）为主，Hash索引是更高效的选择，它有O(1)的查找复杂度。

如果值的差异性相对较差，并且以范围查找为主，B树是更好的选择，它支持范围查找。


Mysql索引如何优化？
作为免费又高效的数据库，mysql基本是首选。良好的安全连接，自带查询解析、sql语句优化，使用读写锁（细化到行）、事物隔离和多版本并发控制提高并发，完备的事务日志记录，强大的存储引擎提供高效查询（表记录可达百万级），如果是InnoDB，还可在崩溃后进行完整的恢复，优点非常多。即使有这么多优点，仍依赖人去做点优化，看书后写个总结巩固下，有错请指正。

完整的mysql优化需要很深的功底，大公司甚至有专门写mysql内核的，sql优化攻城狮，mysql服务器的优化，各种参数常量设定，查询语句优化，主从复制，软硬件升级，容灾备份，sql编程，需要的不是一星半点的知识与时间来掌握，作为一名像俺这样的菜鸟开发，强吃这么多消化不了也没意义：没地儿用啊，况且还有运维和dba，还不如把手头的业务写好，也就是写好点的sql，而且很多sql语句优化跟索引还是有很大关系的。

首先，mysql的查询流程大致是：mysql客户端通过协议与mysql服务器建立连接，发送查询语句，先检查查询缓存，如果命中，直接返回结果，否则进行语句解析，有一系列预处理，比如检查语句是否写正确了，然后是查询优化（比如是否使用索引扫描，如果是一个不可能的条件，则提前终止），生成查询计划，然后查询引擎启动，开始执行查询，从底层存储引擎调用API获取数据，最后返回给客户端。怎么存数据、怎么取数据，都与存储引擎有关。然后，mysql默认使用的BTREE索引，并且一个大方向是，无论怎么折腾sql，至少在目前来说，mysql最多只用到表中的一个索引。

mysql通过存储引擎取数据，自然跟存储引擎有很大关系，不同的存储引擎索引也不一样，如MyISAM的全文索引，即便索引叫一个名字内部组织方式也不尽相同，最常用的当然就是InnoDB了（还有完全兼容mysql的MariaDB，它的默引擎是XtraDB，跟InnoDB很像），这里写的是InnoDB引擎。而索引的实现也跟存储引擎，按照实现方式分，InnoDB的索引目前只有两种：BTREE索引和HASH索引。通常我们说的索引不出意外指的就是B树索引，InnoDB的BTREE索引，实际是用B+树实现的，因为在查看表索引时，mysql一律打印BTREE，所以简称为B树索引。至于B树与B+树的区别，原谅的俺数据结构没好好学，也是需要补的地方。

使用了BTREE索引，意味着所有的索引是按顺序排列存储的（升序），mysql就是这么干的，mysl中的BTREE索引抽象结构如下图（参考高性能mysql）。

结构中，每一层节点均从左往右从小到大排列，key1 < key2 < … < keyN，对于小于key1或在[key1，key2)或其他的值的节点，在进入叶子节点查找，是一个范围分布，同时，同一层节点之间可直接访问，因为他们之间有指针指向联系（MyISAM的BTREE索引没有）。每次搜索是一个区间搜索，有的话就找到了，没有的话就是空。索引能加快访问速度，因为有了它无需全表扫描数据（不总是这样），根据查找的值，跟节点中的值比较，通常使用二分查找，对于排好序的数值来说，平均速度几乎是最快的。

val指向了哪里，对于InnoDB，它指向的就是表数据，因为InnoDB的表数据本身就是索引文件，这是与MyISAM索引的显著区别，MyISAM的索引指向的是表数据的地址（val指向的是类似于0x7DFF…之类）。比如对于InnoDB一个主键索引来说，可能是这样

InnoDB的索引节点val值直接指向表数据，即它的叶子节点就是表数据，它们连在一起，表记录行没有再单独放在其他地方，叶子节点（数据）之间可访问。

前面在BTREE的抽象结构中，索引值的节点是放在页中的，这里有两个需注意的问题：

1. 叶子页、页中的值（上上图），即所谓的页是啥，俺加了个节点注释，即这里的页最小可近似当做是单个节点。我们知道计算机的存储空间是一块一块的，通常一块用完了再用另一块，如果上一块只剩余5kb，但这里刚好要申请8kb的空间，就得在一个新的块上申请这个空间，然后以后的申请又接在这个8kb后面，只要这个块的空间足够，那么上一块的5kb通常就成了所谓的“碎片”，电脑用多了会有很多这样零散的碎片空间，因此有碎片整理。在mysql中，这里的页可理解为块存储空间，即索引的树节点是存放在页中的，每一页（称为逻辑页）有固定大小，InnoDB目前是16kb，一页用完了，当继续插入表生成新的索引节点时，就去新的页中存储这个节点，再有新的节点就继续放在这个新的页的节点后面。

2. 页分裂问题，一页总要被存满，然后新开一页继续，这种行为被称作页分裂。何时开辟新的页，mysql规定了一个分裂因子，达到页存储空间的15/16则存到下一页。页分裂的存在可能极大影响性能维护索引的性能。通常提倡的是，设定一个无意义的整数自增索引，有利于索引存储

如果非自增或不是整数索引，如非自增整数、类似MD5的字符串，以他们作为索引值时，因为待插入的下一条数据的值不一定比上一条大，甚至比当前页所有值都小，需要跑到前几页去比较而找到合适位置，InnoDB无法简单的把新行插入到上一行后面，而找到并插入索引后，可能导致该页达到分裂因子阀值，需要页分裂，进一步导致后面所有的索引页的分裂和排序，数据量小也许没什么问题，数据量大的话可能会浪费大量时间，产生许多碎片。

主键总是唯一且非空，InnoDB自动对它建立了索引（primary key），对于非主键字段上建立的索引，又称辅助索引，索引排列也是顺序排列，只是它还附带一个本条记录的主键值的数据域，不是指向本数据行的指针，在使用辅助索引查找时，先找到对应这一列的索引值，再根据索引节点上的另一个数据域—主键值，来查找该行记录，即每次查找实际经过查找了两次。额外的数据域存储主键值的好处是，当页分裂发生时，无需修改数据域的值，因为即使页分裂，该行的主键值是不变的，而地址就变了。比如name字段的索引简示如下

包含一列的索引称为单列索引，多列的称为复合索引，因为BTREE索引是顺序排列的，所以比较适合范围查询，但是在复合索引中，还应注意列数目、列的顺序以及前面范围查询的列对后边列的影响。

比如有这样一张表

create table staffs(
    id int primary key auto_increment,
    name varchar(24) not null default '' comment '姓名',
    age int not null default 0 comment '年龄',
    pos varchar(20) not null default '' comment '职位',
    add_time timestamp not null default current_timestamp comment '入职时间'
  ) charset utf8 comment '员工记录表';
1
2
3
4
5
6
7
添加三列的复合索引

alter table staffs add index idx_nap(name, age, pos);
1
在BTREE索引的使用上，以下几种情况可以用到该索引或索引的一部分（使用explain简单查看使用情况）：

1. 全值匹配

如select * from staffs where name = 'July' and age = '23' and pos = 'dev' ，key字段显示使用了idx_nap索引

2. 匹配最左列，对于复合索引来说，不总是匹配所有字段列，但是可以匹配索引中靠左的列

如select * from staffs where name = 'July' and age = '23'，key字段显示用到了索引，注意，key_len字段（表示本次语句使用的索引长度）数值比上一条小了，意思是它并未使用全部索引列（通常这个长度可估摸着用了哪些索引列，埋个坑），事实上只用到了name和age列

再试试select * from staffs where name = 'July'，它也用了索引，key_len值更小，实际只用到了索引中的name列

3. 匹配列前缀，即一个索引中列的前一部分，主要用在模糊匹配，如select * fromstaffs where name like 'J%'，explain信息的key字段表示使用了索引，但是mysql的B树索引不能非列前缀的模糊匹配，如select * from staffs where name like '%y' 或者 like ‘%u%’，`据说是由于底层存储引擎的API限制

4. 匹配范围，如select * from staffs where name > 'Mary'，但俺在测试时发现>可以，>=却不行，至少在字符串列上不行（测试mysql版本5.5.12），然而在时间类型（timestamp）上却可以，不测试下还真不能确定说就用到了索引==

出于好奇测了下整型字段的索引（idx_cn(count, name)，count为整型），发现整型受限制少很多，下面的都能用到索引，连前模糊匹配的都行

select * from indexTest1 where count > '10'
  select * from indexTest1 where count >= '10'
  select * from indexTest1 where count > '10%'
  select * from indexTest1 where count >= '10%'
  select * from indexTest1 where count > '%10%'
  select * from indexTest1 where count >= '%10%'
1
2
3
4
5
6
5. 精确匹配一列并范围匹配右侧相邻列，即前一列是固定值，后一列是范围值，它用了name与age两个列的索引（key_len推测）

如select * from staffs where name = 'July' and age > 25

6. 只访问索引的查询，比如staffs表的情况，索引建立在(name,age,pos)上面，前面一直是读取的全部列，如果我们用到了哪些列的索引，查询时也只查这些列的数据，就是只访问索引的查询，如

 select name,age,pos from staffs where name = 'July' and age = 25 and pos = 'dev'
 select name,age from staffs where name = July and age > 25
1
2
第一句用到了全部索引列，第二句只用了索引前两列，select的字段就最多只能是这两列，这种查询情况的索引，mysql称为覆盖索引，就是索引包含（覆盖）了查询的全部字段。是不是用到了索引查询，在explain中需要看最后一个Extra列的信息，Using index表明使用了覆盖索引，同时Using where表明也使用了where过滤

7. 前缀索引

区别于列前缀（类似like 'J%'形式的模糊匹配）和最左列索引（顺序取索引中靠左的列的查询），它只取某列的一部分作为索引。通常在说InnoDB跟MyISAM的区别时，一个明显的区别是：MyISAM支持全文索引，而InnoDB不行，甚至对于text、blob这种超长的字符串或二进制数据时，MyISAM会取前多少个字符作为索引，InnoDb的前缀索引跟这个类似，某些列，一般是字符串类型，很长，全部作为索引大大增加存储空间，索引也需要维护，对于长字符串，又想作为索引列，一个可取的办法就是取前一部分（前缀），代表一整列作为索引串，问题是：如何确保这个前缀能代表或大致代表这一列？所以mysql中有个概念是索引的选择性，是指索引中不重复的值的数目（也称基数）与整个表该列记录总数（#T）的比值，比如一个列表（1,2,2,3），总数是4，不重复值数目为3，选择性为3/4，因此选择性范围是[1/#T, 1]，这个值越大，表示列中不重复值越多，越适合作为前缀索引，唯一索引（UNIQUE KEY）的选择性是1。

比如有一列a varchar(255)，以它作前缀索引，比如以7个测试，逐个增加看看选择性值增长到那个数基本不变，就表示可以代表整列了，再结合这个长度的索引列是否存储数据太多，做个权衡，基本就行了。但如果这个选择性本来就小的可怜还是算了

 select count(distinct left(a, 7))/count(*) as non_repeat from tab;
1
定好一个前缀数目，如9，添加索引时可以这样

  alter table tab add index idx_cpn(count, name(9)) --复合前缀索引
1
以上为常见的使用索引的方式，有这么些情况不能用或不能全用，有的就是上面情况的反例，以key(a, b, c)为例

1. 跳过列，where a = 1 and c = 3，最多用到索引列a；where b = 2 and c = 3，一个也用不到，必须从最左列开始

2. 前面是范围查询，where a = 1 and b > 2 and c = 3，最多用到 a, b两个索引列；

3. 顺序颠倒，where c = 3 and b = 2 and a = 1，一个也用不到；

4. 索引列上使用了表达式，如where substr(a, 1, 3) = ‘hhh’，where a = a + 1，表达式是一大忌讳，再简单mysql也不认。有时数据量不是大到严重影响速度时，一般可以先查出来，比如先查所有有订单记录的数据，再在程序中去筛选以’cp1001’开头的订单，而不是写sql过滤它；

5. 模糊匹配时，尽量写 where a like ‘J%’，字符串放在左边，这样才可能用得到a列索引，甚至可能还用不到，当然这得看数据类型，最好测试一下。

排序对索引的影响

order by是经常用的语句，排序也遵循最左前缀列的原则，比如key(a, b)，下面语句可以用到（测试为妙）

  select * from tab where a > 1 order by b
  select * from tab where a > 1 and b > '2015-12-01 00:00：00' order by b
  select * from tab order by a, b
1
2
3
以下情况用不到

1. 非最左列，select * from tab order by b;

2. 不按索引列顺序来的，select * from tab where b > '2015-12-01 00:00:00' order by a;

3. 多列排序，但列的顺序方向不一致，select * from tab a asc, b desc。

聚簇索引与覆盖索引

前面说到，mysql索引从结构上只有两类，BTREE与HASH，覆盖索引只是在查询时，要查询的列刚好与使用的索引列完全一致，mysql直接扫描索引，然后就可返回数据，大大提高效率，因为不需再去原表查询、过滤，这种形式下的索引称作覆盖索引，比如key(a,b)，查询时select a,b from tab where a = 1 and b > 2，本质原因：BTREE索引存储了原表数据。

聚簇索引也不是单独的索引，前面简要写到，BTREE索引会把数据放在索引中，即索引的叶子页中，包括主键，主键是跟表数据紧挨着放在一起的，因为表数据只有一份，一列键值要跟每一行数据都紧挨在一起，所以一张表只有一个聚簇索引，对于mysql来说，就是主键列，它是默认的。

聚簇索引将表数据组织到了一起（参考前面主键索引简略图），插入时严重依赖主键顺序，最好是连续自增，否则面临频繁页分裂问题，移动许多数据。

哈希索引

简要说下，类似于数据结构中简单实现的HASH表（散列表）一样，当我们在mysql中用哈希索引时，也是对索引列计算一个散列值（类似md5、sha1、crc32），然后对这个散列值以顺序（默认升序）排列，同时记录该散列值对应数据表中某行的指针，当然这只是简略模拟图

比如对姓名列建立hash索引，生成hash值按顺序排列，但是顺序排列的hash值并不对应表中记录，从地址指针可反应出来，而且，hash索引可能建立在两列或者更多列上，取得是多列数据后的hash值，它不存储表中数据。它先计算列数据的hash值，与索引中的hash值比较，找到了然后比对列数据是否相等，可能涉及其他列条件，然后返回数据。hash当然会有冲突，即碰撞，除非有很多冲突，一般hash索引效率很高，否则hash维护成本较高，因此哈希索引通常用在选择性较高的列上面。哈希索引的结构决定了它的特点：

1. hash索引只是hash值顺序排列，跟表数据没有关系，无法应用于order by；

2. hash索引是对它的所有列计算哈希值，因此在查询时，必须带上所有列，比如有(a, b)哈希索引，查询时必须 where a = 1 and b = 2，少任何一个不行；

3. hash索引只能用于比较查询 = 或 IN，其他范围查询无效，本质还是因不存储表数据；

4. 一旦出现碰撞，hash索引必须遍历所有的hash值，将地址所指向数据一一比较，直到找到所有符合条件的行。

填坑

前面提到通过explain的key_len字段，可大致估计出用了哪些列，索引列的长度跟索引列的数据类型直接相关，一般，我们说int是4字节，bigint8字节，char是1字节，考虑到建表时要指定字符集，比如utf8，还跟选的字符集有关（==!），在utf8下边，一个char是3字节，但是知道这些仍不能说key_len就是将用到的索引列的数据类型代表字节数一加不就完啦？事实总有点区别，测试方法比较机械（以下基于mysql 5.5.2）

建表，加索引，int型

–测试表

 create table keyLenTest1(
    id int primary key auto_increment,
    typeKey int default 0 ,
    add_time timestamp not null default current_timestamp
  ) charset utf8
1
2
3
4
5
–添加索引

  alter table keyLenTest1 add index idx_k(typeKey);
1
可知int型索引默认长度为5，在4字节基础上+1

char型

–改为char型，1个字符

 alter table keyLenTest1 modify typeKey char(1);
1
–改为char型，2个字符

  alter table keyLenTest1 modify typeKey char(2);
1
可知，char型初始是4字节（3+1 bytes），后续按照3字节递增

varchar型

–改为varchar型，1个字符

  alter table keyLenTest1 modify typeKey varchar(1);
1
–改为varchar型，2个字符

  alter table keyLenTest1 modify typeKey varchar(2);
1
可知，varchar型，1个字符时，key_len为6，以后以3字节递增

所以，如果一个语句用到了int、char、varchar，key_len如何计算以及用了哪些索引列应该很清楚了。

如果想了解的更详细点，explain各字段意义，索引的更多细节，除了explain，还有show profiles、慢查询日志等（没细看），推荐看高性能mysql
————————————————
版权声明：本文为CSDN博主「longzhutengyue」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/longzhutengyue/article/details/95357467
框架
Spring
SpringIOC
谈谈对Spring IOC的理解
　　学习过Spring框架的人一定都会听过Spring的IoC(控制反转) 、DI(依赖注入)这两个概念，对于初学Spring的人来说，总觉得IoC 、DI这两个概念是模糊不清的，是很难理解的，今天和大家分享网上的一些技术大牛们对Spring框架的IOC的理解以及谈谈我对Spring Ioc的理解。

一、分享Iteye的开涛对Ioc的精彩讲解
　　首先要分享的是Iteye的开涛这位技术牛人对Spring框架的IOC的理解，写得非常通俗易懂，以下内容全部来自原文，原文地址：http://jinnianshilongnian.iteye.com/blog/1413846

1.1、IoC是什么
　　Ioc—Inversion of Control，即“控制反转”，不是什么技术，而是一种设计思想。在Java开发中，Ioc意味着将你设计好的对象交给容器控制，而不是传统的在你的对象内部直接控制。如何理解好Ioc呢？理解好Ioc的关键是要明确“谁控制谁，控制什么，为何是反转（有反转就应该有正转了），哪些方面反转了”，那我们来深入分析一下：

●谁控制谁，控制什么：传统Java SE程序设计，我们直接在对象内部通过new进行创建对象，是程序主动去创建依赖对象；而IoC是有专门一个容器来创建这些对象，即由Ioc容器来控制对 象的创建；谁控制谁？当然是IoC 容器控制了对象；控制什么？那就是主要控制了外部资源获取（不只是对象包括比如文件等）。

●为何是反转，哪些方面反转了：有反转就有正转，传统应用程序是由我们自己在对象中主动控制去直接获取依赖对象，也就是正转；而反转则是由容器来帮忙创建及注入依赖对象；为何是反转？因为由容器帮我们查找及注入依赖对象，对象只是被动的接受依赖对象，所以是反转；哪些方面反转了？依赖对象的获取被反转了。

用图例说明一下，传统程序设计如图2-1，都是主动去创建相关对象然后再组合起来：



图1-1 传统应用程序示意图

当有了IoC/DI的容器后，在客户端类中不再主动去创建这些对象了，如图2-2所示:




图1-2有IoC/DI容器后程序结构示意图

1.2、IoC能做什么
　　IoC 不是一种技术，只是一种思想，一个重要的面向对象编程的法则，它能指导我们如何设计出松耦合、更优良的程序。传统应用程序都是由我们在类内部主动创建依赖对象，从而导致类与类之间高耦合，难于测试；有了IoC容器后，把创建和查找依赖对象的控制权交给了容器，由容器进行注入组合对象，所以对象与对象之间是 松散耦合，这样也方便测试，利于功能复用，更重要的是使得程序的整个体系结构变得非常灵活。

其实IoC对编程带来的最大改变不是从代码上，而是从思想上，发生了“主从换位”的变化。应用程序原本是老大，要获取什么资源都是主动出击，但是在IoC/DI思想中，应用程序就变成被动的了，被动的等待IoC容器来创建并注入它所需要的资源了。

IoC很好的体现了面向对象设计法则之一—— 好莱坞法则：“别找我们，我们找你”；即由IoC容器帮对象找相应的依赖对象并注入，而不是由对象主动去找。

1.3、IoC和DI
　　DI—Dependency Injection，即“依赖注入”：组件之间依赖关系由容器在运行期决定，形象的说，即由容器动态的将某个依赖关系注入到组件之中。依赖注入的目的并非为软件系统带来更多功能，而是为了提升组件重用的频率，并为系统搭建一个灵活、可扩展的平台。通过依赖注入机制，我们只需要通过简单的配置，而无需任何代码就可指定目标需要的资源，完成自身的业务逻辑，而不需要关心具体的资源来自何处，由谁实现。

理解DI的关键是：“谁依赖谁，为什么需要依赖，谁注入谁，注入了什么”，那我们来深入分析一下：

●谁依赖于谁：当然是应用程序依赖于IoC容器；

●为什么需要依赖：应用程序需要IoC容器来提供对象需要的外部资源；

●谁注入谁：很明显是IoC容器注入应用程序某个对象，应用程序依赖的对象；

●注入了什么：就是注入某个对象所需要的外部资源（包括对象、资源、常量数据）。

IoC和DI由什么关系呢？其实它们是同一个概念的不同角度描述，由于控制反转概念比较含糊（可能只是理解为容器控制对象这一个层面，很难让人想到谁来维护对象关系），所以2004年大师级人物Martin Fowler又给出了一个新的名字：“依赖注入”，相对IoC 而言，“依赖注入”明确描述了“被注入对象依赖IoC容器配置依赖对象”。

看过很多对Spring的Ioc理解的文章，好多人对Ioc和DI的解释都晦涩难懂，反正就是一种说不清，道不明的感觉，读完之后依然是一头雾水，感觉就是开涛这位技术牛人写得特别通俗易懂，他清楚地解释了IoC(控制反转) 和DI(依赖注入)中的每一个字，读完之后给人一种豁然开朗的感觉。我相信对于初学Spring框架的人对Ioc的理解应该是有很大帮助的。

二、分享Bromon的blog上对IoC与DI浅显易懂的讲解
2.1、IoC(控制反转)
　　首先想说说IoC（Inversion of Control，控制反转）。这是spring的核心，贯穿始终。所谓IoC，对于spring框架来说，就是由spring来负责控制对象的生命周期和对象间的关系。这是什么意思呢，举个简单的例子，我们是如何找女朋友的？常见的情况是，我们到处去看哪里有长得漂亮身材又好的mm，然后打听她们的兴趣爱好、qq号、电话号、ip号、iq号………，想办法认识她们，投其所好送其所要，然后嘿嘿……这个过程是复杂深奥的，我们必须自己设计和面对每个环节。传统的程序开发也是如此，在一个对象中，如果要使用另外的对象，就必须得到它（自己new一个，或者从JNDI中查询一个），使用完之后还要将对象销毁（比如Connection等），对象始终会和其他的接口或类藕合起来。

那么IoC是如何做的呢？有点像通过婚介找女朋友，在我和女朋友之间引入了一个第三者：婚姻介绍所。婚介管理了很多男男女女的资料，我可以向婚介提出一个列表，告诉它我想找个什么样的女朋友，比如长得像李嘉欣，身材像林熙雷，唱歌像周杰伦，速度像卡洛斯，技术像齐达内之类的，然后婚介就会按照我们的要求，提供一个mm，我们只需要去和她谈恋爱、结婚就行了。简单明了，如果婚介给我们的人选不符合要求，我们就会抛出异常。整个过程不再由我自己控制，而是有婚介这样一个类似容器的机构来控制。Spring所倡导的开发方式就是如此，所有的类都会在spring容器中登记，告诉spring你是个什么东西，你需要什么东西，然后spring会在系统运行到适当的时候，把你要的东西主动给你，同时也把你交给其他需要你的东西。所有的类的创建、销毁都由 spring来控制，也就是说控制对象生存周期的不再是引用它的对象，而是spring。对于某个具体的对象而言，以前是它控制其他对象，现在是所有对象都被spring控制，所以这叫控制反转。

2.2、DI(依赖注入)
　　IoC的一个重点是在系统运行中，动态的向某个对象提供它所需要的其他对象。这一点是通过DI（Dependency Injection，依赖注入）来实现的。比如对象A需要操作数据库，以前我们总是要在A中自己编写代码来获得一个Connection对象，有了 spring我们就只需要告诉spring，A中需要一个Connection，至于这个Connection怎么构造，何时构造，A不需要知道。在系统运行时，spring会在适当的时候制造一个Connection，然后像打针一样，注射到A当中，这样就完成了对各个对象之间关系的控制。A需要依赖 Connection才能正常运行，而这个Connection是由spring注入到A中的，依赖注入的名字就这么来的。那么DI是如何实现的呢？ Java 1.3之后一个重要特征是反射（reflection），它允许程序在运行的时候动态的生成对象、执行对象的方法、改变对象的属性，spring就是通过反射来实现注入的。

理解了IoC和DI的概念后，一切都将变得简单明了，剩下的工作只是在spring的框架中堆积木而已。

三、我对IoC(控制反转)和DI(依赖注入)的理解
　　在平时的java应用开发中，我们要实现某一个功能或者说是完成某个业务逻辑时至少需要两个或以上的对象来协作完成，在没有使用Spring的时候，每个对象在需要使用他的合作对象时，自己均要使用像new object() 这样的语法来将合作对象创建出来，这个合作对象是由自己主动创建出来的，创建合作对象的主动权在自己手上，自己需要哪个合作对象，就主动去创建，创建合作对象的主动权和创建时机是由自己把控的，而这样就会使得对象间的耦合度高了，A对象需要使用合作对象B来共同完成一件事，A要使用B，那么A就对B产生了依赖，也就是A和B之间存在一种耦合关系，并且是紧密耦合在一起，而使用了Spring之后就不一样了，创建合作对象B的工作是由Spring来做的，Spring创建好B对象，然后存储到一个容器里面，当A对象需要使用B对象时，Spring就从存放对象的那个容器里面取出A要使用的那个B对象，然后交给A对象使用，至于Spring是如何创建那个对象，以及什么时候创建好对象的，A对象不需要关心这些细节问题(你是什么时候生的，怎么生出来的我可不关心，能帮我干活就行)，A得到Spring给我们的对象之后，两个人一起协作完成要完成的工作即可。

所以控制反转IoC(Inversion of Control)是说创建对象的控制权进行转移，以前创建对象的主动权和创建时机是由自己把控的，而现在这种权力转移到第三方，比如转移交给了IoC容器，它就是一个专门用来创建对象的工厂，你要什么对象，它就给你什么对象，有了 IoC容器，依赖关系就变了，原先的依赖关系就没了，它们都依赖IoC容器了，通过IoC容器来建立它们之间的关系。

这是我对Spring的IoC(控制反转)的理解。DI(依赖注入)其实就是IOC的另外一种说法，DI是由Martin Fowler 在2004年初的一篇论文中首次提出的。他总结：控制的什么被反转了？就是：获得依赖对象的方式反转了。

四、小结
　　对于Spring Ioc这个核心概念，我相信每一个学习Spring的人都会有自己的理解。这种概念上的理解没有绝对的标准答案，仁者见仁智者见智。如果有理解不到位或者理解错的地方，欢迎广大园友指正！

Spring容器高层视图

Spring 启动时读取应用程序提供的Bean配置信息，并在Spring容器中生成一份相应的Bean配置注册表，然后根据这张注册表实例化Bean，装配好Bean之间的依赖关系，为上层应用提供准备就绪的运行环境。



Bean缓存池：HashMap实现

IOC容器介绍

Spring 通过一个配置文件描述 Bean 及 Bean 之间的依赖关系，利用 Java 语言的反射功能实例化 Bean 并建立 Bean 之间的依赖关系。 Spring 的 IoC 容器在完成这些底层工作的基础上，还提供了 Bean 实例缓存、生命周期管理、 Bean 实例代理、事件发布、资源装载等高级服务。

BeanFactory 是 Spring 框架的基础设施，面向 Spring 本身；

ApplicationContext 面向使用 Spring 框架的开发者，几乎所有的应用场合我们都直接使用 ApplicationContext 而非底层的 BeanFactory。

BeanFactory

BeanFactory体系架构：



BeanDefinitionRegistry： Spring 配置文件中每一个节点元素在 Spring 容器里都通过一个 BeanDefinition 对象表示，它描述了 Bean 的配置信息。而 BeanDefinitionRegistry 接口提供了向容器手工注册 BeanDefinition 对象的方法。

BeanFactory 接口位于类结构树的顶端 ，它最主要的方法就是 getBean(String beanName)，该方法从容器中返回特定名称的 Bean，BeanFactory 的功能通过其他的接口得到不断扩展：

ListableBeanFactory：该接口定义了访问容器中 Bean 基本信息的若干方法，如查看Bean 的个数、获取某一类型 Bean 的配置名、查看容器中是否包括某一 Bean 等方法；

HierarchicalBeanFactory：父子级联 IoC 容器的接口，子容器可以通过接口方法访问父容器； 通过 HierarchicalBeanFactory 接口， Spring 的 IoC 容器可以建立父子层级关联的容器体系，子容器可以访问父容器中的 Bean，但父容器不能访问子容器的 Bean。Spring 使用父子容器实现了很多功能，比如在 Spring MVC 中，展现层 Bean 位于一个子容器中，而业务层和持久层的 Bean 位于父容器中。这样，展现层 Bean 就可以引用业务层和持久层的 Bean，而业务层和持久层的 Bean 则看不到展现层的 Bean。

ConfigurableBeanFactory：是一个重要的接口，增强了 IoC 容器的可定制性，它定义了设置类装载器、属性编辑器、容器初始化后置处理器等方法；

AutowireCapableBeanFactory：定义了将容器中的 Bean 按某种规则（如按名字匹配、按类型匹配等）进行自动装配的方法；

SingletonBeanRegistry：定义了允许在运行期间向容器注册单实例 Bean 的方法；

例子：

使用 Spring 配置文件为 Car 提供配置信息：beans.xml：



通过 BeanFactory 装载配置文件，启动 Spring IoC 容器：



XmlBeanFactory 通过 Resource 装载 Spring 配置信息并启动 IoC 容器，然后就可以通过 BeanFactory#getBean(beanName)方法从 IoC 容器中获取 Bean 了。通过 BeanFactory 启动IoC 容器时，并不会初始化配置文件中定义的 Bean，初始化动作发生在第一个调用时。

对于单实例（ singleton）的 Bean 来说，BeanFactory会缓存 Bean 实例，所以第二次使用 getBean() 获取 Bean 时将直接从 IoC 容器的缓存中获取 Bean 实例。Spring 在 DefaultSingletonBeanRegistry 类中提供了一个用于缓存单实例 Bean 的缓存器，它是一个用HashMap 实现的缓存器，单实例的 Bean 以 beanName 为键保存在这个HashMap 中。

值得一提的是，在初始化 BeanFactory 时，必须为其提供一种日志框架，比如使用Log4J， 即在类路径下提供 Log4J 配置文件，这样启动 Spring 容器才不会报错。

ApplicationContext

ApplicationContext 由 BeanFactory 派生而来，提供了更多面向实际应用的功能。

在BeanFactory 中，很多功能需要以编程的方式实现，而在 ApplicationContext 中则可以通过配置的方式实现。



ApplicationContext 继承了 HierarchicalBeanFactory 和 ListableBeanFactory 接口，在此基础上，还通过多个其他的接口扩展了 BeanFactory 的功能：

ClassPathXmlApplicationContext：默认从类路径加载配置文件

FileSystemXmlApplicationContext：默认从文件系统中装载配置文件

ApplicationEventPublisher：让容器拥有发布应用上下文事件的功能，包括容器启动事件、关闭事件等。实现了 ApplicationListener 事件监听接口的 Bean 可以接收到容器事件 ， 并对事件进行响应处理 。 在 ApplicationContext 抽象实现类AbstractApplicationContext 中，我们可以发现存在一个 ApplicationEventMulticaster，它负责保存所有监听器，以便在容器产生上下文事件时通知这些事件监听者。

MessageSource：为应用提供 i18n 国际化消息访问的功能；

ResourcePatternResolver ： 所 有 ApplicationContext 实现类都实现了类似于PathMatchingResourcePatternResolver 的功能，可以通过带前缀的 Ant 风格的资源文件路径装载 Spring 的配置文件。

LifeCycle：该接口是 Spring 2.0 加入的，该接口提供了 start()和 stop()两个方法，主要用于控制异步处理过程。在具体使用时，该接口同时被 ApplicationContext 实现及具体 Bean 实现， ApplicationContext 会将 start/stop 的信息传递给容器中所有实现了该接口的 Bean，以达到管理和控制 JMX、任务调度等目的。

ConfigurableApplicationContext 扩展于 ApplicationContext，它新增加了两个主要的方法： refresh()和 close()，让 ApplicationContext 具有启动、刷新和关闭应用上下文的能力。在应用上下文关闭的情况下调用 refresh()即可启动应用上下文，在已经启动的状态下，调用 refresh()则清除缓存并重新装载配置信息，而调用close()则可关闭应用上下文。这些接口方法为容器的控制管理带来了便利，但作为开发者，我们并不需要过多关心这些方法。

使用：

如果配置文件放置在类路径下，用户可以优先使用 ClassPathXmlApplicationContext 实现类：

如果配置文件放置在文件系统的路径下，则可以优先考虑使用 FileSystemXmlApplicationContext 实现类：

Spring 3.0 支持基于类注解的配置方式，主要功能来自于 Spring 的一个名为 JavaConfig 子项目，目前 JavaConfig已经升级为 Spring核心框架的一部分。

ApplicationContext 在初始化应用上下文时就实例化所有单实例的 Bean。

WebApplicationContext

WebApplication体系架构：



WebApplicationContext 是专门为 Web 应用准备的，它允许从相对于 Web 根目录的路径中装载配置文件完成初始化工作。从WebApplicationContext 中可以获得 ServletContext 的引用，整个 Web 应用上下文对象将作为属性放置到 ServletContext 中，以便 Web 应用环境可以访问 Spring 应用上下文。 WebApplicationContext 定义了一个常量ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE，在上下文启动时， WebApplicationContext 实例即以此为键放置在 ServletContext 的属性列表中，因此我们可以直接通过以下语句从 Web 容器中获取WebApplicationContext：



Spring 和 Web 应用的上下文融合：



WebApplicationContext 的初始化方式：WebApplicationContext 需要 ServletContext 实例，它必须在拥有 Web 容器的前提下才能完成启动的工作。可以在 web.xml 中配置自启动的 Servlet 或定义 Web 容器监听器（ ServletContextListener），借助这两者中的任何一个就可以完成启动 Spring Web 应用上下文的工作。Spring 分别提供了用于启动 WebApplicationContext 的 Servlet 和 Web 容器监听器：

org.springframework.web.context.ContextLoaderServlet；

org.springframework.web.context.ContextLoaderListener

由于 WebApplicationContext 需要使用日志功能，比如日志框架使用Log4J，用户可以将 Log4J 的配置文件放置到类路径 WEB-INF/classes 下，这时 Log4J 引擎即可顺利启动。如果 Log4J 配置文件放置在其他位置，用户还必须在 web.xml 指定 Log4J 配置文件位置。

Bean的生命周期

1．当调用者通过 getBean(beanName)向容器请求某一个 Bean 时，如果容器注册了org.springframework.beans.factory.config.InstantiationAwareBeanPostProcessor 接口，在实例化 Bean 之前，将调用接口的 postProcessBeforeInstantiation()方法；

2．根据配置情况调用 Bean 构造函数或工厂方法实例化 Bean；

3．如果容器注册了 InstantiationAwareBeanPostProcessor 接口，在实例化 Bean 之后，调用该接口的 postProcessAfterInstantiation()方法，可在这里对已经实例化的对象进行一些“梳妆打扮”；

4．如果 Bean 配置了属性信息，容器在这一步着手将配置值设置到 Bean 对应的属性中，不过在设置每个属性之前将先调用InstantiationAwareBeanPostProcessor 接口的postProcessPropertyValues()方法；

5．调用 Bean 的属性设置方法设置属性值；

6．如果 Bean 实现了 org.springframework.beans.factory.BeanNameAware 接口，将调用setBeanName()接口方法，将配置文件中该 Bean 对应的名称设置到 Bean 中；

7．如果 Bean 实现了 org.springframework.beans.factory.BeanFactoryAware 接口，将调用 setBeanFactory()接口方法，将 BeanFactory 容器实例设置到 Bean 中；

8．如果 BeanFactory 装配了 org.springframework.beans.factory.config.BeanPostProcessor后处理器，将调用 BeanPostProcessor 的 Object postProcessBeforeInitialization(Object bean, String beanName)接口方法对 Bean 进行加工操作。其中入参 bean 是当前正在处理的 Bean，而 beanName 是当前 Bean 的配置名，返回的对象为加工处理后的 Bean。用户可以使用该方法对某些 Bean 进行特殊的处理，甚至改变 Bean 的行为， BeanPostProcessor 在 Spring 框架中占有重要的地位，为容器提供对 Bean 进行后续加工处理的切入点， Spring 容器所提供的各种“神奇功能”（如 AOP，动态代理等）都通过 BeanPostProcessor 实施；

9．如果 Bean 实现了 InitializingBean 的接口，将调用接口的 afterPropertiesSet()方法；

10．如果在通过 init-method 属性定义了初始化方法，将执行这个方法；

11．BeanPostProcessor 后处理器定义了两个方法：其一是 postProcessBeforeInitialization() 在第 8 步调用；其二是 Object postProcessAfterInitialization(Object bean, String beanName)方法，这个方法在此时调用，容器再次获得对 Bean 进行加工处理的机会；

12．如果在中指定 Bean 的作用范围为 scope=“prototype”，将 Bean 返回给调用者，调用者负责 Bean 后续生命的管理， Spring 不再管理这个 Bean 的生命周期。如果作用范围设置为 scope=“singleton”，则将 Bean 放入到 Spring IoC 容器的缓存池中，并将 Bean引用返回给调用者， Spring 继续对这些 Bean 进行后续的生命管理；

13．对于 scope=“singleton”的 Bean，当容器关闭时，将触发 Spring 对 Bean 的后续生命周期的管理工作，首先如果 Bean 实现了 DisposableBean 接口，则将调用接口的afterPropertiesSet()方法，可以在此编写释放资源、记录日志等操作；

14．对于 scope=“singleton”的 Bean，如果通过的 destroy-method 属性指定了 Bean 的销毁方法， Spring 将执行 Bean 的这个方法，完成 Bean 资源的释放等操作。

可以将这些方法大致划分为三类：

Bean 自身的方法：如调用 Bean 构造函数实例化 Bean，调用 Setter 设置 Bean 的属性值以及通过的 init-method 和 destroy-method 所指定的方法；

Bean 级生命周期接口方法：如 BeanNameAware、 BeanFactoryAware、 InitializingBean 和 DisposableBean，这些接口方法由 Bean 类直接实现；

容器级生命周期接口方法：在上图中带“★” 的步骤是由 InstantiationAwareBean PostProcessor 和BeanPostProcessor 这两个接口实现，一般称它们的实现类为“ 后处理器” 。 后处理器接口一般不由 Bean 本身实现，它们独立于 Bean，实现类以容器附加装置的形式注册到 Spring 容器中并通过接口反射为 Spring 容器预先识别。当Spring 容器创建任何 Bean 的时候，这些后处理器都会发生作用，所以这些后处理器的影响是全局性的。当然，用户可以通过合理地编写后处理器，让其仅对感兴趣Bean 进行加工处理

ApplicationContext 和 BeanFactory 另一个最大的不同之处在于：ApplicationContext会利用 Java 反射机制自动识别出配置文件中定义的 BeanPostProcessor、 InstantiationAwareBeanPostProcessor 和 BeanFactoryPostProcessor，并自动将它们注册到应用上下文中；而后者需要在代码中通过手工调用 addBeanPostProcessor()方法进行注册。这也是为什么在应用开发时，我们普遍使用 ApplicationContext 而很少使用 BeanFactory 的原因之一

IOC容器工作机制（执行流程）

容器启动过程

web环境下Spring容器、SpringMVC容器启动过程：

首先，对于一个web应用，其部署在web容器中，web容器提供其一个全局的上下文环境，这个上下文就是ServletContext，其为后面的spring IoC容器提供宿主环境；

其次，在web.xml中会提供有contextLoaderListener（或ContextLoaderServlet）。在web容器启动时，会触发容器初始化事件，此时contextLoaderListener会监听到这个事件，其contextInitialized方法会被调用，在这个方法中，spring会初始化一个启动上下文，这个上下文被称为根上下文，即WebApplicationContext，这是一个接口类，确切的说，其实际的实现类是XmlWebApplicationContext。这个就是spring的IoC容器，其对应的Bean定义的配置由web.xml中的context-param标签指定。在这个IoC容器初始化完毕后，spring容器以WebApplicationContext.ROOTWEBAPPLICATIONCONTEXTATTRIBUTE为属性Key，将其存储到ServletContext中，便于获取；

再次，contextLoaderListener监听器初始化完毕后，开始初始化web.xml中配置的Servlet，这个servlet可以配置多个，以最常见的DispatcherServlet为例（Spring MVC），这个servlet实际上是一个标准的前端控制器，用以转发、匹配、处理每个servlet请求。DispatcherServlet上下文在初始化的时候会建立自己的IoC上下文容器，用以持有spring mvc相关的bean，这个servlet自己持有的上下文默认实现类也是XmlWebApplicationContext。在建立DispatcherServlet自己的IoC上下文时，会利用WebApplicationContext.ROOTWEBAPPLICATIONCONTEXTATTRIBUTE先从ServletContext中获取之前的根上下文(即WebApplicationContext)作为自己上下文的parent上下文（即第2步中初始化的XmlWebApplicationContext作为自己的父容器）。有了这个parent上下文之后，再初始化自己持有的上下文（这个DispatcherServlet初始化自己上下文的工作在其initStrategies方法中可以看到，大概的工作就是初始化处理器映射、视图解析等）。初始化完毕后，spring以与servlet的名字相关(此处不是简单的以servlet名为Key，而是通过一些转换)的属性为属性Key，也将其存到ServletContext中，以便后续使用。这样每个servlet就持有自己的上下文，即拥有自己独立的bean空间，同时各个servlet共享相同的bean，即根上下文定义的那些bean。

Bean加载过程

Spring的高明之处在于，它使用众多接口描绘出了所有装置的蓝图，构建好Spring的骨架，继而通过继承体系层层推演，不断丰富，最终让Spring成为有血有肉的完整的框架。所以查看Spring框架的源码时，有两条清晰可见的脉络：

1）接口层描述了容器的重要组件及组件间的协作关系；

2）继承体系逐步实现组件的各项功能。

接口层清晰地勾勒出Spring框架的高层功能，框架脉络呼之欲出。有了接口层抽象的描述后，不但Spring自己可以提供具体的实现，任何第三方组织也可以提供不同实现， 可以说Spring完善的接口层使框架的扩展性得到了很好的保证。纵向继承体系的逐步扩展，分步骤地实现框架的功能，这种实现方案保证了框架功能不会堆积在某些类的身上，造成过重的代码逻辑负载，框架的复杂度被完美地分解开了。

Spring组件按其所承担的角色可以划分为两类：

1）物料组件：Resource、BeanDefinition、PropertyEditor以及最终的Bean等，它们是加工流程中被加工、被消费的组件，就像流水线上被加工的物料；

BeanDefinition：Spring通过BeanDefinition将配置文件中的配置信息转换为容器的内部表示，并将这些BeanDefinition注册到BeanDefinitionRegistry中。Spring容器的后续操作直接从BeanDefinitionRegistry中读取配置信息。

2）加工设备组件：ResourceLoader、BeanDefinitionReader、BeanFactoryPostProcessor、InstantiationStrategy以及BeanWrapper等组件像是流水线上不同环节的加工设备，对物料组件进行加工处理。

InstantiationStrategy：负责实例化Bean操作，相当于Java语言中new的功能，并不会参与Bean属性的配置工作。属性填充工作留待BeanWrapper完成

BeanWrapper：继承了PropertyAccessor和PropertyEditorRegistry接口，BeanWrapperImpl内部封装了两类组件：（1）被封装的目标Bean（2）一套用于设置Bean属性的属性编辑器；具有三重身份：（1）Bean包裹器（2）属性访问器 （3）属性编辑器注册表。PropertyAccessor：定义了各种访问Bean属性的方法。PropertyEditorRegistry：属性编辑器的注册表

该图描述了
完整Bean的作业流程（图解）



１、ResourceLoader从存储介质中加载Spring配置信息，并使用Resource表示这个配置文件的资源；

２、BeanDefinitionReader读取Resource所指向的配置文件资源，然后解析配置文件。配置文件中每一个解析成一个BeanDefinition对象，并保存到BeanDefinitionRegistry中；

３、容器扫描BeanDefinitionRegistry中的BeanDefinition，使用Java的反射机制自动识别出Bean工厂后处理后器（实现BeanFactoryPostProcessor接口）的Bean，然后调用这些Bean工厂后处理器对BeanDefinitionRegistry中的BeanDefinition进行加工处理。主要完成以下两项工作：

1）对使用到占位符的元素标签进行解析，得到最终的配置值，这意味对一些半成品式的BeanDefinition对象进行加工处理并得到成品的BeanDefinition对象；

2）对BeanDefinitionRegistry中的BeanDefinition进行扫描，通过Java反射机制找出所有属性编辑器的Bean（实现java.beans.PropertyEditor接口的Bean），并自动将它们注册到Spring容器的属性编辑器注册表中（PropertyEditorRegistry）；

4．Spring容器从BeanDefinitionRegistry中取出加工后的BeanDefinition，并调用InstantiationStrategy着手进行Bean实例化的工作；

5．在实例化Bean时，Spring容器使用BeanWrapper对Bean进行封装，BeanWrapper提供了很多以Java反射机制操作Bean的方法，它将结合该Bean的BeanDefinition以及容器中属性编辑器，完成Bean属性的设置工作；

6．利用容器中注册的Bean后处理器（实现BeanPostProcessor接口的Bean）对已经完成属性设置工作的Bean进行后续加工，直接装配出一个准备就绪的Bean。

总结

Spring IOC容器主要有继承体系底层的BeanFactory、高层的ApplicationContext和WebApplicationContext

Bean有自己的生命周期

容器启动原理：
Spring应用的IOC容器通过tomcat的Servlet或Listener监听启动加载；Spring MVC的容器由DispatchServlet作为入口加载；Spring容器是Spring MVC容器的父容器

容器加载Bean原理：

BeanDefinitionReader读取Resource所指向的配置文件资源，然后解析配置文件。配置文件中每一个解析成一个BeanDefinition对象，并保存到BeanDefinitionRegistry中；

容器扫描BeanDefinitionRegistry中的BeanDefinition；调用InstantiationStrategy进行Bean实例化的工作；使用BeanWrapper完成Bean属性的设置工作；

单例Bean缓存池：Spring 在 DefaultSingletonBeanRegistry 类中提供了一个用于缓存单实例 Bean 的缓存器，它是一个用 HashMap 实现的缓存器，单实例的 Bean 以 beanName 为键保存在这个HashMap 中。
SpringAOP
一、什么是SpringAOP
　　AOP就是在某一个类或方法执行前后打个标记，声明在执行到这里之前要先执行什么，执行完这里之后要接着执行什么。

AOP（Aspect-Oriented Programming，面向切面编程）能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可扩展性和可维护性。

Spring AOP是基于动态代理的，如果要代理的对象实现了某个接口，那么Spring AOP就会使用JDK动态代理去创建代理对象；而对于没有实现接口的对象，就无法使用JDK动态代理，转而使用CGlib动态代理生成一个被代理对象的子类来作为代理。

当然也可以使用AspectJ，Spring AOP中已经集成了AspectJ，AspectJ应该算得上是Java生态系统中最完整的AOP框架了。使用AOP之后我们可以把一些通用功能抽象出来，在需要用到的地方直接使用即可，这样可以大大简化代码量。我们需要增加新功能也方便，提高了系统的扩展性。日志功能、事务管理和权限管理等场景都用到了AOP


二、AOP术语
1）连接点（Joinpoint）

    程序执行的某个特定位置：如类开始初始化前、类初始化后、类某个方法调用前、调用后、方法抛出异常后。一个类或一段程序代码拥有一些具有边界性质的特定点，这些点中的特定点就称为“连接点”。Spring仅支持方法的连接点，即仅能在方法调用前、方法调用后、方法抛出异常时以及方法调用前后这些程序执行点织入增强。连接点由两个信息确定：第一是用方法表示的程序执行点；第二是用相对点表示的方位。
 
    
2）切点（Pointcut）
    每个程序类都拥有多个连接点，如一个拥有两个方法的类，这两个方法都是连接点，即连接点是程序类中客观存在的事物。AOP通过“切点”定位特定的连接点。连接点相当于数据库中的记录，而切点相当于查询条件。切点和连接点不是一对一的关系，一个切点可以匹配多个连接点。在Spring中，切点通过org.springframework.aop.Pointcut接口进行描述，它使用类和方法作为连接点的查询条件，Spring AOP的规则解析引擎负责切点所设定的查询条件，找到对应的连接点。其实确切地说，不能称之为查询连接点，因为连接点是方法执行前、执行后等包括方位信息的具体程序执行点，而切点只定位到某个方法上，所以如果希望定位到具体连接点上，还需要提供方位信息。
 
3）增强（Advice，也叫通知）
    增强是织入到目标类连接点上的一段程序代码，在Spring中，增强除用于描述一段程序代码外，还拥有另一个和连接点相关的信息，这便是执行点的方位。结合执行点方位信息和切点信息，我们就可以找到特定的连接点。
 　　切面类中的方法可以通过加上以下注解作为通知

　　　　@Before   前置通知

　　　　@After　　　后置通知

　　　　@Around　　 环绕通知

　　　　@AfterReturning  正常返回通知

　　　　@AfterThrowing   异常通知　　

    
4）目标对象（Target）
    增强逻辑的织入目标类。如果没有AOP，目标业务类需要自己实现所有逻辑，而在AOP的帮助下，目标业务类只实现那些非横切逻辑的程序逻辑，而性能监视和事务管理等这些横切逻辑则可以使用AOP动态织入到特定的连接点上。
 
    
5）引介（Introduction）
    引介是一种特殊的增强，它为类添加一些属性和方法。这样，即使一个业务类原本没有实现某个接口，通过AOP的引介功能，我们可以动态地为该业务类添加接口的实现逻辑，让业务类成为这个接口的实现类。    
 
    
6）织入（Weaving）
    织入是将增强添加对目标类具体连接点上的过程。AOP像一台织布机，将目标类、增强或引介通过AOP这台织布机天衣无缝地编织到一起。根据不同的实现技术，AOP有三种织入的方式：
    a、编译期织入，这要求使用特殊的Java编译器。
    b、类装载期织入，这要求使用特殊的类装载器。
    c、动态代理织入，在运行期为目标类添加增强生成子类的方式。
    Spring采用动态代理织入，而AspectJ采用编译期织入和类装载期织入。
 
    
7）代理（Proxy）
    一个类被AOP织入增强后，就产出了一个结果类，它是融合了原类和增强逻辑的代理类。根据不同的代理方式，代理类既可能是和原类具有相同接口的类，也可能就是原类的子类，所以我们可以采用调用原类相同的方式调用代理类。
 
    
8）切面（Aspect）
    切面由切点和增强（引介）组成，它既包括了横切逻辑的定义，也包括了连接点的定义，Spring AOP就是负责实施切面的框架，它将切面所定义的横切逻辑织入到切面所指定的连接点中。
三、SpringAOP执行过程
①　XML配置文件添加autoproxy标签

②　Spring寻找带有@Aspect的类，例如上面的CalculatorAspect类

③　扫描带有@Aspect的类中带有 @Before，@After，@AfterReturning，@Around，@AfterThrowing 等注解的方法，得到该注解，并根据不同的注解判断是对应的是哪一种增强方式

④　获取表达式，例如上面的（“execution(public int com.jd.calculator.CalculatorService.mul(…))”）这个表达式就是指定需要被增强的方法

⑤　检查Spring能扫描到的所有类，找到表达式匹配的方法对应的类

⑥　为该类创建动态代理对象

四、Spring对AOP的支持
　　1.使用ProxyFactoryBean和对应的接口实现AOP

　　2.使用XML配置AOP　　　　　　

　　3.使用@AspectJ注解驱动切面　　

4.使用AspectJ注入切面
四、使用@AspectJ注解开发AOP（优先）
　　0.Spring实现的是一个方法级别的AOP框架，动态代理的方式拦截指定方法的通知

　　
1.创建切面

　　　　在Spring中使用@Aspect注解一个类，SpringIOC容器就会将其当作一个切面。

　　　　切面类中的方法可以通过加上以下注解作为通知

　　　　@Before   前置通知

　　　　@After　　　后置通知

　　　　@Around　　 环绕通知

　　　　@AfterReturning  正常返回通知

　　　　@AfterThrowing   异常通知　　
环绕通知：@Around注解，同时要加入一个参数，参数类型是ProceedingJoinPoint 此类型对象有一个方法proceed(),用于放行去执行真正的方法。
　　　　
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
package club.llf.aspect;
import org.aspectj.lang.annotation.*;
@Aspect
public class XXXAspect {
    @Before("execution(* club.llf.service.impl.UserServiceImpl.printUser(..))")
    public void before(){
        System.out.println("前置通知");
    }
    @After("execution(* club.llf.service.impl.UserServiceImpl.printUser(..))")
    public void after(){
        System.out.println("后置通知");
    }
    @AfterReturning("execution(* club.llf.service.impl.UserServiceImpl.printUser(..))")
    public void afterReturning(){
        System.out.println("返回通知");
    }
    @AfterThrowing("execution(* club.llf.service.impl.UserServiceImpl.printUser(..))")
    public void afterThrowing(){
        System.out.println("异常通知");
    }
1
@Around("execution(* club.llf.service.impl.UserServiceImpl.printUser(..))")
1
2
　public void around(ProccedingJoinPoint jp){ 　　　　　　　　　System.out.println("环绕通知前");　　　　　　　　　jp.proceed();　　　　　　　　　System.out.println("环绕通知后")
}
1
}
　　
2.定义切点
　　　Spring通过上述代码中的正则表达式  execution(* club.llf.service.impl.UserServiceImpl.printUser(..)) 来判断要拦截的方法
　　　　execution   　　: 执行方法时触发

 　　　*　　　　　　　　 : 任意返回值的方法

　　　　club.llf.service.impl.UserServiceImpl.printUser   :  拦截指定方法名

　　　　(..)　　　　　　　: 方法的参数任意

　　

3.@Pointcut注解（为了少写重复的正则表达式，可以使用）

package club.llf.aspect;
import org.aspectj.lang.annotation.*;
@Aspect
public class XXXAspect {
       @Pointcut("execution(* club.llf.service.impl.UserServiceImpl.printUser(..))")
    public void print(){
    }
    @Before("print()")
    public void before(){
        System.out.println("前置通知");
    }
    @After("print()")
    public void after(){
        System.out.println("后置通知");
    }
    @AfterReturning("print()")
    public void afterReturning(){
        System.out.println("返回通知");
    }
    @AfterThrowing("print()")
    public void afterThrowing(){
        System.out.println("异常通知");
    }
1
2
3
4
@Around("print()")
public void around(ProccedingJoinPoint jp){
    System.out.println("环绕通知前");　　　　　　　　　jp.proceed();　　　　　　　　　System.out.println("环绕通知后")
}
1
}
五、开启AOP自动代理
　　1.注解方式

　　　　　注解配置类如下（也可以给切面类加上@Component，并且扫描其所在包，那么就可以不配置下面的@Bean了）

@Configuration
@EnableAspectJAutoProxy
@ComponentScan(basePackages={"club.llf.pojo"})
public class AOPConfig{
        @Bean
        public RoleAspect  getRoleAspect(){
                return  new XXXAspect();
        }
}       
  2.XML方式


<aop:aspectj-autoproxy/>
<bean id="xxxAspect" class="club.llf.aspect.XXXAspect"/>
....
六、参数传递和引入
　　1.参数传递

　　　　在正则表达式中书写 execution(* club.llf.service.impl.UserServiceImpl.printUser(..) && args(参数名1,参数名2))

　　　　注解方法中加入和正则中相同的参数名即可完成传递。(参数名 一般是方法的真实参数名)

1
2
3
4
@AfterReturning("execution(* club.llf.service.impl.UserServiceImpl.printUser(..)  && args(role,sort))")
public void afterReturning(Role role,int sort){
    System.out.println("返回通知");
}
　　2.引入

 　　引入一般是对原有类没有的功能进行拓展

七、SpringAOP+注解实现日志管理
　　今天在再次深入学习SpringAOP之后想着基于注解的AOP实现日志功能，在面试过程中我们也经常会被问到:假如项目已经上线，如何增加一套日志功能?我们会说使用AOP，AOP也符合开闭原则:对代码的修改禁止的，对代码的扩展是允许的。今天经过自己的实践简单的实现了AOP日志。
　　在这里我只是简单的记录下当前操作的人、做了什么操作、操作结果是正常还是失败、操作时间，实际项目中，如果我们需要记录的更详细，可以记录当前操作人的详细信息，比如说部门、身份证号等信息，这些信息可以直接从session中获取，也可以从session中获取用户ID之后调用userService从数据库获取。我们还可以记录用户调用了哪个类的哪个方法，我们可以使用JoinPoint参数获取或者利用环绕通知ProceedingJoinPoint去获取。可以精确的定位到类、方法、参数，如果有必要我们就可以记录在日志中，看业务需求和我们的日志表的设计。如果再细致的记录日志，我们可以针对错误再建立一个错误日志表，在发生错误的情况下(异常通知里)记录日志的错误信息。
 
　　实现的大致思路是:
　　　　1.前期准备，设计日志表和日志类，编写日志Dao和Service以及实现
　　　　2.自定义注解，注解中加入几个属性，属性可以标识操作的类型(方法是做什么的)
　　　　3.编写切面，切点表达式使用上面的注解直接定位到使用注解的方法，
　　　　4.编写通知，通过定位到方法，获取上面的注解以及注解的属性，然后从session中直接获取或者从数据库获取当前登录用户的信息，最后根据业务处理一些日志信息之后调用日志Service存储日志。
　其实日志记录可以针对Controller层进行切入，也可以选择Service层进行切入，我选择的是基于Service层进行日志记录。网上的日志记录由的用前置通知，有的用环绕通知，我选择在环绕通知中完成，环绕通知中可以完成前置、后置、最终、异常通知的所有功能，因此我选择了环绕通知。(关于AOP的通知使用方法以及XML、注解AOP使用方法参考;http://www.cnblogs.com/qlqwjy/p/8729280.html)
　　
SpringMVC

·  流程说明：

1.客户端（浏览器）发送请求，直接请求到DispatcherServlet。

2.DispatcherServlet根据请求信息调用HandlerMapping，解析请求对应的Handler。

3.解析到对应的Handler（也就是我们平常说的Controller控制器）。

4.HandlerAdapter会根据Handler来调用真正的处理器来处理请求和执行相对应的业务逻辑。

5.处理器处理完业务后，会返回一个ModelAndView对象，Model是返回的数据对象，View是逻辑上的View。

6.ViewResolver会根据逻辑View去查找实际的View。

7.DispatcherServlet把返回的Model传给View（视图渲染）。

8.把View返回给请求者（浏览器）。

·  SpringMVC源码解析(1)-启动过程
1)web.xml配置方式
①　xml配置方式借助了了ContextLoaderListener来启动
②　ContextLoaderListener实现了ServletContextListener
③　由servlet标准可知ServletContextListener是容器的生命周期方法,springmvc就借助其启动与停止
④　ContextLoadListener调用了initWebApplicationContext方法,创建WebApplicationContext作为spring的容器上下文
⑤　而DispatcherServlet创建WebApplicationContext作为springmvc的上下文 并将ContextLoadListener创建的上下文设置为自身的parent
⑥　springmvc的applicationContext会去读取配置文件 我们来看一个最简单的配置文件

⑦　根据spring的自定义schema解析机制 ，可以看到mvc所有的标签解析器都定义在此

来看一下AnnotationDrivenBeanDefinitionParser解析器做了什么

解析过程较为复杂 通过注释我们可以得知以下对象将被装载


2)java方式

可以看到,同xml方式相同,java方式启动也有两个上下文

同xml方式类似,java配置启动方式也需要借助于servlet容器的生命周期方法,就是WebApplicationInitializer接口

注: WebApplicationInitializer是spring定义的接口,它能够响应容器生命周期的原因是因为SpringServletContainerInitializer

启动过程:

a.AbstractContextLoaderInitializer调用createRootApplicationContext创建spring上下文
b.AbstractDispatcherServletInitializer调用createServletApplicationContext创建springMVC上下文


·  SpringMVC源码解析(2)-DispatcherServlet
过程概括:
①　调用HandleMapping得到handler
②　调用HandleAdapter执行handle过程(参数解析 过程调用)
③　调用ViewResolver进行视图解析
④　渲染视图


·  SpringMVC源码解析(3)-HandleMapping
定义: 请求路径-处理过程映射管理

以RequestMappingHandlerMapping为例 我们先看下他的继承关系

可以看到有个InitlizingBean(spring的生命周期接口)我们就由他入手
过程概括:
1)获取所有object子类
2)根据条件过滤出handle处理类
3)解析handle类中定义的处理方法
4)保存解析得出的映射关系


·  SpringMVC源码解析(4)-HandlerAdapter
定义: 根据HandlerMapping.getHandler()得到的Handler信息,对http请求参数解析并绑定
以RequestMappingHandlerAdapter为例来讲,先看下继承关系

同样看到了实现了InitializingBean接口 从这入手
过程概括:

1.装载带有ControllerAdvices注解的对象
2.装载ArgumentResolvers(默认+自定义)
3.装载InitBinderArgumentResolvers(默认+自定义)
4.装载ReturnValueHandlers(默认+自定义)


·  SpringMVC源码解析(5)-HandlerExceptionResolver
Spring的异常统一处理非常简单，首先我们需要看一下Spring中定义的HandlerExceptionResolver接口：

他定义了一个resolveException方法，我们如果要处理异常的话，需要实现这个接口类，并且实现resolveException方法，在resolveException方法中处理自己的异常逻辑

自定义异常。
1)定义异常类MyExceptionResolver，实现这个接口类，并且实现resolveException方法，在resolveException方法中处理自己的异常逻辑
2)然后需要将我们的我们自定义的MyExceptionResolver类注入到bean中

·  SpringMVC源码解析(6)-异步请求
异步处理过程：当一个http请求进入后，tomcat等中间件的主线程调用副线程来执行业务逻辑，当副线程处理完成后，主线程再返回结果，在副线程处理整个业务逻辑的中，主线程会空闲出来去出来其他请求，也就是说采用上述这种模式处理http请求，服务器的吞吐量会有有明显的提升。使用异步返回，需使在web.xml将version配置为3.0版本的。


在servlet及所有的filter中配置异步支持。


简单实现如下：

更为复杂的业务场景的异步返回如下所示：

Htpp请求通过线程一处理，并将消息发送到消息队列，应用2处于不同的服务器，其接收到消息并将消息返回，线程2监听到处理结果，将消息返回，线程一及线程二不知道对方的存在。这种业务情况，单开一个线程是无法解决的，需要使用DeferredResult类。

拦截器
①　定义
　springMVC 的所有连接入口都会进入 DispatcherServlet，然后在这里面去调用真正的 Controller。而拦截器要达到的作用则是在调用 Controller 前后去做一些事情。所以现在需要看看 DispatcherServlet 的源码。
②　业务场景：
1、日志记录：记录请求信息的日志，以便进行信息监控、信息统计、计算PV（Page View）等。

2、权限检查：如登录检测，进入处理器检测检测是否登录，如果没有直接返回到登录页面；

3、性能监控：有时候系统在某段时间莫名其妙的慢，可以通过拦截器在进入处理器之前记录开始时间，在处理完后记录结束时间，从而得到该请求的处理时间（如果有反向代理，如apache可以自动记录）；
4、通用行为：读取cookie得到用户信息并将用户对象放入请求，从而方便后续流程使用，还有如提取Locale、Theme信息等，只要是多个处理器都需要的即可使用拦截器实现。

③　拦截器执行流程
当Interceptor 拦截到请求的时候，会执行 preHandle方法，如果这个preHandle方法返回true，就代表这个拦截器放行了，如果返回false，代表这个拦截器不会放行请求


如果放行了这个请求，那么，接下来就会执行目标方法，然后执行这个Interceptor的 postHandler方法，然后目标方法才渲染界面，返回视图模型(ModelAndView)，如果没有放行，接下来就不会执行其它的方法了


最后会执行这个Interceptor的afterCompletion方法


接下来说一下有多个拦截器:
假设有两个 拦截器 :
如果第一个 Interceptor 在执行preHandle放法的时候 就没有放行目标的话，那么其它的Interceptor 就都不放行了，接下来的所有方法都不会执行，什么都不会做，

如果第一个Interceptor放行了， 其它 Interceptor 没有放行的话，那么，第一个Interceptor的afterCompletion方法会被调用

多个拦截器情况下的方法的执行顺序：
所有的preHandle方法按顺序执行，postHandle和afterCompletion都逆序执行，具体原因，接下来会分析.

上面是Interceptor的大概运行流程，而接下来，我会在源码上进行分析:
锁定DispatcherServlet类的 doDispatcher 方法，注意打断点的地方:



先看第一个断点，在这个地方，执行了一个叫 applyPreHandle的方法，见名知意，一看就是执行preHandle的方法，这是这个方法的源码:


不用细看，只需要注意在这里面有一个 for循环，并且这个 for 循环是按照顺序执行的，这个for循环里干的事情就是按顺序执行所有的 Interceptor的 preHandle方法，如果这个方法执行失败，就直接结束整个doDispatcher方法的运行，这就可以验证了我上面所说的，如果一旦第一个preHandle方法运行失败，也就是第一个拦截器都不放行的话，后面的流程都没法继续走了，如果第一个preHandle方法放行了，第二个没有放行，那么也会继续走下去

再看第二个断点:



这里就不用细说了，就是执行目标方法，也验证了 我上面说的 执行完 preHandle方法后，就会执行目标方法，如果第一个preHandle方法放行了，第二个方法没有放行，那么也会执行到这个目标方法

第三个断点:



这里也是见方法名知意:执行 postHandle方法，也是如上所说，只要放行了，这个方法铁定会被执行,这是这个方法的源码:


它的作用就是 逆序执行 所有已放行了的Interceptor的 postHandler 方法的

和接下来看这个3个断点:

先是处理了 ModelAndView 的结果，然后在 2 个catch 语句里 有两个 triggerAfterCompletion方法，
先不管这个这个方法是什么意思，首先，既然是在catch语句里，那么 铁定是出现异常才会执行的方法，对吧,ok

先看最后一个断点:


首先这个方法就是在finally里面的，最后try cath 里面的内容结束的时候就会执行这个finally块里面的
代码，最后再看 triggerAfterCompletion 方法和 这最后一个断点方法的作用：

这是triggerAfterCompletion 方法的源码:



同样，这个方法里也有 for循环，并且是 逆序 的，它的作用就是 逆序 执行所有Interceptor的 afterCompletion 方法的

这是tapplyAfterConcurrentHandlingStarted方法的源码:


可以看到这个方法的源码和 上面那个方法的源码非常相似，由此可推出，这个方法也是逆序执行
所有已放行了的Interceptor 的 afterCompletion 方法的

综上可知， triggerAfterCompletion 方法 和 applyAfterConcurrentHandlingStarted方法都是执行afterCompletion 的，但是两个方法的位置不同，所以决定了它们的运行的时机不同，也就是我所说的，如果，第一个Interceptor 就放行了， 那么 这个方法的 afterCompletion 就会被执行，不管第二个，第三个，，，等等的Interceptor放行没有.
④　Springmvc拦截器三个方法执行时机？
springmvc的拦截器实现HandlerInterceptor接口后，会有三个抽象方法需要实现，分别为方法前执行preHandle，方法后postHandle，页面渲染后afterCompletion。

一.拦截器三个方法分别是:

1.1  preHandle

     预处理回调方法，实现处理器的预处理（如登录检查），第三个参数为响应的处理器（如具体的Controller实现）； 
返回值：true表示继续流程（如调用下一个拦截器或处理器）；false表示流程中断（如登录检查失败），不会继续调用其他的拦截器或处理器，此时我们需要通过response来产生响应；

1.2 postHandle

     后处理回调方法，实现处理器的后处理（但在渲染视图之前），此时我们可以通过modelAndView（模型和视图对象）对模型数据进行处理或对视图进行处理，modelAndView也可能为null。

1.3 afterCompletion

     整个请求处理完毕回调方法，即在视图渲染完毕时回调，如性能监控中我们可以在此记录结束时间并输出消耗时间，还可以进行一些资源清理，类似于try-catch-finally中的finally，但仅调用处理器执行链中preHandle返回true的拦截器的afterCompletion.

  首先用户请求到达前端控制器 DispatcherServlet,前端控制器找到处理器映射器,根据请求的方法找到对应的处理器handler,生成拦截器和handler执行顺序的执行链,交给DispatcherServlet,

dispatcherServlet找到对应的处理器适配器进行处理.

      prehandler在请求处理之前执行.该方法的返回值是布尔值 Boolean 类型的，当它返回为 false 时，表示请求结束，后续的 Interceptor 和 Controller 都不会再执行；当返回值为 true 时，就会继续调用下一个 Interceptor 的 preHandle 方法，如果已经是最后一个 Interceptor 的时候，就会是调用当前请求的 Controller 中的方法。

     postHandler 方法在当前请求进行处理之后，也就是在 Controller 中的方法调用之后执行，但是它会在 DispatcherServlet 进行视图返回渲染之前被调用，所以我们可以在这个方法中对 Controller 处理之后的 ModelAndView 对象进行操作。

     afterCompletion该方法将在整个请求结束之后，也就是在 DispatcherServlet 渲染了对应的视图之后执行，这个方法的主要作用是用于进行资源清理的工作。像异常处理资源释放会放在这一步.

   多个拦截器的执行顺序是: 拦截器A的preHandler-->拦截器B的preHandler-->B的postHandler-->A的postHandler-->B的afterCompletion-->A的afterCompletion 

 

⑤　拦截器使用

 

1、先自定义拦截器类，实现HandlerInterceptor接口，并重写抽象方法进行拦截器的拦截逻辑

 
过滤器
定义：
是在javaweb中，你传入的request、response提前过滤掉一些信息，或者提前设置一些参数，然后再传入servlet或者struts的action进行业务逻辑，比如过滤掉非法url（不是login.do的地址请求，如果用户没有登陆都过滤掉），或者在传入servlet或者 struts的action前统一设置字符集，或者去除掉一些非法字符.。
使用：
过滤器在web.xml中配置：
（1）因为一开始在过滤器中映射的url-pattern填写路径是*.action。所有的action要经过它的过滤。<url-pattern>*.action</url-pattern>


业务场景：
Filter主要是针对URL地址做一个编码的事情、过滤掉没用的参数、安全校验（比较泛的，比如登录不登录之类）


过滤器和拦截器的执行顺序




拦截器与过滤器具体的区别
过滤器： 是servlet规范中的一部分，任何java web工程都可以使用。

拦截器： 是SpringMVC框架自己的，只使用了SpringMVC框架的工程才能用。

过滤器： 在url-pattern中配置了/*之后，可以对所有要访问的资源拦截。

拦截器： 他是只会拦截访问的控制器方法，如果访问的是jsp，html，css，image或者js是不会进行拦截的。

它也是AOP思想的具体应用。

我们要向自定义拦截器，要求必须实现：HandlerInterceptor接口。


拦截器 ：是在面向切面编程的就是在你的service或者一个方法，前调用一个方法，或者在方法后调用一个方法比如动态代理就是拦截器的简单实现，在你调用方法前打印出字符串（或者做其它业务逻辑的操作），也可以在你调用方法后打印出字符串，甚至在你抛出异常的时候做业务逻辑的操作。

过滤器：是在javaweb中，你传入的request、response提前过滤掉一些信息，或者提前设置一些参数，然后再传入servlet或者struts的action进行业务逻辑，比如过滤掉非法url（不是login.do的地址请求，如果用户没有登陆都过滤掉），或者在传入servlet或者 struts的action前统一设置字符集，或者去除掉一些非法字符.。

具体区别
拦截器是AOP( Aspect-Oriented Programming)的一种实现，底层通过动态代理模式完成。

区别：

（1）拦截器是基于java的反射机制的，而过滤器是基于函数回调。

（2）拦截器不依赖于servlet容器，而过滤器依赖于servlet容器。

（3）拦截器只能对action请求起作用，而过滤器则可以对几乎所有的请求起作用。

（4）拦截器可以访问action上下文、值栈里的对象，而过滤器不能。

（5）在action的生命周期中，拦截器可以多次被调用，而过滤器只能在容器初始化时被调用一次。

 两者的本质区别：从灵活性上说拦截器功能更强大些，Filter能做的事情，他都能做，而且可以在请求前，请求后执行，比较灵活。Filter主要是针对URL地址做一个编码的事情、过滤掉没用的参数、安全校验（比较泛的，比如登录不登录之类），太细的话，还是建议用interceptor。不过还是根据不同情况选择合适的。

Spring中运用了那些设计模式
1.工厂设计模式：Spring使用工厂模式通过BeanFactory和ApplicationContext创建bean对象。
2.代理设计模式：Spring AOP功能的实现。
3.单例设计模式：Spring中的bean默认都是单例的。
4.模板方法模式：Spring中的jdbcTemplate、hibernateTemplate等以Template结尾的对数据库操作的类，它们就使用到了模板模式。
5.包装器设计模式：我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。
6.观察者模式：Spring事件驱动模型就是观察者模式很经典的一个应用。
7.适配器模式：Spring AOP的增强或通知（Advice）使用到了适配器模式、Spring MVC中也是用到了适配器模式适配Controller。

Spring的Bean是单例还是多例
springboot 采用的是单例模式
@Component注解默认实例化的对象是单例，如果想声明成多例对象可以使用
 @Repository默认单例
 @Service默认单例
@Controller默认多例
Spring中的Bean是线程安全的吗
Spring容器中的Bean是否线程安全，容器本身并没有提供Bean的线程安全策略，因此可以说Spring容器中的Bean本身不具备线程安全的特性，但是具体还是要结合具体scope的Bean去研究。

Spring 的 bean 作用域（scope）类型
1、singleton:单例，默认作用域。

2、prototype:原型，每次创建一个新对象。

3、request:请求，每次Http请求创建一个新对象，适用于WebApplicationContext环境下。

4、session:会话，同一个会话共享一个实例，不同会话使用不用的实例。

5、global-session:全局会话，所有会话共享一个实例。

线程安全这个问题，要从单例与原型Bean分别进行说明。

原型Bean
对于原型Bean,每次创建一个新对象，也就是线程之间并不存在Bean共享，自然是不会有线程安全的问题。

单例Bean
对于单例Bean,所有线程都共享一个单例实例Bean,因此是存在资源的竞争。

如果单例Bean,是一个无状态Bean，也就是线程中的操作不会对Bean的成员执行查询以外的操作，那么这个单例Bean是线程安全的。比如Spring mvc 的 Controller、Service、Dao等，这些Bean大多是无状态的，只关注于方法本身。

对于有状态的bean，Spring官方提供的bean，一般提供了通过ThreadLocal去解决线程安全的方法，比如RequestContextHolder、TransactionSynchronizationManager、LocaleContextHolder等。

使用ThreadLocal的好处
使得多线程场景下，多个线程对这个单例Bean的成员变量并不存在资源的竞争，因为ThreadLocal为每个线程保存线程私有的数据。这是一种以空间换时间的方式。

当然也可以通过加锁的方法来解决线程安全，这种以时间换空间的场景在高并发场景下显然是不实际的。
Spring中的Bean的作用域
① singleton
使用该属性定义Bean时，IOC容器仅创建一个Bean实例，IOC容器每次返回的是同一个Bean实例。
② prototype
使用该属性定义Bean时，IOC容器可以创建多个Bean实例，每次返回的都是一个新的实例。
③ request
该属性仅对HTTP请求产生作用，使用该属性定义Bean时，每次HTTP请求都会创建一个新的Bean，适用于WebApplicationContext环境。
④ session
该属性仅用于HTTP Session，同一个Session共享一个Bean实例。不同Session使用不同的实例。
⑤ global-session
该属性仅用于HTTP Session，同session作用域不同的是，所有的Session共享一个Bean实例。

Spring中的Bean的生命周期
1.Bean容器找到配置文件中Spring Bean的定义。

2.Bean容器利用Java Reflection API创建一个Bean的实例。

3.如果涉及到一些属性值，利用set()方法设置一些属性值。

4.如果Bean实现了BeanNameAware接口，调用setBeanName()方法，传入Bean的名字。

5.如果Bean实现了BeanClassLoaderAware接口，调用setBeanClassLoader()方法，传入ClassLoader对象的实例。

6.如果Bean实现了BeanFactoryAware接口，调用setBeanClassFacotory()方法，传入ClassLoader对象的实例。

7.与上面的类似，如果实现了其他*Aware接口，就调用相应的方法。

8.如果有和加载这个Bean的Spring容器相关的BeanPostProcessor对象，执行postProcessBeforeInitialization()方法。

9.如果Bean实现了InitializingBean接口，执行afeterPropertiesSet()方法。

10.如果Bean在配置文件中的定义包含init-method属性，执行指定的方法。

11.如果有和加载这个Bean的Spring容器相关的BeanPostProcess对象，执行postProcessAfterInitialization()方法。

12.当要销毁Bean的时候，如果Bean实现了DisposableBean接口，执行destroy()方法。

13.当要销毁Bean的时候，如果Bean在配置文件中的定义包含destroy-method属性，执行指定的方法。



spring事务与数据库事务与锁之间的关系
spring事务本质上使用数据库事务，而数据库事务本质上使用数据库锁，所以spring事务本质上使用数据库锁，开启spring事务意味着使用数据库锁；
Spring事务的传播行为
Spring定义了七种传播行为：

事务传播行为类型 说明
①　PROPAGATION_REQUIRED 如果当前没有事务，就新建一个事务，如果已经存在一个事务中，加入到这个事务中。这是最常见的选择。
②　PROPAGATION_SUPPORTS 支持当前事务，如果当前没有事务，就以非事务方式执行。
③　PROPAGATION_MANDATORY 使用当前的事务，如果当前没有事务，就抛出异常。
④　PROPAGATION_REQUIRES_NEW 新建事务，如果当前存在事务，把当前事务挂起。
⑤　PROPAGATION_NOT_SUPPORTED 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。
⑥　PROPAGATION_NEVER 以非事务方式执行，如果当前存在事务，则抛出异常。
⑦　PROPAGATION_NESTED 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则执行与PROPAGATION_REQUIRED类似的操作。
Spring事务默认抛出什么异常
RuntimeException或者Error。

        常见的RuntimeException
        1、NullPointerException：一般都是在null对象上调用方法了。
        2、NumberFormatException：继承IllegalArgumentException(非法参数)，字符串转换为数字时出现。
        3、ArrayIndexOutOfBoundsException:数组越界。
        4、StringIndexOutOfBoundsException：字符串越界。比如 String s="hello"; char c=s.chatAt(6);
        5、ClassCastException:类型转换错误。比如 Object obj=new Object(); String s=(String)obj;
        6、UnsupportedOperationException:该操作不被支持。有可能子类中不想支持父类中有的方法，可以直接抛出
        7、ArithmeticException：算术错误，典型的就是0作为除数的时候。
SpringBoot
什么是SpringBoot？
1、用来简化spring初始搭建和开发过程使用特定的方式进行配置(properties或者yml文件)
2、创建独立的spring引用程序main方法运行
3、嵌入Tomcat无需部署war包，直接打成jar包nohup java -jar – & 启动就好
4、简化了maven的配置
4、自动配置spring添加对应的starter自动化配置


SpringBoot自动配置原理：
1、@EnableAutoConfiguration这个注解会"猜"你将如何配置spring，前提是你已经添加了jar依赖项，如果spring-boot-starter-web已经添加Tomcat和SpringMVC，这个注释就会自动假设您在开发一个web应用程序并添加相应的spring配置，会自动去maven中读取每个starter中的spring.factories文件，该文件里配置了所有需要被创建spring容器中bean
2、在main方法中加上@SpringBootApplication和@EnableAutoConfiguration
SpringBoot starter工作原理：
1、SpringBoot在启动时扫描项目依赖的jar包，寻找包含spring.factories文件的jar
2、根据spring.factories配置加载AutoConfigure
3、根据@Conditional注解的条件，进行自动配置并将bean注入到Spring Context
SpringBoot的优点：
1、减少开发、测试时间和努力
2、使用JavaConfig有助于避免使用XML
3、避免大量的maven导入和各种版本冲突
4、提供意见发展方法
5、通过提供默认值快速开始开发
6、没有单独的web服务器需要，这就意味着不再需要启动Tomcat、Glassfish或其他任何东西
7、需要更少的配置，因为没有web.xml文件。只需添加用@Configuration注释的类，然后添加用@Bean注释的方法，Spring将自动加载对象并像以前一样对其进行管理。甚至可以将@Autowired添加到bean方法中，以使用Spring自动装入需要的依赖关系中
SpringBoot实现原理
1)springboot应用，只需要配置@SpringBootAplication注解，便可以自动启动，为什么呢？

2)我们进入这个注解可以发现@SpringBootAplication是一个组合annotation，其中最重要的annotation是@SpringBootConfiguration，@EnableAutoConfiguration和@ComponentScan。 
3)@SpringBootConfiguration本质上是一个@Configuration。启动类标注了@SpringBootConfiguration之后，本身其实也是一个IOC容器的配置类。

4)@ComponentScan注解完成的是自动扫描的功能，相当于Spring XML配置文件中的：<context:component-scan>，可以使用encludeFilters，includeFilters等属性指定或排除要扫描的包，以及扫描的条件。最终将这些bean定义加载到容器中。

5)@EnableAutoConfiguration是让Spring Boot的配置能够如此简化的关键性注解，Spring-Boot 根据应用所声明的jar包依赖来对Spring框架进行自动配置。比如根据spring-boot-starter-web ，来判断你的项目是否需要添加了webmvc和tomcat，就会自动的帮你配置web项目中所需要的默认配置。
6)类EnableAutoConfigurationImportSelector是一个ImportSelector接口的实现类，而ImportSelector接口中的selectImports方法所返回的类将被Spring容器管理起来。
注解@EnableAutoConfiguration怎么生效的呢？

在主启动类的main函数中，会调用SpringApplication.run(DemoApplication.class,args)，然后会分两步执行，第一步先创建SpringApplication对象，第二步运行run方法。
第一步、创建SpringApplication对象

这一步的主要功能初始化SpringApplication对象，从类路径下找到META-INF/spring.factories配置的所有ApplicationContextInitializer和ApplicationListener，然后保存起来，以便后边调用，最后找到main方法的主配置类。
第二步、运行run方法

run方法中的重要点就是创建IOc容器context = createApplicationContext();，最后返回启动的IOC容器。中间会调用refreshContent()，最后调用到spring容器的refresh时，invokeBeanFactoryPostProcessors(beanFactory)方法中调用到ConfigurationClassPostProcessor。ConfigurationClassPostProcessor会解析到我们的主类，把@Import中的类拿出来，调用它的selectImports()方法。然后spring容器会对selectInports方法返回的配置类进行处理。
RestController与Controller区别
1 @RestController=@Controller+@ResponseBody
 2是否可以返回页面
　　答：@RestController无法返回指定页面，而@Controller可以。
　　解析：对于Controller， 如果只是使用@RestController注解，则其方法无法返回指定页面，此时配置的视图解析器 InternalResourceViewResolver不起作用，返回的内容就是 return 里的内容。 如果需要返回到指定页面，则需要用 @Controller配合视图解析器InternalResourceViewResolver才行。
 3 返回内容
　　如果需要返回JSON，XML或自定义mediaType内容到页面，@RestController自己就可以搞定,这个注解对于返回数据比较方便，因为它会自动将对象实体转换为JSON格式。而@Controller需要在对应的方法加上@ResponseBody注解。

SpringCloud
SpringCloud分布式开发五大组件详解
服务发现——Netflix Eureka
客服端负载均衡——Netflix Ribbon
断路器——Netflix Hystrix
服务网关——Netflix Zuul
分布式配置——Spring Cloud Config
Eureka

一个RESTful服务，用来定位运行在AWS地区（Region）中的中间层服务。由两个组件组成：Eureka服务器和Eureka客户端。Eureka服务器用作服务注册服务器。Eureka客户端是一个java客户端，用来简化与服务器的交互、作为轮询负载均衡器，并提供服务的故障切换支持。Netflix在其生产环境中使用的是另外的客户端，它提供基于流量、资源利用率以及出错状态的加权负载均衡。
Ribbon

Ribbon，主要提供客户侧的软件负载均衡算法。
Ribbon客户端组件提供一系列完善的配置选项，比如连接超时、重试、重试算法等。Ribbon内置可插拔、可定制的负载均衡组件。下面是用到的一些负载均衡策略：
①　简单轮询负载均衡
②　加权响应时间负载均衡
③　区域感知轮询负载均衡
④　随机负载均衡
Ribbon中还包括以下功能：
易于与服务发现组件（比如Netflix的Eureka）集成
使用Archaius完成运行时配置
使用JMX暴露运维指标，使用Servo发布
多种可插拔的序列化选择
异步和批处理操作（即将推出）
自动SLA框架（即将推出）
系统管理/指标控制台（即将推出）
Hystrix

断路器可以防止一个应用程序多次试图执行一个操作，即很可能失败，允许它继续而不等待故障恢复或者浪费 CPU 周期，而它确定该故障是持久的。断路器模式也使应用程序能够检测故障是否已经解决。如果问题似乎已经得到纠正​​，应用程序可以尝试调用操作。

断路器增加了稳定性和灵活性，以一个系统，提供稳定性，而系统从故障中恢复，并尽量减少此故障的对性能的影响。它可以帮助快速地拒绝对一个操作，即很可能失败，而不是等待操作超时（或者不返回）的请求，以保持系统的响应时间。如果断路器提高每次改变状态的时间的事件，该信息可以被用来监测由断路器保护系统的部件的健康状况，或以提醒管理员当断路器跳闸，以在打开状态。

流程图


zuul

类似nginx，反向代理的功能，不过netflix自己增加了一些配合其他组件的特性。
Springcloud config

这个还是静态的，得配合Spring Cloud Bus实现动态的配置更新。

参考

一.微服务的优点缺点?说下开发项目中遇到的坑?

优点:

1.每个服务直接足够内聚，代码容易理解
2.开发效率高，一个服务只做一件事，适合小团队开发
3.松耦合，有功能意义的服务。
4.可以用不同语言开发，面向接口编程。
5.易于第三方集成
6.微服务只是业务逻辑的代码，不会和HTML,CSS或其他界面结合.
7.可以灵活搭配，连接公共库/连接独立库

缺点:
1.分布式系统的责任性
2.多服务运维难度加大。
3.系统部署依赖，服务间通信成本，数据一致性，系统集成测试，性能监控。

二.什么是springcloud？
Spring cloud流应用程序启动器是基于Spring Boot的Spring集成应用程序，提供与外部系统的集成。Spring cloud Task，一个生命周期短暂的微服务框架，用于快速构建执行有限数据处理的应用程序

三.spring cloud 和dubbo区别?

1.服务调用方式 dubbo是RPC springcloud Rest Api
2.注册中心,dubbo 是zookeeper springcloud是eureka，也可以是zookeeper
3.服务网关,dubbo本身没有实现，只能通过其他第三方技术整合，springcloud有Zuul路由网关，作为路由服务器，进行消费者的请求分发,springcloud支持断路器，与git完美集成配置文件支持版本控制，事物总线实现配置文件的更新与服务自动装配等等一系列的微服务架构要素。

四.REST 和RPC对比

1.RPC主要的缺陷是服务提供方和调用方式之间的依赖太强，需要对每一个微服务进行接口的定义，并通过持续继承发布，严格版本控制才不会出现冲突。
2.REST是轻量级的接口，服务的提供和调用不存在代码之间的耦合，只需要一个约定进行规范。

五.你所知道的微服务技术栈？

维度(springcloud)
服务开发：springboot spring springmvc
服务配置与管理:Netfix公司的Archaiusm ,阿里的Diamond
服务注册与发现:Eureka,Zookeeper
服务调用:Rest RPC gRpc
服务熔断器:Hystrix
服务负载均衡:Ribbon Nginx
服务接口调用:Fegin
消息队列:Kafka Rabbitmq activemq
服务配置中心管理:SpringCloudConfig
服务路由（API网关）Zuul
事件消息总线:SpringCloud Bus

六.负载均衡的意义是什么?

在计算中，负载均衡可以改善跨计算机，计算机集群，网络链接，中央处理单元或磁盘驱动器等多种计算资源的工作负载分布。负载均衡旨在优化资源使用，最大吞吐量，最小响应时间并避免任何单一资源的过载。使用多个组件进行负载均衡而不是单个组件可能会通过冗余来提高可靠性和可用性。负载平衡通常涉及专用软件或硬件，例如多层交换机或域名系统服务进程。

七.微服务之间是如何独立通讯的?

1.远程调用，比如feign调用，直接通过远程过程调用来访问别的service。
2.消息中间件

八.springcloud如何实现服务的注册?

1.服务发布时，指定对应的服务名,将服务注册到 注册中心(eureka zookeeper)
2.注册中心加@EnableEurekaServer,服务用@EnableDiscoveryClient，然后用ribbon或feign进行服务直接的调用发现。
九.Eureka和Zookeeper区别

1.Eureka取CAP的AP，注重可用性，Zookeeper取CAP的CP注重
一致性。
2.Zookeeper在选举期间注册服务瘫痪，虽然服务最终会恢复，但选举期间不可用。
3.eureka的自我保护机制，会导致一个结果就是不会再从注册列表移除因长时间没收到心跳而过期的服务。依然能接受新服务的注册和查询请求，但不会被同步到其他节点。不会服务瘫痪。
4.Zookeeper有Leader和Follower角色，Eureka各个节点平等。
5.Zookeeper采用过半数存活原则，Eureka采用自我保护机制解决分区问题。
6.eureka本质是一个工程，Zookeeper只是一个进程。

十.eureka自我保护机制是什么?
1.当Eureka Server 节点在短时间内丢失了过多实例的连接时（比如网络故障或频繁启动关闭客户端）节点会进入自我保护模式，保护注册信息，不再删除注册数据，故障恢复时，自动退出自我保护模式。

十一.什么是服务熔断？什么是服务降级?
服务直接的调用，比如在高并发情况下出现进程阻塞，导致当前线程不可用，慢慢的全部线程阻塞，导致服务器雪崩。
服务熔断：相当于保险丝，出现某个异常，直接熔断整个服务，而不是一直等到服务超时。通过维护一个自己的线程池，当线程到达阈值的时候就启动服务降级，如果其他请求继续访问就直接返回fallback的默认值。

十二.什么是Ribbon？
ribbon是一个负载均衡客户端，可以很好的控制htt和tcp的一些行为。feign默认集成了ribbon。

十三.什么是feigin？它的优点是什么？
1.feign采用的是基于接口的注解
2.feign整合了ribbon，具有负载均衡的能力
3.整合了Hystrix，具有熔断的能力

使用:
1.添加pom依赖。
2.启动类添加@EnableFeignClients
3.定义一个接口@FeignClient(name=“xxx”)指定调用哪个服务

十四.Ribbon和Feign的区别？
1.Ribbon都是调用其他服务的，但方式不同。
2.启动类注解不同，Ribbon是@RibbonClient feign的是@EnableFeignClients
3.服务指定的位置不同，Ribbon是在@RibbonClient注解上声明，Feign则是在定义抽象方法的接口中使用@FeignClient声明。
4.调用方式不同，Ribbon需要自己构建http请求，模拟http请求然后使用RestTemplate发送给其他服务，步骤相当繁琐。Feign需要将调用的方法定义成抽象方法即可。
十五.什么是Spring Cloud Bus?
spring cloud bus 将分布式的节点用轻量的消息代理连接起来，它可以用于广播配置文件的更改或者服务直接的通讯，也可用于监控。
如果修改了配置文件，发送一次请求，所有的客户端便会重新读取配置文件。
使用:
1.添加依赖
2.配置rabbimq

十六.springcloud断路器作用?
当一个服务调用另一个服务由于网络原因或自身原因出现问题，调用者就会等待被调用者的响应 当更多的服务请求到这些资源导致更多的请求等待，发生连锁效应（雪崩效应）
断路器有完全打开状态:一段时间内 达到一定的次数无法调用 并且多次监测没有恢复的迹象 断路器完全打开 那么下次请求就不会请求到该服务
半开:短时间内 有恢复迹象 断路器会将部分请求发给该服务，正常调用时 断路器关闭
关闭：当服务一直处于正常状态 能正常调用

十七.什么是SpringCloudConfig?
在分布式系统中，由于服务数量巨多，为了方便服务配置文件统一管理，实时更新，所以需要分布式配置中心组件。在Spring Cloud中，有分布式配置中心组件spring cloud config ，它支持配置服务放在配置服务的内存中（即本地），也支持放在远程Git仓库中。在spring cloud config 组件中，分两个角色，一是config server，二是config client。

使用：
1、添加pom依赖
2、配置文件添加相关配置
3、启动类添加注解@EnableConfigServer

十八.Spring Cloud Gateway?
Spring Cloud Gateway是Spring Cloud官方推出的第二代网关框架，取代Zuul网关。网关作为流量的，在微服务系统中有着非常作用，网关常见的功能有路由转发、权限校验、限流控制等作用。

使用了一个RouteLocatorBuilder的bean去创建路由，除了创建路由RouteLocatorBuilder可以让你添加各种predicates和filters，predicates断言的意思，顾名思义就是根据具体的请求的规则，由具体的route去处理，filters是各种过滤器，用来对请求做各种判断和修改。

十九.架构?
在微服务架构中，需要几个基础的服务治理组件，包括服务注册与发现、服务消费、负载均衡、断路器、智能路由、配置管理等，由这几个基础组件相互协作，共同组建了一个简单的微服务系统

在Spring Cloud微服务系统中，一种常见的负载均衡方式是，客户端的请求首先经过负载均衡（zuul、Ngnix），再到达服务网关（zuul集群），然后再到具体的服。，服务统一注册到高可用的服务注册中心集群，服务的所有的配置文件由配置服务管理，配置服务的配置文件放在git仓库，方便开发人员随时改配置。

二十.什么是Hystrix?
防雪崩利器，具备服务降级，服务熔断，依赖隔离，监控（Hystrix Dashboard）
服务降级:
双十一 提示 哎哟喂，被挤爆了。 app秒杀 网络开小差了，请稍后再试。
优先核心服务，非核心服务不可用或弱可用。通过HystrixCommand注解指定。
fallbackMethod(回退函数)中具体实现降级逻辑。


Mybatis
Mybatis原理
1.SqlSessionFactory 与 SqlSession.
通过前面的章节对于mybatis 的介绍及使用，大家都能体会到SqlSession的重要性了吧， 没错，从表面上来看，咱们都是通过SqlSession去执行sql语句（注意：是从表面看，实际的待会儿就会讲）。那么咱们就先看看是怎么获取SqlSession的吧：

1)首先，SqlSessionFactoryBuilder去读取mybatis的配置文件，然后build一个DefaultSqlSessionFactory
2)当我们获取到SqlSessionFactory之后，就可以通过SqlSessionFactory去获取SqlSession对象
3)SqlSession咱们也拿到了，咱们可以调用SqlSession中一系列的select...,  insert..., update..., delete...方法轻松自如的进行CRUD操作了。 就这样？ 那咱配置的映射文件去哪儿了？  别急， 咱们接着往下看：
2.利器之MapperProxy:

在mybatis中，通过MapperProxy动态代理咱们的dao， 也就是说， 当咱们执行自己写的dao里面的方法的时候，其实是对应的mapperProxy在代理。那么，咱们就看看怎么获取MapperProxy对象吧：
（1）通过SqlSession从Configuration中获取。源码如下：

复制代码
/**
   * 什么都不做，直接去configuration中找， 哥就是这么任性
   */
  @Override
  public <T> T getMapper(Class<T> type) {
    return configuration.<T>getMapper(type, this);
  }
复制代码
（2）SqlSession把包袱甩给了Configuration, 接下来就看看Configuration。源码如下：

复制代码
/**
   * 烫手的山芋，俺不要，你找mapperRegistry去要
   * @param type
   * @param sqlSession
   * @return
   */
  public <T> T getMapper(Class<T> type, SqlSession sqlSession) {
    return mapperRegistry.getMapper(type, sqlSession);
  }
复制代码
（3）Configuration不要这烫手的山芋，接着甩给了MapperRegistry， 那咱看看MapperRegistry。 源码如下：

复制代码
/**
   * 烂活净让我来做了，没法了，下面没人了，我不做谁来做
   * @param type
   * @param sqlSession
   * @return
   */
  @SuppressWarnings("unchecked")
  public <T> T getMapper(Class<T> type, SqlSession sqlSession) {
    //能偷懒的就偷懒，俺把粗活交给MapperProxyFactory去做
    final MapperProxyFactory<T> mapperProxyFactory = (MapperProxyFactory<T>) knownMappers.get(type);
    if (mapperProxyFactory == null) {
      throw new BindingException("Type " + type + " is not known to the MapperRegistry.");
    }
    try {
      //关键在这儿
      return mapperProxyFactory.newInstance(sqlSession);
    } catch (Exception e) {
      throw new BindingException("Error getting mapper instance. Cause: " + e, e);
    }
  }
复制代码
(4) MapperProxyFactory是个苦B的人，粗活最终交给它去做了。咱们看看源码：

复制代码
/**
   * 别人虐我千百遍，我待别人如初恋
   * @param mapperProxy
   * @return
   */
  @SuppressWarnings("unchecked")
  protected T newInstance(MapperProxy<T> mapperProxy) {
    //动态代理我们写的dao接口
    return (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[] { mapperInterface }, mapperProxy);
  }
  
  public T newInstance(SqlSession sqlSession) {
    final MapperProxy<T> mapperProxy = new MapperProxy<T>(sqlSession, mapperInterface, methodCache);
    return newInstance(mapperProxy);
  }
复制代码
通过以上的动态代理，咱们就可以方便地使用dao接口啦， 就像之前咱们写的demo那样：

 UserDao userMapper = sqlSession.getMapper(UserDao.class);  
 User insertUser = new User();
这下方便多了吧， 呵呵， 貌似mybatis的源码就这么一回事儿啊。

别急，还没完， 咱们还没看具体是怎么执行sql语句的呢。
3.Excutor（具体执行sql）:


接下来，咱们才要真正去看sql的执行过程了。

上面，咱们拿到了MapperProxy, 每个MapperProxy对应一个dao接口， 那么咱们在使用的时候，MapperProxy是怎么做的呢？ 源码奉上：


MapperProxy:


复制代码
/**
   * MapperProxy在执行时会触发此方法
   */
  @Override
  public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
    if (Object.class.equals(method.getDeclaringClass())) {
      try {
        return method.invoke(this, args);
      } catch (Throwable t) {
        throw ExceptionUtil.unwrapThrowable(t);
      }
    }
    final MapperMethod mapperMethod = cachedMapperMethod(method);
    //二话不说，主要交给MapperMethod自己去管
    return mapperMethod.execute(sqlSession, args);
  }
复制代码
MapperMethod:


复制代码
 /**
   * 看着代码不少，不过其实就是先判断CRUD类型，然后根据类型去选择到底执行sqlSession中的哪个方法，绕了一圈，又转回sqlSession了
   * @param sqlSession
   * @param args
   * @return
   */
  public Object execute(SqlSession sqlSession, Object[] args) {
    Object result;
    if (SqlCommandType.INSERT == command.getType()) {
      Object param = method.convertArgsToSqlCommandParam(args);
      result = rowCountResult(sqlSession.insert(command.getName(), param));
    } else if (SqlCommandType.UPDATE == command.getType()) {
      Object param = method.convertArgsToSqlCommandParam(args);
      result = rowCountResult(sqlSession.update(command.getName(), param));
    } else if (SqlCommandType.DELETE == command.getType()) {
      Object param = method.convertArgsToSqlCommandParam(args);
      result = rowCountResult(sqlSession.delete(command.getName(), param));
    } else if (SqlCommandType.SELECT == command.getType()) {
      if (method.returnsVoid() && method.hasResultHandler()) {
        executeWithResultHandler(sqlSession, args);
        result = null;
      } else if (method.returnsMany()) {
        result = executeForMany(sqlSession, args);
      } else if (method.returnsMap()) {
        result = executeForMap(sqlSession, args);
      } else {
        Object param = method.convertArgsToSqlCommandParam(args);
        result = sqlSession.selectOne(command.getName(), param);
      }
    } else {
      throw new BindingException("Unknown execution method for: " + command.getName());
    }
    if (result == null && method.getReturnType().isPrimitive() && !method.returnsVoid()) {
      throw new BindingException("Mapper method '" + command.getName() 
          + " attempted to return null from a method with a primitive return type (" + method.getReturnType() + ").");
    }
    return result;
  }
复制代码
既然又回到SqlSession了， 那么咱们就看看SqlSession的CRUD方法了，为了省事，还是就选择其中的一个方法来做分析吧。这儿，咱们选择了selectList方法：

复制代码
public <E> List<E> selectList(String statement, Object parameter, RowBounds rowBounds) {
    try {
      MappedStatement ms = configuration.getMappedStatement(statement);
      //CRUD实际上是交给Excetor去处理， excutor其实也只是穿了个马甲而已，小样，别以为穿个马甲我就不认识你嘞！
      return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER);
    } catch (Exception e) {
      throw ExceptionFactory.wrapException("Error querying database.  Cause: " + e, e);
    } finally {
      ErrorContext.instance().reset();
    }
  }
复制代码
然后，通过一层一层的调用，最终会来到doQuery方法， 这儿咱们就随便找个Excutor看看doQuery方法的实现吧，我这儿选择了SimpleExecutor:

复制代码
public <E> List<E> doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException {
    Statement stmt = null;
    try {
      Configuration configuration = ms.getConfiguration();
      StatementHandler handler = configuration.newStatementHandler(wrapper, ms, parameter, rowBounds, resultHandler, boundSql);
      stmt = prepareStatement(handler, ms.getStatementLog());
      //StatementHandler封装了Statement, 让 StatementHandler 去处理
      return handler.<E>query(stmt, resultHandler);
    } finally {
      closeStatement(stmt);
    }
  }
复制代码
接下来，咱们看看StatementHandler 的一个实现类 PreparedStatementHandler（这也是我们最常用的，封装的是PreparedStatement）, 看看它使怎么去处理的：

复制代码
public <E> List<E> query(Statement statement, ResultHandler resultHandler) throws SQLException {
     //到此，原形毕露， PreparedStatement, 这个大家都已经滚瓜烂熟了吧
    PreparedStatement ps = (PreparedStatement) statement;
    ps.execute();
    //结果交给了ResultSetHandler 去处理
    return resultSetHandler.<E> handleResultSets(ps);
  }
复制代码
到此， 一次sql的执行流程就完了。 我这儿仅抛砖引玉，建议有兴趣的去看看Mybatis3的源码。

好啦，本次就到此结束啦，最近太忙了， 又该忙去啦。


Mybatis缓存
mybatis的查询缓存分为一级缓存和二级缓存，一级缓存是SqlSession级别的缓存，二级缓存时mapper级别的缓存，二级缓存是多个SqlSession共享的。mybatis通过缓存机制减轻数据压力，提高数据库性能。


一级缓存：

mybatis的一级缓存是SQLSession级别的缓存，在操作数据库时需要构造SqlSession对象，在对象中有一个HashMap用于存储缓存数据，不同的SqlSession之间缓存数据区域（HashMap）是互相不影响的。

一级缓存的作用域是SqlSession范围的，当在同一个SqlSession中执行两次相同的sql语句时，第一次执行完毕会将数据库中查询的数据写到缓存（内存）中，第二次查询时会从缓存中获取数据，不再去底层进行数据库查询，从而提高了查询效率。需要注意的是：如果SqlSession执行了DML操作（insert、update、delete），并执行commit（）操作，mybatis则会清空SqlSession中的一级缓存，这样做的目的是为了保证缓存数据中存储的是最新的信息，避免出现脏读现象。

当一个SqlSession结束后该SqlSession中的一级缓存也就不存在了，Mybatis默认开启一级缓存，不需要进行任何配置。

注意：Mybatis的缓存机制是基于id进行缓存，也就是说Mybatis在使用HashMap缓存数据时，是使用对象的id作为key，而对象作为value保存

二级缓存：

二级缓存是mapper级别的缓存，使用二级缓存时，多个SqlSession使用同一个Mapper的sql语句去操作数据库，得到的数据会存在二级缓存区域，它同样是使用HashMapper进行数据存储，相比一级缓存SqlSession，二级缓存的范围更大，多个SqlSession可以共用二级缓存，二级缓存是跨SqlSession的。

二级缓存是多个SqlSession共享的，其作用域是mapper的同一个namespace，不同的SqlSession两次执行相同的namespace下的sql语句，且向sql中传递的参数也相同，即最终执行相同的sql语句，则第一次执行完毕会将数据库中查询的数据写到缓存（内存），第二次查询时会从缓存中获取数据，不再去底层数据库查询，从而提高查询效率。

Mybatis默认没有开启二级缓存，需要在setting全局参数中配置开启二级缓存。

在mybatis-config.xml中配置：

cacheEnabled的value为true表示在此配置文件下开启二级缓存，该属性默认为false。
在EmployeeMapper.xml中配置：

以上配置创建了一个LRU缓存，并每隔60秒刷新，最大存储512个对象，而且返回的对象被认为是只读。
Mybatis中运用了那些设计模式
什么是Mybatis？#
Mybatis 是一个半 ORM（对象关系映射）框架，它内部封装了 JDBC，开发时
只需要关注 SQL 语句本身，不需要花费精力去处理加载驱动、创建连接、创建
statement 等繁杂的过程。程序员直接编写原生态 sql，可以严格控制 sql 执行性
能，灵活度高。
MyBatis 可以使用 XML 或注解来配置和映射原生信息，将 POJO 映射成数
据库中的记录，避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。
通过 xml 文件或注解的方式将要执行的各种 statement 配置起来，并通过
java 对象和 statement 中 sql 的动态参数进行映射生成最终执行的 sql 语句，最
后由 mybatis 框架执行 sql 并将结果映射为 java 对象并返回。（从执行 sql 到返
回 result 的过程）。
Mybatis的优点#
基于 SQL 语句编程，相当灵活，不会对应用程序或者数据库的现有设计造成任
何影响，SQL 写在 XML 里，解除 sql 与程序代码的耦合，便于统一管理；提供 XML
标签，支持编写动态 SQL 语句，并可重用。
与 JDBC 相比，减少了 50%以上的代码量，消除了 JDBC 大量冗余的代码，不
需要手动开关连接；
很好的与各种数据库兼容（因为 MyBatis 使用 JDBC 来连接数据库，所以只要
JDBC 支持的数据库 MyBatis 都支持）。
能够与 Spring 很好的集成；
提供映射标签，支持对象与数据库的 ORM 字段关系映射；提供对象关系映射
标签，支持对象关系组件维护。
Mybatis的缺点#
SQL 语句的编写工作量较大，尤其当字段多、关联表多时，对开发人员编写
SQL 语句的功底有一定要求。
SQL 语句依赖于数据库，导致数据库移植性差，不能随意更换数据库。
MyBatis 框架适用场合#
MyBatis 专注于 SQL 本身，是一个足够灵活的 DAO 层解决方案。
对性能的要求很高，或者需求变化较多的项目，如互联网项目，MyBatis 将是
不错的选择。
MyBatis 与 Hibernate 有哪些不同？#
Mybatis 和 hibernate 不同，它不完全是一个 ORM 框架，因为 MyBatis 需要
程序员自己编写 Sql 语句。
Mybatis 直接编写原生态 sql，可以严格控制 sql 执行性能，灵活度高，非常
适合对关系数据模型要求不高的软件开发，因为这类软件需求变化频繁，一但需
求变化要求迅速输出成果。但是灵活的前提是 mybatis 无法做到数据库无关性，
如果需要实现支持多种数据库的软件，则需要自定义多套 sql 映射文件，工作量大。
Hibernate 对象/关系映射能力强，数据库无关性好，对于关系模型要求高的
软件，如果用 hibernate 开发可以节省很多代码，提高效率。
#{}和${}的区别是什么？#
#{} 是预编译处理，${}是字符串替换。Mybatis 在处理#{}时，会将 sql 中的#{}替换为?号，调用 PreparedStatement 的
set 方法来赋值；
Mybatis 在处理时，就是把{}替换成变量的值。
使用#{}可以有效的防止 SQL 注入，提高系统安全性。

当实体类中的属性名和表中的字段名不一样 ，怎么办 ？#
第 1 种： 通过在查询的 sql 语句中定义字段名的别名，让字段名的别名和实体类
的属性名一致

Copy
<select id=”selectorder” parametertype=”int” resultetype=”
me.gacl.domain.order”>
select order_id id, order_no orderno ,order_price price fo``rm
orders where order_id=#{id};
</select>
第 2 种： 通过 <resultMap> 来映射字段名和实体类属性名的一一对应的关系。

Copy
<select id="getOrder" parameterType="int"
resultMap="orderresultmap">
select * from orders where order_id=#{id}
</select>
<resultMap type=”me.gacl.domain.order” id=”orderresultmap”>
<!–用 id 属性来映射主键字段–>
<id property=”id” column=”order_id”>
<!–用 result 属性来映射非主键字段，property 为实体类属性名，column
为数据表中的属性–>
<result property = “orderno” column =”order_no”/>
<result property=”price” column=”order_price” />
</reslutMap>
模糊查询 like 语句该怎么写?#
第 1 种：在 Java 代码中添加 sql 通配符。

Copy
string wildcardname = “%smi%”;
list<name> names = mapper.selectlike(wildcardname);
<select id=”selectlike”>
select * from foo where bar like #{value}
</select>
第 2 种：在 sql 语句中拼接通配符，会引起 sql 注入

Copy
string wildcardname = “smi”;
list<name> names = mapper.selectlike(wildcardname);
<select id=”selectlike”>
select * from foo where bar like "%"#{value}"%"
</select>
通常一个 Xml 映射文件，都会写一个 Dao 接口与之对应，请问，这个 Dao 接口的工作原理是什么？Dao 接口里的方法，参数不同时，方法能重载吗？#
Dao 接口即 Mapper 接口。接口的全限名，就是映射文件中的 namespace 的值；
接口的方法名，就是映射文件中 Mapper 的 Statement 的 id 值；接口方法内的
参数，就是传递给 sql 的参数。
Mapper 接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符
串作为 key 值，可唯一定位一个 MapperStatement。在 Mybatis 中，每一个
<select>、<insert>、<update>、<delete> 标签，都会被解析为一个
MapperStatement 对象。
举例： com.mybatis3.mappers.StudentDao.findStudentById ，可以唯
一找到 namespace 为 com.mybatis3.mappers.StudentDao 下面 id 为
findStudentById 的 MapperStatement。
Mapper 接口里的方法，是不能重载的，因为是使用 全限名+方法名 的保存和寻
找策略。Mapper 接口的工作原理是 JDK 动态代理，Mybatis 运行时会使用 JDK
动态代理为 Mapper 接口生成代理对象 proxy，代理对象会拦截接口方法，转而
执行 MapperStatement 所代表的 sql，然后将 sql 执行结果返回。
Mybatis 是如何进行分页的？分页插件的原理是什么？#
Mybatis 使用 RowBounds 对象进行分页，它是针对 ResultSet 结果集执行的内
存分页，而非物理分页。可以在 sql 内直接书写带有物理分页的参数来完成物理分
页功能，也可以使用分页插件来完成物理分页。

分页插件的基本原理是使用 Mybatis 提供的插件接口，实现自定义插件，在插件
的拦截方法内拦截待执行的 sql，然后重写 sql，根据 dialect 方言，添加对应的物
理分页语句和物理分页参数。

Mybatis是如何将sql执行结果封装为目标对象并返回的？都有哪些映射形式？#
第一种是使用 标签，逐一定义数据库列名和对象属性名之间的映
射关系。
第二种是使用 sql 列的别名功能，将列的别名书写为对象属性名。有了列名与属性名的映射关系后，Mybatis 通过反射创建对象，同时使用反射给
对象的属性逐一赋值并返回，那些找不到映射关系的属性，是无法完成赋值的。

如何执行批量插入?#
首先,创建一个简单的 insert 语句:

Copy
<insert id=”insertname”>
insert into names (name) values (#{value})
</insert>
然后在 java 代码中像下面这样执行批处理插入:

Copy
list < string > names = new arraylist();
names.add(“fred”);
names.add(“barney”);
names.add(“betty”);
names.add(“wilma”);
// 注意这里 executortype.batch
sqlsession sqlsession = sqlsessionfactory.opensession(executortype.batch);
try {
    namemapper mapper = sqlsession.getmapper(namemapper.class);
    for (string name: names) {
        mapper.insertname(name);
    }
    sqlsession.commit();
}catch (Exception e) {
    e.printStackTrace();
    sqlSession.rollback();
    throw e;
}finally {
    sqlsession.close();
}
如何获取自动生成的(主)键值?#
insert 方法总是返回一个 int 值 ，这个值代表的是插入的行数。
如果采用自增长策略，自动生成的键值在 insert 方法执行完后可以被设置到传入
的参数对象中。

Copy
<insert id=”insertname” usegeneratedkeys=”true” keyproperty=”
id”>
insert into names (name) values (#{name})
</insert>


name name = new name();
name.setname(“fred”);
int rows = mapper.insertname(name);
// 完成后,id 已经被设置到对象中
system.out.println(“rows inserted = ” + rows);
system.out.println(“generated key value = ” + name.getid());
在 mapper 中如何传递多个参数?#
DAO 层的函数

Copy
public UserselectUser(String name,String area);
对应的 xml,#{0}代表接收的是 dao 层中的第一个参数，#{1}代表 dao 层中第二
参数，更多参数一致往后加即可。
Copy
<select id="selectUser"resultMap="BaseResultMap">
select * fromuser_user_t whereuser_name = #{0}
anduser_area=#{1}
</select>
使用 @param 注解

Copy
public interface usermapper {
    user selectuser(@param(“username”) string username,
                    @param(“hashedpassword”) string hashedpassword);
}
Copy
<select id=”selectuser” resulttype=”user”>
select id, username, hashedpassword
from some_table
where username = #{username}
and hashedpassword = #{hashedpassword}
</select>
多个参数封装成 map

Copy
try {
//映射文件的命名空间.SQL 片段的 ID，就可以调用对应的映射文件中的SQL
//由于我们的参数超过了两个，而方法中只有一个 Object 参数收集，因此
我们使用 Map 集合来装载我们的参数
    Map < String, Object > map = new HashMap();
    map.put("start", start);
    map.put("end", end);
    return sqlSession.selectList("StudentID.pagination", map);
} catch (Exception e) {
    e.printStackTrace();
    sqlSession.rollback();
    throw e;
} finally {
    MybatisUtil.closeSqlSession();
}
Mybatis 动态 sql 有什么用？执行原理？有哪些动态 sql？#
Mybatis 动态 sql 可以在 Xml 映射文件内，以标签的形式编写动态 sql，执行原理
是根据表达式的值 完成逻辑判断并动态拼接 sql 的功能。
Mybatis 提供了 9 种动态 sql 标签：
trim | where | set | foreach | if | choose| when | otherwise | bind 。

Xml 映射文件中，除了常见的 select|insert|updae|delete标签之外，还有哪些标签？#
<resultMap>、<parameterMap>、<sql>、<include>、<selectKey>，加上动态 sql 的 9 个标签，其中 <sql> 为 sql 片段标签，通过
<include> 标签引入 sql 片段， <selectKey> 为不支持自增的主键生成策略标
签。
Mybatis 的 Xml 映射文件中，不同的 Xml 映射文件，id 是否可以重复？#
不同的 Xml 映射文件，如果配置了 namespace，那么 id 可以重复；如果没有配
置 namespace，那么 id 不能重复；
原因就是 namespace+id 是作为Map <String, MapperStatement>的 key
使用的，如果没有 namespace，就剩下 id，那么，id 重复会导致数据互相覆盖。
有了 namespace，自然 id 就可以重复，namespace 不同，namespace+id 自然
也就不同

为什么说 Mybatis 是半自动 ORM 映射工具？它与全自动的区别在哪里?#
Hibernate 属于全自动 ORM 映射工具，使用 Hibernate 查询关联对象或者关联
集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。而 Mybatis
在查询关联对象或关联集合对象时，需要手动编写 sql 来完成，所以，称之为半自
动 ORM 映射工具

一对一、一对多的关联查询#
Copy
<mapper namespace="com.lcb.mapping.userMapper">
<!--association 一对一关联查询 -->
<select id="getClass" parameterType="int" resultMap="ClassesResultMap">
    select * from class c,teacher t where c.teacher_id=t.t_id and
    c.c_id=#{id}
</select>
<resultMap type="com.lcb.user.Classes" id="ClassesResultMap">
<!-- 实体类的字段名和数据表的字段名映射 -->
<id property="id" column="c_id"/>
    <result property="name" column="c_name"/>
    <association property="teacher" javaType="com.lcb.user.Teacher">
    <id property="id" column="t_id"/>
    <result property="name" column="t_name"/>
    </association>
    </resultMap>
<!--collection 一对多关联查询 -->
<select id="getClass2" parameterType="int"
resultMap="ClassesResultMap2">
select * from class c,teacher t,student s where c.teacher_id=t.t_id
and c.c_id=s.class_id and c.c_id=#{id}
</select>
<resultMap type="com.lcb.user.Classes" id="ClassesResultMap2">
<id property="id" column="c_id"/>
<result property="name" column="c_name"/>
<association property="teacher"
javaType="com.lcb.user.Teacher">
<id property="id" column="t_id"/>
<result property="name" column="t_name"/>
</association>
<collection property="student"
ofType="com.lcb.user.Student">
<id property="id" column="s_id"/>
<result property="name" column="s_name"/>
</collection>
</resultMap>
</mapper>
MyBatis 实现一对一有几种方式?具体怎么操作的？#
有联合查询和嵌套查询,联合查询是几个表联合查询,只查询一次, 通过在
resultMap 里面配置 association 节点配置一对一的类就可以完成；
嵌套查询是先查一个表，根据这个表里面的结果的 外键 id，去再另外一个表里面
查询数据,也是通过 association 配置，但另外一个表的查询通过 select 属性配置。

MyBatis 实现一对多有几种方式,怎么操作的？#
有联合查询和嵌套查询。联合查询是几个表联合查询,只查询一次,通过在
resultMap 里面的 collection 节点配置一对多的类就可以完成；嵌套查询是先查
一个表,根据这个表里面的 结果的外键 id,去再另外一个表里面查询数据,也是通过
配置 collection,但另外一个表的查询通过 select 节点配置。

Mybatis 是否支持延迟加载？如果支持，它的实现原理是什么？#
Mybatis 仅支持 association 关联对象和 collection 关联集合对象的延迟加
载，association 指的就是一对一，collection 指的就是一对多查询。在 Mybatis
配置文件中，可以配置是否启用延迟加载 lazyLoadingEnabled=true|false。
它的原理是，使用 CGLIB 创建目标对象的代理对象，当调用目标方法时，进入拦
截器方法，比如调用 a.getB().getName()，拦截器 invoke()方法发现 a.getB()是
null 值，那么就会单独发送事先保存好的查询关联 B 对象的 sql，把 B 查询上来，
然后调用 a.setB(b)，于是 a 的对象 b 属性就有值了，接着完成 a.getB().getName()
方法的调用。这就是延迟加载的基本原理。
当然了，不光是 Mybatis，几乎所有的包括 Hibernate，支持延迟加载的原理都
是一样的。

Mybatis 的一级、二级缓存:#
1）一级缓存: 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为
Session，当 Session flush 或 close 之后，该 Session 中的所有 Cache 就
将清空，默认打开一级缓存。
2）二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap
存储，不同在于其存储作用域为 Mapper(Namespace)，并且可自定义存储源，
如 Ehcache。默认不打开二级缓存，要开启二级缓存，使用二级缓存属性类需要
实现 Serializable 序列化接口(可用来保存对象的状态),可在它的映射文件中配置
<cache/>
3）对于缓存数据更新机制，当某一个作用域(一级缓存 Session/二级缓存
Namespaces)的进行了 C/U/D 操作后，默认该作用域下所有 select 中的缓存将
被 clear。

什么是 MyBatis 的接口绑定？有哪些实现方式？#
接口绑定，就是在 MyBatis 中任意定义接口,然后把接口里面的方法和 SQL 语句绑
定, 我们直接调用接口方法就可以,这样比起原来了 SqlSession 提供的方法我们可
以有更加灵活的选择和设置。
接口绑定有两种实现方式,一种是通过注解绑定，就是在接口的方法上面加上
@Select、@Update 等注解，里面包含 Sql 语句来绑定；另外一种就是通过 xml
里面写 SQL 来绑定, 在这种情况下,要指定 xml 映射文件里面的 namespace 必须
为接口的全路径名。当 Sql 语句比较简单时候,用注解绑定, 当 SQL 语句比较复杂
时候,用 xml 绑定,一般用 xml 绑定的比较多。

使用 MyBatis 的 mapper 接口调用时有哪些要求？#
1、Mapper 接口方法名和 mapper.xml 中定义的每个 sql 的 id 相同；
2、Mapper 接口方法的输入参数类型和 mapper.xml 中定义的每个 sql 的
parameterType 的类型相同；
3、Mapper 接口方法的输出参数类型和 mapper.xml 中定义的每个 sql 的
resultType 的类型相同；
4、Mapper.xml 文件中的 namespace 即是 mapper 接口的类路径。

Mapper 编写有哪几种方式？#
第一种：接口实现类继承 SqlSessionDaoSupport：使用此种方法需要编写
mapper 接口，mapper 接口实现类、mapper.xml 文件。
1、在 sqlMapConfig.xml 中配置 mapper.xml 的位置

Copy
<mappers>
    <mapper resource="mapper.xml 文件的地址" />
    <mapper resource="mapper.xml 文件的地址" />
</mappers>
1、定义 mapper 接口
3、实现类集成 SqlSessionDaoSupport
mapper 方法中可以 this.getSqlSession()进行数据增删改查。
4、spring 配置

Copy
<bean id=" " class="mapper 接口的实现">
<property name="sqlSessionFactory"
ref="sqlSessionFactory"></property>
</bean>
使用 org.mybatis.spring.mapper.MapperFactoryBean ：
在 sqlMapConfig.xml 中配置 mapper.xml 的位置，如果 mapper.xml 和
mappre 接口的名称相同且在同一个目录，这里可以不用配置

Copy
<mappers>
<mapper resource="mapper.xml 文件的地址" />
<mapper resource="mapper.xml 文件的地址" />
</mappers>
定义 mapper 接口：
1、mapper.xml 中的 namespace 为 mapper 接口的地址
2、mapper 接口中的方法名和 mapper.xml 中的定义的 statement 的 id 保持一
致
3、Spring 中定义

Copy
<bean id="" class="org.mybatis.spring.mapper.MapperFactoryBean">
<property name="mapperInterface" value="mapper 接口地址" />
<property name="sqlSessionFactory" ref="sqlSessionFactory" />
</bean>
使用 mapper 扫描器：

1、mapper.xml 文件编写：
mapper.xml 中的 namespace 为 mapper 接口的地址；
mapper 接口中的方法名和 mapper.xml 中的定义的 statement 的 id 保持一致；
如果将 mapper.xml 和 mapper接口的名称保持一致则不用在 sqlMapConfig.xml
中进行配置。
2、定义 mapper 接口：
注意 mapper.xml 的文件名和 mapper 的接口名称保持一致，且放在同一个目录
3、配置 mapper 扫描器：

Copy
<bean class="org.mybatis.spring.mapper.MapperScannerConfigurer">
<property name="basePackage" value="mapper 接口包地址
"></property>
<property name="sqlSessionFactoryBeanName"
value="sqlSessionFactory"/>
</bean>
4、使用扫描器后从 spring 容器中获取 mapper 的实现对象。

简述 Mybatis 的插件运行原理，以及如何编写一个插件。#
Mybatis 仅可以编写针对 ParameterHandler、ResultSetHandler、
StatementHandler、Executor 这 4 种接口的插件，Mybatis 使用 JDK 的动态代
理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这 4 种
接口对象的方法时，就会进入拦截方法，具体就是 InvocationHandler 的 invoke()
方法，当然，只会拦截那些你指定需要拦截的方法。
编写插件：实现 Mybatis 的 Interceptor 接口并复写 intercept()方法，然后在给
插件编写注解，指定要拦截哪一个接口的哪些方法即可，记住，别忘了在配置文
件中配置你编写的插件。
Double
Double原理
dubbo作为当前国内热门的RPC框架，其基本原理、配置调优等是面试中会经常问到的，了解这些或者知道这些配置项的存在对工作也会事半功倍，遇到类似的问题可以不再去问那个广告满天飞的某度了。

1. Dubbo简介

Dubbo |db|是一个由阿里巴巴开源的、分布式的RPC(Remote Procedure Call Protocol-远程过程调用)和微服务框架，现为Apache顶级项目。

Dubbo提供了三个关键功能：基于接口的远程调用，容错与负载均衡，服务自动注册与发现。

Dubbo使得调用远程服务就像调用本地java服务一样简单。

下图为Dubbo的结构图：



关于Dubbo的使用可以参考官方文档http://dubbo.apache.org ，本文不作赘述。

2. Dubbo服务暴露与消费过程

先来看下面问题：

1) Dubbo服务提供者发布服务的流程

2) Dubbo服务消费者消费服务的流程

3) 什么是本地暴露和远程暴露,他们的区别

Dubbo服务提供者发布服务过程：

先来看dubbo的启动日志：



图中从上到下框起来的日志分别是：

1) 暴露服务到本地

2) 暴露服务到远程

3) 启动netty服务

4) 连接zookeeper

5) 注册服务到zookeeper

6) 监听zookeeper中消费服务

关于这个过程的实现细节可以参考Dubbo官方文档->实现细节->远程调用细节->服务提供者暴露一个服务的详细过程。截图如下：


首先ServiceConfig类拿到对外提供服务的实际类ref（如HelloWorldImpl），然后通过ProxyFactory的getInvoker()方法使用ref生成一个AbstractProxyInvoker实例，到这一步就完成具体服务到Invoker的转化。接下来就是Invoker转换到Exporter的过程。

Dubbo处理服务暴露的关键就在Invoker转换到Exporter的过程(如上图中的红色部分)，下面我们以Dubbo和RMI这两种典型协议的实现来进行说明：

Dubbo的实现

Dubbo协议的Invoker转为Exporter发生在DubboProtocol类的export方法，它主要是打开socket侦听服务，并接收客户端发来的各种请求，通讯细节由Dubbo自己实现。

RMI的实现

RMI协议的Invoker转为Exporter发生在RmiProtocol类的export方法，

它通过Spring或Dubbo或JDK来实现RMI服务，通讯细节这一块由JDK底层来实现，这就省了不少工作量。

Dubbo服务消费者消费服务过程：

关于这个过程的实现细节可以参考Dubbo官方文档->实现细节->远程调用细节->服务消费者消费一个服务的详细过程。截图如下：



下面来看本地暴露于远程暴露的区别：

本地暴露是暴露在本机JVM中,调用本地服务不需要网络通信.

远程暴露是将ip,端口等信息暴露给远程客户端,调用远程服务时需要网络通信.

3. Dubbo相关协议

Dubbo 允许配置多协议，在不同服务上支持不同协议或者同一服务上同时支持多种协议。

不同服务在性能上适用不同协议进行传输，比如大数据用短连接协议，小数据大并发用长连接协议。

Dubbo支持的协议主要有：

dubbo：

Dubbo 缺省协议是dubbo协议，采用单一长连接和 NIO 异步通讯，适合于小数据量大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况。

反之，Dubbo 缺省协议不适合传送大数据量的服务，比如传文件，传视频等，除非请求量很低。rmi:

RMI协议采用阻塞式(同步)短连接和 JDK 标准序列化方式。适用范围：传入传出参数数据包大小混合，消费者与提供者个数差不多，可传文件。

hessian:

Hessian底层采用Http通讯(同步)，采用Servlet暴露服务。适用于传入传出参数数据包较大，提供者比消费者个数多，提供者压力较大，可传文件。

dubbo还支持的其他协议有：http, webservice, thrift, memcached, redis

4. Dubbo相关配置

先看下面问题：

Dubbo主要的配置项有哪些，作用是什么？

如果Dubbo的服务端未启动，消费端能起来吗？

Dubbo主要配置项：

配置应用信息：

配置注册中心相关信息：

配置服务协议：

配置所有暴露服务缺省值：

配置暴露服务：

配置所有引用服务缺省值：

配置引用服务：

备注:其中reference的check默认=true，启动时会检查引用的服务是否已存在，不存在时报错

注解配置：

com.alibaba.dubbo.config.annotation.Service 配置暴露服务

com.alibaba.dubbo.config.annotation.Reference配置引用服务

好了，如果觉得有收获记得关注哦。
Dubbo RPC原理解读
Dubbo的RPC流程，包括：服务引用、服务暴露、服务调用三个部分，

在Dubbo中有这么两个概念是最核心的：

Invoker：是 Dubbo 的核心模型，其它模型都向它靠扰，或转换成它，它代表一个可执行体，可向它发起 invoke 调用。
Protocol：是 Invoker 暴露和引用的主功能入口，它负责 Invoker 的生命周期管理。
支撑起Dubbo RPC的就是如下这一段话：

当用户调用 refer() 所返回的 Invoker 对象的 invoke() 方法时，Protocol 需相应执行同 URL 远端 export() 传入的 Invoker 对象的 invoke() 方法。
画了一幅图来辅助理解这句话：

服务解析
所谓服务解析，就是在 Dubbo 初始化时进行，把 <xx style="box-sizing: border-box; outline-style: none;">的标签解析并注册为 Spring Bean 的过程。</xx>

基于 Spring 的 Schema 扩展机制，这里不再展开，可以参考这篇文章：https://www.jianshu.com/p/8639e5e9fba6。

基于 dubbo.jar 内的 META-INF/spring.handlers 配置：

http\://dubbo.apache.org/schema/dubbo=org.apache.dubbo.config.spring.schema.DubboNamespaceHandler
http\://code.alibabatech.com/schema/dubbo=org.apache.dubbo.config.spring.schema.DubboNamespaceHandler
Spring 在遇到 dubbo 名称空间时，会回调 DubboNamespaceHandler：

/**
* DubboNamespaceHandler
*
* @export
*/

public class DubboNamespaceHandler extends NamespaceHandlerSupport {

static {
       Version.checkDuplicate(DubboNamespaceHandler.class);
}

@Override
public void init() {
      registerBeanDefinitionParser("application", new DubboBeanDefinitionParser(ApplicationConfig.class, true));
      registerBeanDefinitionParser("module", new DubboBeanDefinitionParser(ModuleConfig.class, true));
      registerBeanDefinitionParser("registry", new DubboBeanDefinitionParser(RegistryConfig.class, true));
      registerBeanDefinitionParser("monitor", new DubboBeanDefinitionParser(MonitorConfig.class, true));
      registerBeanDefinitionParser("provider", new DubboBeanDefinitionParser(ProviderConfig.class, true));
      registerBeanDefinitionParser("consumer", new DubboBeanDefinitionParser(ConsumerConfig.class, true));
      registerBeanDefinitionParser("protocol", new DubboBeanDefinitionParser(ProtocolConfig.class, true));
      registerBeanDefinitionParser("service", new DubboBeanDefinitionParser(ServiceBean.class, true));
      registerBeanDefinitionParser("reference", new DubboBeanDefinitionParser(ReferenceBean.class, false));
      registerBeanDefinitionParser("annotation", new AnnotationBeanDefinitionParser());
}

}
所有 dubbo namespace下的的标签，都统一用 DubboBeanDefinitionParser 进行解析，基于一对一属性映射，将XML 标签解析为对应的 Bean 对象，再注册到容器里。

和RPC核心流程密切相关的标签：

<reference style="box-sizing: border-box; outline-style: none;">解析为 ReferenceBean</reference>
<service style="box-sizing: border-box; outline-style: none;">解析为 ServiceBean</service>
服务引用
所谓服务引用，就是服务的消费端从Protocol中拿到正确的Invoker的过程，对应核心契约图的左半部分，按照契约的说法，接下来消费端对这个Invoker发起调用，就会达到调用远端同URL暴露出来的Invoker的效果了。


关键代码路径：

ReferenceBean#getObject：ReferenceBean实现了Spring的FactoryBean接口，因此IOC容器想要获取ReferenceBean的实例时会去调用getObject方法来获取。
ReferenceConfig#get：第一次调用时会触发初始化逻辑，之后直接返回创建好的实例引用。
ReferenceConfig#createProxy




这里的两行代码分别解析：

Dubbo基于扩展点的自适应机制（http://dubbo.apache.org/zh-cn/docs/dev/SPI.html），会自动识别URL中的协议类型，并调用合适的Protocol实现类，在这里我们默认用的是Dubbo协议，因此会调用DubboProtocol进行服务引用：


ProxyFactory同样是扩展点，这里默认使用了JavassistProxyFactory的实现：


可以看到这里创建的代理，把对接口方法的调用交给了 InvokerInvocationHandler这个类去处理。这里先不展开，可以留待分析服务调用时再看。

服务暴露
所谓服务暴露，就是服务提供方把代理了真正实现类的Invoker暴露给Protocol的过程，对应核心契约的右半部分。



关键代码路径：

ServiceBean#onApplicationEvent
ServiceConfig#export
……
ServiceConfig#doExportUrlsFor1Protocol



展开看高亮出的两段逻辑：

同服务引用一样，这里的ProxyFactory依旧是扩展点，具体实现如下，可以看到这里创建一个AbstractProxyInvoker，当该Invoker的doInvoke方法被调用时，就会调用真正实现类 ref 里的方法：


2.同样，Protocol也是扩展点，默认采用Dubbo实现：


export方法干了两件事：

将上一步拿到的Invoker export出去，然后创建 DubboExporter实例并存储在 exporterMap 里，Exporter并没有什么玄机，就是一个存储了Invoker实例及其他各种信息的容器，用于之后获取Invoker用。
打开服务端口，并注册ExchangeHandler，用于后续响应客户端网络请求，响应逻辑这里先不展开。
服务调用分两部分来分析：客户端发起调用、服务端响应调用。
客户端发起调用

按照之前服务引用代码的分析，客户端对拿到的Bean发起调用时，会被 JavassistProxyFactory#getProxy 创建的代理接管，把请求交给InvokerInvocationHandler处理：


按照之前服务引用代码的分析，这里调用的Invoker是调用refprotocol#refer拿到的，是一个DubboInvoker，因此会进入下面的逻辑，其实就是发起一次网络请求，拿到返回值：

这样，就通过代理机制，就把客户端发起的本地请求转换为了对服务端的网络请求。

服务端响应调用
上文里服务提供端暴露服务时，在openServer逻辑里，注册了一个ExchangeHandler用于响应客户端的网络请求


getInvoker方法是通过exporterMap获取之前暴露的 Invoker 实例，然后调用其invoke方法。

由前文服务暴露部分（JavassistProxyFactory#getInvoker ）里的逻辑可知：当DubboInvoker的doInvoke方法被调用时，就会调用真正实现类 ref 里的方法，这样就触达到了服务的提供者。

结语
以上就是Dubbo RPC的核心流程逻辑了，这次研究时有很多逻辑被我当做了黑盒来处理，比如扩展点加载、负载均衡、网络请求、线程模型等……实际上一次RPC过程远比上述描述的复杂，按照官网的说明，RPC过程的流程如下所示：


什么时候真正吃透了这张图，才敢说自己懂了Dubbo RPC吧。
Dubbo 序列化
一、Dubbo适合高并发小数据的互联网场景，

二、序列化：将一个对象变成一个二进制流就是序列化。

反序列化：将二进制流转换成对象。

三、RPC的封装过程

1、消费端调用服务；

2、消费方接收到调用后， 将方法参数等封装成能够进行网络传输的消息体；

3、消费方找到服务地址，将消息发送到服务端；

4、服务提供方接收到消息后进行解码（解码就是所谓的序列化）；

5、服务提供方根据解码的结果调用本地服务；

6、本地服务执行、并将执行后的结果打包成消息体发送给消费方；

7、消费方接收到消息解码、拿到结果。

四、RPC通过jdk的动态代理的方式来实现调用远程服务的

首先消费方从代理类中获取服务提供的接口，当消费方调用服务的时候回执行invoke方法，实现远程调用

五、RPC序列化的优点

1、通用性（支持复杂数据结构，如map）

2、性能（序列化效率和节约内存，节省宽带）

3、可扩展性（由于业务发展的变化快）

六、dubbo的序列化方案

1、RPC默认的序列化方式是使用的阿里修改或的hession2,，而不是原生态的hession2（非Java，夸语言的序列化方案） -----------效率2

2、Java原生态的序列化方式（性能差） -----------效率4

3、json序列化，非二进制流的序列化方式，性能没有二进制流的效率高 -----------效率3

4、dubbo序列化，阿里开发的并不成熟的高效的Java序列化，不建议在生产环境下使用 -----------效率1

最近几年，各种新的高效序列化方式层出不穷，不断创建序列化性能上限，经典的有，kryo，FST等。FST不成熟

什么是RPC协议？
RPC是一种远程过程调用的协议，使用这种协议向另一台计算机上的程序请求服务，不需要了解底层网络技术的协议。

在 RPC 中，发出请求的程序是客户程序，而提供服务的程序是服务器。

HTTP是一种超文本传输协议。是WWW浏览器和WWW服务器之间的应用层通讯协议。
TCP/IP协议与Http协议的区别
TPC/IP协议是传输层协议，主要解决数据如何在网络中传输，而HTTP是应用层协议，主要解决如何包装数据。
RPC协议与HTTP协议的区别？
1、RPC是一种API，HTTP是一种无状态的网络协议。RPC可以基于HTTP协议实现，也可以直接在TCP协议上实现。

2、RPC主要是用在大型网站里面，因为大型网站里面系统繁多，业务线复杂，而且效率优势非常重要的一块，这个时候RPC的优势就比较明显了。

HTTP主要是用在中小型企业里面，业务线没那么繁多的情况下。

3、HTTP开发方便简单、直接。开发一个完善的RPC框架难度比较大。

4、HTTP发明的初衷是为了传送超文本的资源，协议设计的比较复杂，参数传递的方式效率也不高。开源的RPC框架针对远程调用协议上的效率会比HTTP快很多。

5、HTTP需要事先通知，修改Nginx/HAProxy配置。RPC能做到自动通知，不影响上游。

6、HTTP大部分是通过Json来实现的，字节大小和序列化耗时都比Thrift要更消耗性能。RPC，可以基于Thrift实现高效的二进制传输。
Dubbo协议
Dubbo支持的协议：
Dubbo支持dubbo、rmi、hessian、http、webservice、thrift、redis等多种协议，但是Dubbo官网是推荐我们使用Dubbo协议的。下面我们就针对Dubbo的每种协议详解讲解，以便我们在实际应用中能够正确取舍。

dubbo协议
缺省协议，使用基于mina1.1.7+hessian3.2.1的tbremoting交互。
连接个数：单连接
连接方式：长连接
传输协议：TCP
传输方式：NIO异步传输
序列化：Hessian二进制序列化
适用范围：传入传出参数数据包较小（建议小于100K），消费者比提供者个数多，单一消费者无法压满提供者，尽量不要用dubbo协议传输大文件或超大字符串。
适用场景：常规远程服务方法调用

1、dubbo默认采用dubbo协议，dubbo协议采用单一长连接和NIO异步通讯，适合于小数据量大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况
2、他不适合传送大数据量的服务，比如传文件，传视频等，除非请求量很低。
配置如下：

<dubbo:protocol name="dubbo" port="20880" />
<!-- Set default protocol: -->
<dubbo:provider protocol="dubbo" />
<~-- Set service protocol -->
<dubbo:service protocol="dubbo" />
<!-- Multi port -->
<dubbo:protocol id="dubbo1" name="dubbo" port="20880" />
<dubbo:protocol id="dubbo2" name="dubbo" port="20881" />.
<!-- Dubbo protocol options: -->
<dubbo:protocol name=“dubbo” port=“9090” server=“netty” client=“netty” codec=“dubbo” 
serialization=“hessian2” charset=“UTF-8” threadpool=“fixed” threads=“100” queues=“0” iothreads=“9” 
buffer=“8192” accepts=“1000” payload=“8388608” />
1
2
3
4
5
6
7
8
9
10
11
12
3、Dubbo协议缺省每服务每提供者每消费者使用单一长连接，如果数据量较大，可以使用多个连接。

<dubbo:protocol name="dubbo" connections="2" />
1
<dubbo:service connections=”0”>或<dubbo:reference connections=”0”>表示该服务使用JVM共享长连接。(缺省)
<dubbo:service connections=”1”>或<dubbo:reference connections=”1”>表示该服务使用独立长连接。
<dubbo:service connections=”2”>或<dubbo:reference connections=”2”>表示该服务使用独立两条长连接。
4、为防止被大量连接撑挂，可在服务提供方限制大接收连接数，以实现服务提供方自我保护

<dubbo:protocol name="dubbo" accepts="1000" />
1
dubbo协议为什么要消费者比提供者个数多？
因dubbo协议采用单一长连接，假设网络为千兆网卡(1024Mbit=128MByte)，根据测试经验数据每条连接最多只能压满7MByte(不同的环境可能不一样，供参考)，理论上1个服务提供者需要20个服务消费者才能压满网卡
dubbo协议为什么不能传大包？
因dubbo协议采用单一长连接，如果每次请求的数据包大小为500KByte，假设网络为千兆网卡(1024Mbit=128MByte)，每条连接最大7MByte(不同的环境可能不一样，供参考)，单个服务提供者的TPS(每秒处理事务数)最大为：128MByte / 500KByte = 262。单个消费者调用单个服务提供者的TPS(每秒处理事务数)最大为：7MByte / 500KByte = 14。如果能接受，可以考虑使用，否则网络将成为瓶颈。
为什么采用异步单一长连接？
因为服务的现状大都是服务提供者少，通常只有几台机器，而服务的消费者多，可能整个网站都在访问该服务，比如Morgan的提供者只有6台提供者，却有上百台消费者，每天有1.5亿次调用，如果采用常规的hessian服务，服务提供者很容易就被压跨，通过单一连接，保证单一消费者不会压死提供者，长连接，减少连接握手验证等，并使用异步IO，复用线程池，防止C10K问题。
接口增加方法，对客户端无影响，如果该方法不是客户端需要的，客户端不需要重新部署；
输入参数和结果集中增加属性，对客户端无影响，如果客户端并不需要新属性，不用重新
部署；
输入参数和结果集属性名变化，对客户端序列化无影响，但是如果客户端不重新部署，不管输入还是输出，属性名变化的属性值是获取不到的。
总结：服务器端和客户端对领域对象并不需要完全一致，而是按照最大匹配原则。
如果不是集成Spring，单独配置如下

dubbo.service.protocol=dubbo
1
rmi协议
Java标准的远程调用协议。
连接个数：多连接
连接方式：短连接
传输协议：TCP
传输方式：同步传输
序列化：Java标准二进制序列化
适用范围：传入传出参数数据包大小混合，消费者与提供者个数差不多，可传文件。
适用场景：常规远程服务方法调用，与原生RMI服务互操作

1、RMI协议采用JDK标准的java.rmi.*实现，采用阻塞式短连接和JDK标准序列化方式
注：
如果正在使用RMI提供服务给外部访问（公司内网环境应该不会有攻击风险），同时应用里依赖了老的common-collections包（dubbo不会依赖这个包，请排查自己的应用有没有使用）的情况下，存在反序列化安全风险。
请检查应用：
将commons-collections3 请升级到3.2.2版本：
https://commons.apache.org/proper/commons-collections/release_3_2_2.html
将commons-collections4 请升级到4.1版本：https://commons.apache.org/proper/commons-collections/release_4_1.html
新版本的commons-collections解决了该问题
如果服务接口继承了java.rmi.Remote接口，可以和原生RMI互操作，即：
提供者用Dubbo的RMI协议暴露服务，消费者直接用标准RMI接口调用，或者提供方用标准RMI暴露服务，消费方用Dubbo的RMI协议调用。如果服务接口没有继承java.rmi.Remote接口，缺省Dubbo将自动生成一个com.xxx.XxxService$Remote的接口，并继承java.rmi.Remote接口，并以此接口暴露服务，但如果设置了<dubbo:protocol name="rmi" codec="spring" />，将不生成$Remote接口，而使用Spring的RmiInvocationHandler接口暴露服务，和Spring兼容。

<!-- Define rmi protocol -->
<dubbo:protocol name="rmi" port="1099" />.
<!-- Set default protocol: -->
<dubbo:provider protocol="rmi" />
<!-- Set service protocol: -->
<dubbo:service protocol="rmi" />
<!-- Multi port -->
<dubbo:protocol id="rmi1" name="rmi" port="1099" />
<dubbo:protocol id="rmi2" name="rmi" port="2099" />
<dubbo:service protocol="rmi1" />
<!-- Spring compatible: -->
<dubbo:protocol name="rmi" codec="spring" />
1
2
3
4
5
6
7
8
9
10
11
12
hessian协议
基于Hessian的远程调用协议。
连接个数：多连接
连接方式：短连接
传输协议：HTTP
传输方式：同步传输
序列化：表单序列化
适用范围：传入传出参数数据包大小混合，提供者比消费者个数多，可用浏览器查看，可用表单或URL传入参数，暂不支持传文件。
适用场景：需同时给应用程序和浏览器JS使用的服务。

1、Hessian协议用于集成Hessian的服务，Hessian底层采用Http通讯，采用Servlet暴露服务，Dubbo缺省内嵌Jetty作为服务器实现。
2、Hessian是Caucho开源的一个RPC框架：http://hessian.caucho.com，其通讯效率高于WebService和Java自带的序列化。
依赖

<dependency>
    <groupId>com.caucho</groupId>
    <artifactId>hessian</artifactId>
    <version>4.0.7</version>
</dependency>
1
2
3
4
5
可以和原生Hessian服务互操作，即：
提供者用Dubbo的Hessian协议暴露服务，消费者直接用标准Hessian接口调用，或者提供方用标准Hessian暴露服务，消费方用Dubbo的Hessian协议调用。
约束
1、参数及返回值需实现Serializable接口
2、参数及返回值不能自定义实现List, Map, Number, Date, Calendar等接口，只能用JDK自带的实现，因为hessian会做特殊处理，自定义实现类中的属性值都会丢失。

<!-- Define hessian protocol: -->
<dubbo:protocol name="hessian" port="8080" server="jetty" />
<!-- Set default protocol: -->
<dubbo:provider protocol="hessian" />
<!-- Set service protocol: -->
<dubbo:service protocol="hessian" />
<!-- Multi port: -->
<dubbo:protocol id="hessian1" name="hessian" port="8080" />
<dubbo:protocol id="hessian2" name="hessian" port="8081" />
<!-- Directly provider: -->
<dubbo:reference id="helloService" interface="HelloWorld" url="hessian://10.20.153.10:8080/helloWorld" />
<!-- Jetty Server -->
<dubbo:protocol ... server="jetty" />
<!-- Servlet Bridge Server -->
<dubbo:protocol ... server="servlet" />
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
web.xml配置

<servlet>
         <servlet-name>dubbo</servlet-name>
         <servlet-class>com.alibaba.dubbo.remoting.http.servlet.DispatcherServlet</servlet-class>
         <load-on-startup>1</load-on-startup>
</servlet>
<servlet-mapping>
         <servlet-name>dubbo</servlet-name>
         <url-pattern>/*</url-pattern>
</servlet-mapping>
1
2
3
4
5
6
7
8
9
注意：如果使用servlet派发请求
协议的端口<dubbo:protocol port="8080" />必须与servlet容器的端口相同，
协议的上下文路径<dubbo:protocol contextpath="foo" />必须与servlet应用的上下文路径相同。

http协议
基于http表单的远程调用协议。参见：[HTTP协议使用说明]
连接个数：多连接
连接方式：短连接
传输协议：HTTP
传输方式：同步传输
序列化：表单序列化
适用范围：传入传出参数数据包大小混合，提供者比消费者个数多，可用浏览器查看，可用表单或URL传入参数，暂不支持传文件。
适用场景：需同时给应用程序和浏览器JS使用的服务。

1、采用Spring的HttpInvoker实现
配置

<dubbo:protocol name="http" port="8080" />
<!-- Jetty Server -->
<dubbo:protocol ... server="jetty" />
<!-- Servlet Bridge Server -->
<dubbo:protocol ... server="servlet" />
1
2
3
4
5
web.xml配置

<servlet>
         <servlet-name>dubbo</servlet-name>
         <servlet-class>com.alibaba.dubbo.remoting.http.servlet.DispatcherServlet</servlet-class>
         <load-on-startup>1</load-on-startup>
</servlet>
<servlet-mapping>
         <servlet-name>dubbo</servlet-name>
         <url-pattern>/*</url-pattern>
</servlet-mapping>
1
2
3
4
5
6
7
8
9
注意:如果使用servlet派发请求
协议的端口<dubbo:protocol port="8080" />必须与servlet容器的端口相同，
协议的上下文路径<dubbo:protocol contextpath="foo" />必须与servlet应用的上下文路径相同。

webservice协议
基于WebService的远程调用协议。
连接个数：多连接
连接方式：短连接
传输协议：HTTP
传输方式：同步传输
序列化：SOAP文本序列化
适用场景：系统集成，跨语言调用

1、基于CXF的frontend-simple和transports-http实现。
2、CXF是Apache开源的一个RPC框架：http://cxf.apache.org，由Xfire和Celtix合并而来 。
依赖

<dependency>
    <groupId>org.apache.cxf</groupId>
    <artifactId>cxf-rt-frontend-simple</artifactId>
    <version>2.6.1</version>
</dependency>
<dependency>
    <groupId>org.apache.cxf</groupId>
    <artifactId>cxf-rt-transports-http</artifactId>
    <version>2.6.1</version>
</dependency>
1
2
3
4
5
6
7
8
9
10
可以和原生WebService服务互操作，即：
提供者用Dubbo的WebService协议暴露服务，消费者直接用标准WebService接口调用，或者提供方用标准WebService暴露服务，消费方用Dubbo的WebService协议调用。
约束：
参数及返回值需实现Serializable接口
参数尽量使用基本类型和POJO。

<!-- Define webservice protocol -->
<dubbo:protocol name="webservice" port="8080" server="jetty" />
<!-- Set default protocol -->
<dubbo:provider protocol="webservice" />
<!-- Set service protocol -->
<dubbo:service protocol="webservice" />
<!-- Multi port -->
<dubbo:protocol id="webservice1" name="webservice" port="8080" />
<dubbo:protocol id="webservice2" name="webservice" port="8081" />
<!-- Directly provider -->
<dubbo:reference id="helloService" interface="HelloWorld" url="webservice://10.20.153.10:8080/com.foo.HelloWorld" />
<!-- WSDL -->
http://10.20.153.10:8080/com.foo.HelloWorld?wsdl
<!-- Jetty Server: (default) -->
<dubbo:protocol ... server="jetty" />
<!-- Servlet Bridge Server: (recommend) -->
<dubbo:protocol ... server="servlet" />
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
web.xml

<servlet>
         <servlet-name>dubbo</servlet-name>
         <servlet-class>com.alibaba.dubbo.remoting.http.servlet.DispatcherServlet</servlet-class>
         <load-on-startup>1</load-on-startup>
</servlet>
<servlet-mapping>
         <servlet-name>dubbo</servlet-name>
         <url-pattern>/*</url-pattern>
</servlet-mapping>
1
2
3
4
5
6
7
8
9
注意:如果使用servlet派发请求：
协议的端口<dubbo:protocol port="8080" />必须与servlet容器的端口相同，
协议的上下文路径<dubbo:protocol contextpath="foo" />必须与servlet应用的上下文路径相同。

thrift协议
当前 dubbo 支持的 thrift 协议是对 thrift 原生协议的扩展，在原生协议的基础上添加了一些额外的头信息，比如service name，magic number等。使用dubbo thrift协议同样需要使用thrift的idl compiler编译生成相应的java代码，后续版本中会在这方面做一些增强
依赖

<dependency>
    <groupId>org.apache.thrift</groupId>
    <artifactId>libthrift</artifactId>
    <version>0.8.0</version>
</dependency>
1
2
3
4
5
配置

<dubbo:protocol name="thrift" port="3030" />
1
Thrift不支持null值，不能在协议中传null

memcached协议
可以通过脚本或监控中心手工填写表单注册memcached服务的地址：

RegistryFactory registryFactory = ExtensionLoader.getExtensionLoader(RegistryFactory.class).getAdaptiveExtension();
Registry registry = registryFactory.getRegistry(URL.valueOf("zookeeper://10.20.153.10:2181"));
registry.register(URL.valueOf("memcached://10.20.153.11/com.foo.BarService?category=providers&dynamic=false&application=foo&group=member&loadbalance=consistenthash"));
1
2
3
然后在客户端使用时，不需要感知Memcached的地址：

<dubbo:reference id="cache" interface="http://10.20.160.198/wiki/display/dubbo/java.util.Map" group="member" />
1
或者点对点直连

<dubbo:reference id="cache" interface="http://10.20.160.198/wiki/display/dubbo/java.util.Map" url="memcached://10.20.153.10:11211" />
1
自定义接口

<dubbo:reference id="cache" interface="com.foo.CacheService" url="memcached://10.20.153.10:11211" />
1
方法名建议和memcached的标准方法名相同，即：get(key), set(key, value), delete(key)。
如果方法名和memcached的标准方法名不相同，则需要配置映射关系：(其中”p:xxx”为spring的标准p标签)

<dubbo:reference id="cache" interface="com.foo.CacheService" url="memcached://10.20.153.10:11211" p:set="putFoo" p:get="getFoo" p:delete="removeFoo" />
1
redis协议
可以通过脚本或监控中心手工填写表单注册redis服务的地址：

RegistryFactory registryFactory = ExtensionLoader.getExtensionLoader(RegistryFactory.class).getAdaptiveExtension();
Registry registry = registryFactory.getRegistry(URL.valueOf("zookeeper://10.20.153.10:2181"));
registry.register(URL.valueOf("redis://10.20.153.11/com.foo.BarService?category=providers&dynamic=false&application=foo&group=member&loadbalance=consistenthash"));
1
2
3
然后在客户端使用时，不需要感知Redis的地址：

<dubbo:reference id="store" interface="http://10.20.160.198/wiki/display/dubbo/java.util.Map" group="member" />
1
点对点直连

<dubbo:reference id="store" interface="http://10.20.160.198/wiki/display/dubbo/java.util.Map" url="redis://10.20.153.10:6379" />
1
自定义接口

<dubbo:reference id="store" interface="com.foo.StoreService" url="redis://10.20.153.10:6379" />
1
方法名建议和redis的标准方法名相同，即：get(key), set(key, value), delete(key)。
如果方法名和redis的标准方法名不相同，则需要配置映射关系：(其中”p:xxx”为spring的标准p标签)

<dubbo:reference id="cache" interface="com.foo.CacheService" url="memcached://10.20.153.10:11211" p:set="putFoo" p:get="getFoo" p:delete="removeFoo" />


dubbo的负载均衡策略和容错策略
一、dubbo 负载均衡策略
 

random loadbalance（随机均衡算法）
　随机，按权重设置随机概率。
在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。

roundrobin loadbalance（权重轮循均衡算法）
　　这个的话默认就是均匀地将流量打到各个机器上去，但是如果各个机器的性能不一样，容易导致性能差的机器负载过高。所以此时需要调整权重，让性能差的机器承载权重小一些，流量少一些。

举个栗子。

跟运维同学申请机器，有的时候，我们运气好，正好公司资源比较充足，刚刚有一批热气腾腾、刚刚做好的一批虚拟机新鲜出炉，配置都比较高：8 核 + 16G 机器，申请到 2 台。过了一段时间，我们感觉 2 台机器有点不太够，我就去找运维同学说，“哥儿们，你能不能再给我一台机器”，但是这时只剩下一台 4 核 + 8G 的机器。我要还是得要。

这个时候，可以给两台 8 核 16G 的机器设置权重 4，给剩余 1 台 4 核 8G 的机器设置权重 2。

leastactive loadbalance（最少活跃调用数均衡算法）
　　这个就是自动感知一下，如果某个机器性能越差，那么接收的请求越少，越不活跃，此时就会给不活跃的性能差的机器更少的请求。

consistanthash loadbalance（一致性Hash均衡算法）
　　一致性 Hash 算法，相同参数的请求一定分发到一个 provider 上去，provider 挂掉的时候，会基于虚拟节点均匀分配剩余的流量，抖动不会太大。如果你需要的不是随机负载均衡，是要一类请求都到一个节点，那就走这个一致性 Hash 策略。

 

二、dubbo 集群容错策略
 

failover cluster 模式
　　失败自动切换，自动重试其他机器，默认就是这个，常见于读操作。（失败重试其它机器）

failfast cluster模式
　　一次调用失败就立即失败，常见于写操作。（调用失败就立即失败）

failsafe cluster 模式
　　出现异常时忽略掉，常用于不重要的接口调用，比如记录日志。

failback cluster 模式
　　失败了后台自动记录请求，然后定时重发，比较适合于写消息队列这种。

forking cluster 模式
　　并行调用多个 provider，只要一个成功就立即返回。

broadcacst cluster
　　逐个调用所有的 provider。
Double如何实现异步调用

整个异步过程图片描述的很清楚，下面来看看代码：

1.服务提供者
1.1 服务提供者接口
package com.test.dubboser;
 
public interface ServiceDemo2 {
public Person getPerson(String str,int age);
}
1.2 Person 类
package com.test.dubboser;
 
import java.io.Serializable;
 
public class Person implements Serializable {
/**
	 * 
	 */
	private static final long serialVersionUID = 8661104133888956335L;
private int age;
private String name;
public Person(){}
public Person(int age ,String name){
	this.age= age;
	this.name=name;
}
public int getAge() {
	return age;
}
public void setAge(int age) {
	this.age = age;
}
public String getName() {
	return name;
}
public void setName(String name) {
	this.name = name;
}
 
@Override
public String  toString(){
	StringBuffer  buffer= new StringBuffer();
	buffer.append("name:"+name+"\t");
	buffer.append("age:"+age);
	return buffer.toString();
	
}
}
1.3 服务提供者接口实现类
package com.test.dubboser;
 
public class ServiceImp2 implements ServiceDemo2{
 
	public Person getPerson(String str,int age) {
       Person person=new Person();
       person.setName(str);
       person.setAge(age);
       return person;
	}
}
1.4 配置文件
<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xmlns:dubbo="http://code.alibabatech.com/schema/dubbo"
    xsi:schemaLocation="http://www.springframework.org/schema/beans
        http://www.springframework.org/schema/beans/spring-beans.xsd
        http://code.alibabatech.com/schema/dubbo
        http://code.alibabatech.com/schema/dubbo/dubbo.xsd
        ">     
     <!-- 提供方应用信息，用于计算依赖关系,这个和client没必要一致 -->
	<dubbo:application name="hello-world-app-my" />
	<!-- 多注册中心配置 -->
   <dubbo:registry  protocol="zookeeper"  address="192.168.0.102:2181"/>
     <!-- 用dubbo协议在20880端口暴露服务 -->
   	<dubbo:protocol name="dubbo" port="20880"/>  
   	 <!-- 声明需要暴露的服务接口 -->
	<dubbo:service  interface="com.test.dubboser.ServiceDemo"
		ref="demoService"/> 
	<dubbo:service  interface="com.test.dubboser.ServiceDemo2"
		ref="demoService2"/> 
	<dubbo:service  interface="com.test.dubboser.CacheService"
		ref="cacheService"/> 
    <!--和本地bean一样实现服务 -->
	<bean id="demoService" class="com.test.dubboser.ServiceImp"/>
	<bean id="demoService2" class="com.test.dubboser.ServiceImp2"/>
	<!-- 结果缓存 -->
	<bean id="cacheService" class="com.test.dubboser.CacheServiceImp"/>
	
</beans>
 

2.服务消费者
2.1 配置文件
<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xmlns:dubbo="http://code.alibabatech.com/schema/dubbo"
    xsi:schemaLocation="http://www.springframework.org/schema/beans
        http://www.springframework.org/schema/beans/spring-beans.xsd
        http://code.alibabatech.com/schema/dubbo
        http://code.alibabatech.com/schema/dubbo/dubbo.xsd
        ">     
	<!-- 消费方应用名，用于计算依赖关系，不是匹配条件，不要与提供方一样 -->
	<dubbo:application name="consumer-of-helloworld-app-my" />     
	<!-- 使用zookeeper广播注册中心暴露发现服务地址 -->
	<dubbo:registry  protocol="zookeeper"  address="192.168.0.102:2181"/>  
	<!--获取服务  -->
    <dubbo:reference id="demoServicemy"    interface="com.test.dubboser.ServiceDemo"/>
    <!--异步功能实现-->
    <dubbo:reference id="demoServicemy2"   interface="com.test.dubboser.ServiceDemo2">
      <dubbo:method name="getPerson" async="true" />
    </dubbo:reference>
    <!--结果缓存配置  -->
    <dubbo:reference id="cacheService"   interface="com.test.dubboser.CacheService"  cache="true"/>
</beans>
注意这里的这一行，实现异步配置：

<dubbo:reference id="demoServicemy2"   interface="com.test.dubboser.ServiceDemo2">
    <dubbo:method name="getPerson" async="true" />
</dubbo:reference>
2.2 消费者代码
package com.test.dubbocli;
 
import java.util.concurrent.ExecutionException;
import java.util.concurrent.Future;
 
import org.springframework.context.support.ClassPathXmlApplicationContext;
 
import com.alibaba.dubbo.rpc.RpcContext;
import com.test.dubboser.CacheService;
import com.test.dubboser.Person;
import com.test.dubboser.ServiceDemo;
import com.test.dubboser.ServiceDemo2;
 
public class Main {
 
    public static void main(String[] args) throws InterruptedException, ExecutionException {
    	run();
    }
     public static void run() throws InterruptedException, ExecutionException{
    	ClassPathXmlApplicationContext context = new ClassPathXmlApplicationContext(new String[] { "applicationConsumer.xml" });
    	context.start();
    	//ServiceDemo demoServer = (ServiceDemo) context.getBean("demoServicemy");
    	ServiceDemo2 demoServer2 = (ServiceDemo2) context.getBean("demoServicemy2");
    	/*ServiceDemo demoServer3 = (ServiceDemo) context.getBean("demoServicemy3");*/
    	/*String str=demoServer.say("java ---->>>");*/
    	//调用后立即返回null
    	Person person=demoServer2.getPerson("www", 13);
    	System.err.println("立即返回的为null:"+person);
    	//拿到调用的Future引用，当结果返回后，会被通知和设置到此Future。
        Future<Person> pFuture = RpcContext.getContext().getFuture();
        //如果Person已返回，直接拿到返回值，否则线程wait，等待Person返回后，线程会被notify唤醒。
        person = pFuture.get();
        System.out.println("返回的有值"+person);
        System.out.println(person);
     }
}
 

运行结果:

立即返回的为null:null
future中获取值：name:www	age:13
 

3.异步返回值，和异步无返回值
你也可以设置是否等待消息发出：(异步总是不等待返回)

1、sent="true" 等待消息发出，消息发送失败将抛出异常。

2、sent="false" 不等待消息发出，将消息放入IO队列，即刻返回。

<dubbo:reference id="demoServicemy2"   interface="com.test.dubboser.ServiceDemo2">
    <dubbo:method name="getPerson" async="true" sent="true" />
</dubbo:reference>

3、如果你只是想异步，完全忽略返回值，可以配置return="false"，以减少Future对象的创建和管理成本：

<dubbo:reference id="demoServicemy2"   interface="com.test.dubboser.ServiceDemo2">
    <dubbo:method name="getPerson" async="true" return="false" />
</dubbo:reference>

设置了return =“false”后我们就获取不到Future对象，当然就获取不到返回值，这样就只有异步调用了服务端方法而没有返回值，执行的流程也就是最开始图形中的1和2 这两步，没有了其他的步骤所以速度也就比较快……

1、Dubbo是什么？
Dubbo是阿里巴巴开源的基于 Java 的高性能 RPC 分布式服务框架，现已成为 Apache 基金会孵化项目。
面试官问你如果这个都不清楚，那下面的就没必要问了。
官网：http://dubbo.apache.org
2、为什么要用Dubbo？
因为是阿里开源项目，国内很多互联网公司都在用，已经经过很多线上考验。内部使用了 Netty、Zookeeper，保证了高性能高可用性。
使用 Dubbo 可以将核心业务抽取出来，作为独立的服务，逐渐形成稳定的服务中心，可用于提高业务复用灵活扩展，使前端应用能更快速的响应多变的市场需求。
 
下面这张图可以很清楚的诠释，最重要的一点是，分布式架构可以承受更大规模的并发流量。

下面是 Dubbo 的服务治理图。

3、Dubbo 和 Spring Cloud 有什么区别？
两个没关联，如果硬要说区别，有以下几点。
1）通信方式不同
Dubbo 使用的是 RPC 通信，而 Spring Cloud 使用的是 HTTP RESTFul 方式。
2）组成部分不同

4、dubbo都支持什么协议，推荐用哪种？

dubbo://（推荐）


rmi://


hessian://


http://


webservice://


thrift://


memcached://


redis://


rest://

5、Dubbo需要 Web 容器吗？
不需要，如果硬要用 Web 容器，只会增加复杂性，也浪费资源。
6、Dubbo内置了哪几种服务容器？

Spring Container


Jetty Container


Log4j Container

Dubbo 的服务容器只是一个简单的 Main 方法，并加载一个简单的 Spring 容器，用于暴露服务。
7、Dubbo里面有哪几种节点角色？

8、画一画服务注册与发现的流程图

该图来自 Dubbo 官网，供你参考，如果你说你熟悉 Dubbo, 面试官经常会让你画这个图，记好了。
9、Dubbo默认使用什么注册中心，还有别的选择吗？
推荐使用 Zookeeper 作为注册中心，还有 Redis、Multicast、Simple 注册中心，但不推荐。
10、Dubbo有哪几种配置方式？
1）Spring 配置方式
2）Java API 配置方式
11、Dubbo 核心的配置有哪些？
我曾经面试就遇到过面试官让你写这些配置，我也是蒙逼。。

配置之间的关系见下图。

12、在 Provider 上可以配置的 Consumer 端的属性有哪些？
1）timeout：方法调用超时
2）retries：失败重试次数，默认重试 2 次
3）loadbalance：负载均衡算法，默认随机
4）actives 消费者端，最大并发调用限制
13、Dubbo启动时如果依赖的服务不可用会怎样？
Dubbo 缺省会在启动时检查依赖的服务是否可用，不可用时会抛出异常，阻止 Spring 初始化完成，默认 check="true"，可以通过 check="false" 关闭检查。
14、Dubbo推荐使用什么序列化框架，你知道的还有哪些？
推荐使用Hessian序列化，还有Duddo、FastJson、Java自带序列化。
15、Dubbo默认使用的是什么通信框架，还有别的选择吗？
Dubbo 默认使用 Netty 框架，也是推荐的选择，另外内容还集成有Mina、Grizzly。
16、Dubbo有哪几种集群容错方案，默认是哪种？

17、Dubbo有哪几种负载均衡策略，默认是哪种？

18、注册了多个同一样的服务，如果测试指定的某一个服务呢？
可以配置环境点对点直连，绕过注册中心，将以服务接口为单位，忽略注册中心的提供者列表。
19、Dubbo支持服务多协议吗？
Dubbo 允许配置多协议，在不同服务上支持不同协议或者同一服务上同时支持多种协议。
20、当一个服务接口有多种实现时怎么做？
当一个接口有多种实现时，可以用 group 属性来分组，服务提供方和消费方都指定同一个 group 即可。
21、服务上线怎么兼容旧版本？
可以用版本号（version）过渡，多个不同版本的服务注册到注册中心，版本号不同的服务相互间不引用。这个和服务分组的概念有一点类似。
22、Dubbo可以对结果进行缓存吗？
可以，Dubbo 提供了声明式缓存，用于加速热门数据的访问速度，以减少用户加缓存的工作量。
23、Dubbo服务之间的调用是阻塞的吗？
默认是同步等待结果阻塞的，支持异步调用。
Dubbo 是基于 NIO 的非阻塞实现并行调用，客户端不需要启动多线程即可完成并行调用多个远程服务，相对多线程开销较小，异步调用会返回一个 Future 对象。
异步调用流程图如下。

24、Dubbo支持分布式事务吗？
目前暂时不支持，后续可能采用基于 JTA/XA 规范实现，如以图所示。

25、Dubbo telnet 命令能做什么？
dubbo 通过 telnet 命令来进行服务治理，具体使用看这篇文章《dubbo服务调试管理实用命令》。
telnet localhost 8090
26、Dubbo支持服务降级吗？
Dubbo 2.2.0 以上版本支持。
27、Dubbo如何优雅停机？
Dubbo 是通过 JDK 的 ShutdownHook 来完成优雅停机的，所以如果使用 kill -9 PID 等强制关闭指令，是不会执行优雅停机的，只有通过 kill PID 时，才会执行。
28、服务提供者能实现失效踢出是什么原理？
服务失效踢出基于 Zookeeper 的临时节点原理。
29、如何解决服务调用链过长的问题？
Dubbo 可以使用 Pinpoint 和 Apache Skywalking(Incubator) 实现分布式服务追踪，当然还有其他很多方案。
30、服务读写推荐的容错策略是怎样的？
读操作建议使用 Failover 失败自动切换，默认重试两次其他服务器。
写操作建议使用 Failfast 快速失败，发一次调用失败就立即报错。
31、Dubbo必须依赖的包有哪些？
Dubbo 必须依赖 JDK，其他为可选。
32、Dubbo的管理控制台能做什么？
管理控制台主要包含：路由规则，动态配置，服务降级，访问控制，权重调整，负载均衡，等管理功能。
33、说说 Dubbo 服务暴露的过程。
Dubbo 会在 Spring 实例化完 bean 之后，在刷新容器最后一步发布 ContextRefreshEvent 事件的时候，通知实现了 ApplicationListener 的 ServiceBean 类进行回调 onApplicationEvent 事件方法，Dubbo 会在这个方法中调用 ServiceBean 父类 ServiceConfig 的 export 方法，而该方法真正实现了服务的（异步或者非异步）发布。
34、Dubbo 停止维护了吗？
2014 年开始停止维护过几年，17 年开始重新维护，并进入了 Apache 项目。
35、Dubbo 和 Dubbox 有什么区别？
Dubbox 是继 Dubbo 停止维护后，当当网基于 Dubbo 做的一个扩展项目，如加了服务可 Restful 调用，更新了开源组件等。
36、你还了解别的分布式框架吗？
别的还有 Spring cloud、Facebook 的 Thrift、Twitter 的 Finagle 等。
37、Dubbo 能集成 Spring Boot 吗？
可以的，项目地址如下。
https://github.com/apache/incubator-dubbo-spring-boot-project
38、在使用过程中都遇到了些什么问题？
Dubbo 的设计目的是为了满足高并发小数据量的 rpc 调用，在大数据量下的性能表现并不好，建议使用 rmi 或 http 协议。
39、你读过 Dubbo 的源码吗？
要了解 Dubbo 就必须看其源码，了解其原理，花点时间看下吧，网上也有很多教程，后续有时间我也会在公众号上分享 Dubbo 的源码。

SpringCloud与Double区别？
性能：
dubbo由于是二进制的传输，占用带宽会更少。dubbo的网络消耗小于springcloud

springCloud是http协议传输，带宽会比较多，同时使用http协议一般会使用JSON报文，消耗会更大。（网络消耗不是什么太大问题，如果真的成了问题，通过压缩、二进制、高速缓存、分段降级等方法，很容易解）
开发难度：
dubbo的开发难度较大，原因是dubbo的jar包依赖问题很多大型工程无法解决

springcloud的接口协议约定比较自由且松散，需要有强有力的行政措施来限制接口无序升级
复杂度：
dubbo各种复杂的Url，protocol，register，invocation，dubbofilter，dubboSPI，dubbo序列化、dubbo协议，系统复杂
springcloud的系统结构更简单
注册中心：
dubbo的注册中心可以选择zk,redis等多种，springcloud的注册中心只能用eureka、zk或者自研
后续改进：
dubbo的改进是通过dubbofilter，很多东西没有，需要自己继承，如监控，如日志，如限流，如追踪。springcloud自己带了很多监控、限流措施，但是功能可能和欧美习惯相同，国内需要进行适当改造，但更简单，就是ServletFilter而已，但是总归比dubbo多一些东西是好的
配套设施：
dubbo仅是个单纯的服务化的工具。一个完整的系统，要有前台、中台、后台、前台包括前端和交互，中台包括交易、任务、数据，后台包括财务、账户、管理...........单纯的服务化解决不了“任何问题”，唯有体系才能解决。
springcloud是个往“体系”方向发展的方案。
社区活跃度：
Springcloud，spring团队大力支持。
dubbo刚出来的那会技术理念还是非常先进，解决了各大互联网公司服务治理的问题，中国的各中小公司也从中受益不少。经过了这么多年的发展，互联网行业也是涌现了更多先进的技术和理念，Dubbo一直停滞不前，自然有些掉队
SpringCloud与Double技术选型？
虽然Dubbo 支持短连接大数据量的服务提供模式，但绝大多数情况下都是使用长连接小数据量的模式提供服务使用的。
对于类似于电商等同步调用场景多并且能支撑搭建Dubbo 这套比较复杂环境的成本的产品而言，Dubbo 确实是一个可以考虑的选择。但如果产品业务中由于后台业务逻辑复杂、时间长而导致异步逻辑比较多的话，可能Dubbo 并不合适。同时，对于人手不足的初创产品而言，这么重的架构维护起来也不是很方便。
RPC和HTTP区别？
RPC:(Remote Produce Call)远程过程调用

1.基于Socket
2.自定义数据格式
3.速度快，效率高
4.典型应用代表：Dubbo，WebService，ElasticSearch集群间互相调用

HTTP：网络传输协议
1.基于TCP/IP
2.规定数据传输格式
3.缺点是消息封装比较臃肿、传输速度比较慢
4.优点是对服务提供和调用方式没有任何技术限定，自由灵活，更符合微服务理念

RPC和HTTP的区别：RPC是根据语言API来定义，而不是根据基于网络的应用来定义。
分布式事务中常见的三种解决方案
一、分布式事务前奏
事务：事务是由一组操作构成的可靠的独立的工作单元，事务具备ACID的特性，即原子性、一致性、隔离性和持久性。
本地事务：当事务由资源管理器本地管理时被称作本地事务。本地事务的优点就是支持严格的ACID特性，高效，可靠，状态可以只在资源管理器中维护，而且应用编程模型简单。但是本地事务不具备分布式事务的处理能力，隔离的最小单位受限于资源管理器。
全局事务：当事务由全局事务管理器进行全局管理时成为全局事务，事务管理器负责管理全局的事务状态和参与的资源，协同资源的一致提交回滚。
TX协议：应用或者应用服务器与事务管理器的接口。
XA协议：全局事务管理器与资源管理器的接口。XA是由X/Open组织提出的分布式事务规范。该规范主要定义了全局事务管理器和局部资源管理器之间的接口。主流的数据库产品都实现了XA接口。XA接口是一个双向的系统接口，在事务管理器以及多个资源管理器之间作为通信桥梁。之所以需要XA是因为在分布式系统中从理论上讲两台机器是无法达到一致性状态的，因此引入一个单点进行协调。由全局事务管理器管理和协调的事务可以跨越多个资源和进程。全局事务管理器一般使用XA二阶段协议与数据库进行交互。
AP：应用程序，可以理解为使用DTP（Data Tools Platform）的程序。
RM：资源管理器，这里可以是一个DBMS或者消息服务器管理系统，应用程序通过资源管理器对资源进行控制，资源必须实现XA定义的接口。资源管理器负责控制和管理实际的资源。
TM：事务管理器，负责协调和管理事务，提供给AP编程接口以及管理资源管理器。事务管理器控制着全局事务，管理事务的生命周期，并且协调资源。
两阶段提交协议：XA用于在全局事务中协调多个资源的机制。TM和RM之间采取两阶段提交的方案来解决一致性问题。两节点提交需要一个协调者（TM）来掌控所有参与者（RM）节点的操作结果并且指引这些节点是否需要最终提交。两阶段提交的局限在于协议成本，准备阶段的持久成本，全局事务状态的持久成本，潜在故障点多带来的脆弱性，准备后，提交前的故障引发一系列隔离与恢复难题。
BASE理论：BA指的是基本业务可用性，支持分区失败，S表示柔性状态，也就是允许短时间内不同步，E表示最终一致性，数据最终是一致的，但是实时是不一致的。原子性和持久性必须从根本上保障，为了可用性、性能和服务降级的需要，只有降低一致性和隔离性的要求。
CAP定理：对于共享数据系统，最多只能同时拥有CAP其中的两个，任意两个都有其适应的场景，真是的业务系统中通常是ACID与CAP的混合体。分布式系统中最重要的是满足业务需求，而不是追求高度抽象，绝对的系统特性。C表示一致性，也就是所有用户看到的数据是一样的。A表示可用性，是指总能找到一个可用的数据副本。P表示分区容错性，能够容忍网络中断等故障。
柔性事务中的服务模式：
可查询操作：服务操作具有全局唯一的标识，操作唯一的确定的时间。
幂等操作：重复调用多次产生的业务结果与调用一次产生的结果相同。一是通过业务操作实现幂等性，二是系统缓存所有请求与处理的结果，最后是检测到重复请求之后，自动返回之前的处理结果。
TCC操作：Try阶段，尝试执行业务，完成所有业务的检查，实现一致性；预留必须的业务资源，实现准隔离性。Confirm阶段：真正的去执行业务，不做任何检查，仅适用Try阶段预留的业务资源，Confirm操作还要满足幂等性。Cancel阶段：取消执行业务，释放Try阶段预留的业务资源，Cancel操作要满足幂等性。TCC与2PC(两阶段提交)协议的区别：TCC位于业务服务层而不是资源层，TCC没有单独准备阶段，Try操作兼备资源操作与准备的能力，TCC中Try操作可以灵活的选择业务资源，锁定粒度。TCC的开发成本比2PC高。实际上TCC也属于两阶段操作，但是TCC不等同于2PC操作。
可补偿操作：Do阶段：真正的执行业务处理，业务处理结果外部可见。Compensate阶段：抵消或者部分撤销正向业务操作的业务结果，补偿操作满足幂等性。约束：补偿操作在业务上可行，由于业务执行结果未隔离或者补偿不完整带来的风险与成本可控。实际上，TCC的Confirm和Cancel操作可以看做是补偿操作。
二、柔性事务解决方案架构
在电商领域等互联网场景下，传统的事务在数据库性能和处理能力上都暴露出了瓶颈。柔性事务有两个特性：基本可用和柔性状态。所谓基本可用是指分布式系统出现故障的时候允许损失一部分的可用性。柔性状态是指允许系统存在中间状态，这个中间状态不会影响系统整体的可用性，比如数据库读写分离的主从同步延迟等。柔性事务的一致性指的是最终一致性。

（一）、基于可靠消息的最终一致性方案概述



实现：业务处理服务在业务事务提交之前，向实时消息服务请求发送消息，实时消息服务只记录消息数据，而不是真正的发送。业务处理服务在业务事务提交之后，向实时消息服务确认发送。只有在得到确认发送指令后，实时消息服务才会真正发送。
消息：业务处理服务在业务事务回滚后，向实时消息服务取消发送。消息发送状态确认系统定期找到未确认发送或者回滚发送的消息，向业务处理服务询问消息状态，业务处理服务根据消息ID或者消息内容确认该消息是否有效。被动方的处理结果不会影响主动方的处理结果，被动方的消息处理操作是幂等操作。
成本：可靠的消息系统建设成本，一次消息发送需要两次请求，业务处理服务需要实现消息状态回查接口。
优点：消息数据独立存储，独立伸缩，降低业务系统和消息系统之间的耦合。对最终一致性时间敏感度较高，降低业务被动方的实现成本。兼容所有实现JMS标准的MQ中间件，确保业务数据可靠的前提下，实现业务的最终一致性，理想状态下是准实时的一致性。
（二）、TCC事务补偿型方案



实现：一个完整的业务活动由一个主业务服务于若干的从业务服务组成。主业务服务负责发起并完成整个业务活动。从业务服务提供TCC型业务操作。业务活动管理器控制业务活动的一致性，它登记业务活动的操作，并在业务活动提交时确认所有的TCC型操作的Confirm操作，在业务活动取消时调用所有TCC型操作的Cancel操作。
成本：实现TCC操作的成本较高，业务活动结束的时候Confirm和Cancel操作的执行成本。业务活动的日志成本。
使用范围：强隔离性，严格一致性要求的业务活动。适用于执行时间较短的业务，比如处理账户或者收费等等。
特点：不与具体的服务框架耦合，位于业务服务层，而不是资源层，可以灵活的选择业务资源的锁定粒度。TCC里对每个服务资源操作的是本地事务，数据被锁住的时间短，可扩展性好，可以说是为独立部署的SOA服务而设计的。
（三）、最大努力通知型



实现：业务活动的主动方在完成处理之后向业务活动的被动方发送消息，允许消息丢失。业务活动的被动方根据定时策略，向业务活动的主动方查询，恢复丢失的业务消息。
约束：被动方的处理结果不影响主动方的处理结果。
成本：业务查询与校对系统的建设成本。
使用范围：对业务最终一致性的时间敏感度低。跨企业的业务活动。
特点：业务活动的主动方在完成业务处理之后，向业务活动的被动方发送通知消息。主动方可以设置时间阶梯通知规则，在通知失败后按规则重复通知，知道通知N次后不再通知。主动方提供校对查询接口给被动方按需校对查询，用户恢复丢失的业务消息。
适用范围：银行通知，商户通知。
三、基于可靠消息的最终一致性方案详解
（一）、消息发送一致性
消息中间件在分布式系统中的核心作用就是异步通讯、应用解耦和并发缓冲（也叫作流量削峰）。在分布式环境下，需要通过网络进行通讯，就引入了数据传输的不确定性，也就是CAP理论中的分区容错性。




消息发送一致性是指产生消息的业务动作与消息发送一致，也就是说如果业务操作成功，那么由这个业务操作所产生的消息一定要发送出去，否则就丢失。

处理方式一

public void completeOrderService() {
    // 处理订单
    order.process();
    
    // 发送会计原始凭证消息
    pipe.sendAccountingVouchetMessage();
}
在上面的情况中，如果业务操作成功，执行的消息发送之前应用发生故障，消息发送不出去，导致消息丢失，将会产生订单系统与会计系统的数据不一致。如果消息系统或者网络异常，也会导致消息发送不出去，也会造成数据不一致。

处理方式二

public void completeOrderService() {
    // 发送会计原始凭证消息
    pipe.sendAccountingVouchetMessage();
    
    // 处理订单
    order.process();
}
如果将上面的两个操作调换一下顺序，这种情况就会更加不可控了，消息发出去了业务订单可能会失败，会造成订单系统与业务系统的数据不一致。那么JMS标准中的XA协议是否可以保障发送的一致性？

JMS协议标准的API中，有很多以XA开头的接口，其实就是前面讲到的支持XA协议（基于两阶段提交协议）的全局事务型接口。

XAConnection.class
XAConnectionFactory.class
XAQueueConnection.class
XAQueueConnectionFactory.class
XASession.class
XATopicConnection.class
XATopicConnectionFactory.class
XATopicSession.class
JMS中的XA系列的接口可以提供分布式事务的支持。但是引用XA方式的分布式事务，就会带来很多局限性。
要求业务操作的资源必须支持XA协议，但是并不是所有的资源都支持XA协议。
两阶段提交协议的成本。
持久化成本等DTP模型的局限性，例如：全局锁定、成本高、性能低。
使用XA协议违背了柔性事务的初衷。
（二）、保证消息一致的变通做法



发送消息：主动方现将应用把消息发给消息中间件，消息状态标记为“待确认”状态。
消息中间件收到消息后，把消息持久化到消息存储中，但是并不影响被动方投递消息。
消息中间件返回消息持久化结果，主动方根据返回的结果进行判断如何进行业务操作处理：
失败：放弃执行业务操作处理，结束，必要时向上层返回处理结果。
成功：执行业务操作处理。
业务操作完成后，把业务操作结果返回给消息中间件。
消息中间件收到业务操作结构后，根据业务结果进行处理：
失败：删除消息存储中的消息，结束。
成功：更新消息存储中的消息状态为“待发送”，然后执行消息投递。
前面的正向流程都成功之后，向被动方应用投递消息。
但是在上面的处理流程中，任何一个环节都有可能出现问题。

（三）、常规MQ消息处理流程和特点



常规的MQ队列处理流程无法实现消息的一致性。
投递消息的本质就是消息消费，可以细化。
（四）、消息重复发送问题和业务接口幂等性设计



对于未确认的消息，采用按规则重新投递的方式进行处理。对于以上流程，消息重复发送会导致业务处理接口出现重复调用的问题。消息消费过程中消息重复发送的主要原因就是消费者成功接收处理完消息后，消息中间件没有及时更新投递状态导致的。如果允许消息重复发送，那么消费方应该实现业务接口的幂等性设计。

（五）、本地消息服务方案



实现思路：
主动方应用系统通过业务操作完成业务数据的操作，在准备发送消息的时候将消息存储在主动方应用系统一份，另一份发送到实时消息服务
被动方应用系统监听实时消息系统中的消息，当被动方完成消息处理后通过调用主动方接口完成消息确认
主动方接收到消息确认以后删除消息数据。
通过消息查询服务查询到消息被接收之后再规定的时间内没有返回ACK确认消息就通过消息恢复系统重新发送消息。
优点：
消息的时效性比较高
从应用设计的角度实现了消息数据的可靠性，消息数据的可靠性不依赖于MQ中间件，弱化了对MQ中间件特性的依赖。
方案轻量级，容易实现。
缺点：
与具体的业务场景绑定，耦合性强，不可以共用。
消息数据与业务数据同步，占用业务系统资源。
业务系统在使用关系型数据库的情况下消息服务性能会受到关系型数据库的并发性能限制。
（六）、独立消息服务方案



实现思路：
预发送消息：主动方应用系统预发送消息，由消息服务子系统存储消息，如果存储失败，那么也就无法进行业务操作。如果返回存储成功，然后执行业务操作。
执行业务操作：执行业务操作如果成功的时候，将业务操作执行成功的状态发送到消息服务子系统。消息服务子系统修改消息的标识为“可发送”状态。
发送消息到实时消息服务：当消息的状态发生改变的时候，立刻将消息发送到实时消息服务中。接下来，消息将会被消息业务的消费端监听到，然后被消费。
消息状态子系统：相当于定时任务系统，在消息服务子系统中定时查找确认超时的消息，在主动方应用系统中也去定时查找没有处理成功的任务，进行相应的处理。
消息消费：当消息被消费的时候，向实时消息服务发送ACK，然后实时消息服务删除消息。同时调用消息服务子系统修改消息为“被消费”状态。
消息恢复子系统：当消费方返回消息的时候，由于网络中断等其他原因导致消息没有及时确认，那么需要消息恢复子系统定时查找出在消息服务子系统中没有确认的消息。将没有被确认的消息放到实时消息服务中，进行重做，因为被动方应用系统的接口是幂等的。
优点：
消息服务独立部署，独立维护，独立伸缩。
消息存储可以按需选择不同的数据库来集成实现。
消息服务可以被相同的的使用场景使用，降低重复建设服务的成本。
从分布式服务应用设计开发角度实现了消息数据的可靠性，消息数据的可靠性不依赖于MQ中间件，弱化了对MQ中间件特性的依赖。
降低了业务系统与消息系统之间的耦合，有利于系统的扩展维护。
缺点：
一次消息发送需要两次请求。
主动方应用系统需要实现业务操作状态的校验与查询接口。
（七）、消息服务子系统的设计实现
示例消息数据表：

名称	数据类型	允许空	默认值	属性	释义
uuid	varchar(50)	No	—	unique	UUID
version	int(11)	No	0	—	版本号
editer	varchar(100)	Yes	NULL	—	修改者
creater	varchar(100)	Yes	NULL	—	创建者
edit_time	datetime	Yes	0000-00-00 00:00:00	—	最后修改时间
create_time	datetime	No	0000-00-00 00:00:00	—	创建时间
msg_id	varchar(50)	No	—	—	消息ID
msg_body	longtext	No	—	—	消息内容
msg_date_type	varchar(50)	Yes	—	—	消息数据类型
consumer_queue	varchar(100)	No	—	—	消费队列
send_times	int(6)	No	0	—	消息重发次数
is_dead	varchar(20)	No	—	—	是否死亡
status	varchar(20)	No	—	—	状态
remark	varchar(200)	Yes	—	—	备注
field0	varchar(200)	Yes	—	—	扩展字段0
field1	varchar(200)	Yes	—	—	扩展字段1
field2	varchar(200)	Yes	—	—	扩展字段2


分布式系统面试题：分布式事务处理方案？
分布式事务的实现主要有以下 5 种方案.
1)TCC 方案(distributed-transacion-TCC)
TCC 的全称是：Try、Confirm、Cancel。
Try 阶段：这个阶段说的是对各个服务的资源做检测以及对资源进行锁定或者者预留。
Confirm 阶段：这个阶段说的是在各个服务中执行实际的操作。
Cancel 阶段：假如任何一个服务的业务方法执行出错，那么这里就需要进行补偿，就是执行已经执行成功的业务逻辑的回滚操作。（把那些执行成功的回滚）
这种方案说实话几乎很少人使用，我们用的也比较少，但是也有使用的场景。由于这个事务回滚实际上是严重依赖于你自己写代码来回滚和补偿了，会造成补偿代码巨大，非常之恶心。
比方说我们，一般来说跟钱相关的，跟钱打交道的，支付、交易相关的场景，我们会用 TCC，严格保证分布式事务要么一律成功，要么一律自动回滚，严格保证资金的正确性，保证在资金上不会出现问题。
而且最好是你的各个业务执行的时间都比较短。
但是说实话，一般尽量别这么搞，自己手写回滚逻辑，或者者是补偿逻辑，实在太恶心了，那个业务代码很难维护。


2)本地消息表(distributed-transaction-local-message-table)
本地消息表其实是国外的 ebay 搞出来的这么一套思想。
这个大概意思是这样的：
A 系统在自己本地一个事务里操作同时，插入一条数据到消息表；
接着 A 系统将这个消息发送到 MQ 中去；
B 系统接收到消息之后，在一个事务里，往自己本地消息表里插入一条数据，同时执行其余的业务操作，假如这个消息已经被解决过了，那么此时这个事务会回滚，这样保证不会重复解决消息；
B 系统执行成功之后，就会升级自己本地消息表的状态以及 A 系统消息表的状态；
假如 B 系统解决失败了，那么就不会升级消息表状态，那么此时 A 系统会定时扫描自己的消息表，假如有未解决的消息，会再次发送到 MQ 中去，让 B 再次解决；
这个方案保证了最终一致性，哪怕 B 事务失败了，但是 A 会不断重发消息，直到 B 那边成功为止。
这个方案说实话最大的问题就在于严重依赖于数据库的消息表来管理事务啥的，会导致假如是高并发场景咋办呢？咋扩展呢？所以一般的确很少用。


3)可靠消息最终一致性方案(主流)(distributed-transaction-reliable-message)
这个的意思，就是干脆不要用本地的消息表了，直接基于 MQ 来实现事务。比方阿里的 RocketMQ 就支持消息事务。
大概的意思就是：
A 系统先发送一个 prepared 消息到 mq，假如这个 prepared 消息发送失败那么就直接取消操作别执行了；
假如这个消息发送成功过了，那么接着执行本地事务，假如成功就告诉 mq 发送确认消息，假如失败就告诉 mq 回滚消息；
假如发送了确认消息，那么此时 B 系统会接收到确认消息，而后执行本地的事务；
mq 会自动定时轮询所有 prepared 消息回调你的接口，问你，这个消息是不是本地事务解决失败了，所有没发送确认的消息，是继续重试还是回滚？一般来说这里你即可以查下数据库看之前本地事务能否执行，假如回滚了，那么这里也回滚吧。这个就是避免可能本地事务执行成功了，而确认消息却发送失败了。
这个方案里，要是系统 B 的事务失败了咋办？重试咯，自动不断重试直到成功，假如实在是不行，要么就是针对重要的资金类业务进行回滚，比方 B 系统本地回滚后，想办法通知系统 A 也回滚；或者者是发送报警由人工来手工回滚和补偿。
这个还是比较合适的，目前国内互联网公司大都是这么玩儿的，要不你举用 RocketMQ 支持的，要不你就自己基于相似 ActiveMQ？RabbitMQ？自己封装一套相似的逻辑出来，总之思路就是这样子的。


4)最大努力通知方案
这个方案的大致意思就是：

系统 A 本地事务执行完之后，发送个消息到 MQ；
这里会有个专门消费 MQ 的最大努力通知服务，这个服务会消费 MQ 而后写入数据库中记录下来，或者者是放入个内存队列也可以，接着调用系统 B 的接口；
要是系统 B 执行成功就 ok 了；要是系统 B 执行失败了，那么最大努力通知服务就定时尝试重新调用系统 B，反复 N 次，最后还是不行就放弃。

5)两阶段提交方案/XA方案(distributed-transacion-XA)
所谓的 XA 方案，即：两阶段提交，有一个事务管理器的概念，负责协调多个数据库（资源管理器）的事务，事务管理器先问问各个数据库你准备好了吗？假如每个数据库都回复 ok，那么就正式提交事务，在各个数据库上执行操作；假如任何其中一个数据库答复不 ok，那么就回滚事务。
这种分布式事务方案，比较适合单块应用里，跨多个库的分布式事务，而且由于严重依赖于数据库层面来搞定复杂的事务，效率很低，绝对不适合高并发的场景。假如要玩儿，那么基于 Spring + JTA 即可以搞定，自己随意搜个 demo 看看就知道了。
这个方案，我们很少用，一般来说某个系统内部假如出现跨多个库的这么一个操作，是不合规的。我可以给大家详情一下， 现在微服务，一个大的系统分成几十个甚至几百个服务。一般来说，我们的规定和规范，是要求每个服务只能操作自己对应的一个数据库。
假如你要操作别的服务对应的库，不允许直连别的服务的库，违背微服务架构的规范，你随意交叉胡乱访问，几百个服务的话，全体乱套，这样的一套服务是没法管理的，没法治理的，可能会出现数据被别人改错，自己的库被别人写挂等情况。
假如你要操作别人的服务的库，你必需是通过调用别的服务的接口来实现，绝对不允许交叉访问别人的数据库。


你们公司是如何解决分布式事务的？
假如你真的被问到，可以这么说，我们某某特别严格的场景，用的是 TCC 来保证强一致性；而后其余的少量场景基于阿里的 RocketMQ 来实现分布式事务。

你找一个严格资金要求绝对不能错的场景，你可以说你是用的 TCC 方案；假如是一般的分布式事务场景，订单插入之后要调用库存服务升级库存，库存数据没有资金那么的敏感，可以用可靠消息最终一致性方案。

友情提醒一下，RocketMQ 3.2.6 之前的版本，是可以按照上面的思路来的，但是之后接口做了少量改变，我这里不再赘述了。

当然假如你愿意，你可以参考可靠消息最终一致性方案来自己实现一套分布式事务，比方基于 RocketMQ 来玩儿。
中间件
Redis
redis的数据类型，使用场景
回答：一共五种
(一)String
这个其实没啥好说的，最常规的set/get操作，value可以是String也可以是数字。一般做一些复杂的计数功能的缓存。
(二)hash
这里value存放的是结构化的对象，比较方便的就是操作其中的某个字段。博主在做单点登录的时候，就是用这种数据结构存储用户信息，以cookieId作为key，设置30分钟为缓存过期时间，能很好的模拟出类似session的效果。
(三)list
使用List的数据结构，可以做简单的消息队列的功能。另外还有一个就是，可以利用lrange命令，做基于redis的分页功能，性能极佳，用户体验好。本人还用一个场景，很合适—取行情信息。就也是个生产者和消费者的场景。LIST可以很好的完成排队，先进先出的原则。
(四)set
因为set堆放的是一堆不重复值的集合。所以可以做全局去重的功能。为什么不用JVM自带的Set进行去重？因为我们的系统一般都是集群部署，使用JVM自带的Set，比较麻烦，难道为了一个做一个全局去重，再起一个公共服务，太麻烦了。
另外，就是利用交集、并集、差集等操作，可以计算共同喜好，全部的喜好，自己独有的喜好等功能。
(五)sorted set
sorted set多了一个权重参数score,集合中的元素能够按score进行排列。可以做排行榜应用，取TOP N操作。

Redis 持久化机制
Redis是一个支持持久化的内存数据库，通过持久化机制把内存中的数据同步到硬盘文件来保证数据持久化。当Redis重启后通过把硬盘文件重新加载到内存，就能达到恢复数据的目的。
实现：单独创建fork()一个子进程，将当前父进程的数据库数据复制到子进程的内存中，然后由子进程写入到临时文件中，持久化的过程结束了，再用这个临时文件替换上次的快照文件，然后子进程退出，内存释放。

RDB是Redis默认的持久化方式。按照一定的时间周期策略把内存的数据以快照的形式保存到硬盘的二进制文件。即Snapshot快照存储，对应产生的数据文件为dump.rdb，通过配置文件中的save参数来定义快照的周期。（ 快照可以是其所表示的数据的一个副本，也可以是数据的一个复制品。）
AOF：Redis会将每一个收到的写命令都通过Write函数追加到文件最后，类似于MySQL的binlog。当Redis重启是会通过重新执行文件中保存的写命令来在内存中重建整个数据库的内容。
当两种方式同时开启时，数据恢复Redis会优先选择AOF恢复。

缓存雪崩、缓存穿透、缓存预热、缓存更新、缓存降级等问题
一、缓存雪崩
我们可以简单的理解为：由于原有缓存失效，新缓存未到期间
(例如：我们设置缓存时采用了相同的过期时间，在同一时刻出现大面积的缓存过期)，所有原本应该访问缓存的请求都去查询数据库了，而对数据库CPU和内存造成巨大压力，严重的会造成数据库宕机。从而形成一系列连锁反应，造成整个系统崩溃。
解决办法：
大多数系统设计者考虑用加锁（ 最多的解决方案）或者队列的方式保证来保证不会有大量的线程对数据库一次性进行读写，从而避免失效时大量的并发请求落到底层存储系统上。还有一个简单方案就时讲缓存失效时间分散开。

二、缓存穿透
缓存穿透是指用户查询数据，在数据库没有，自然在缓存中也不会有。这样就导致用户查询的时候，在缓存中找不到，每次都要去数据库再查询一遍，然后返回空（相当于进行了两次无用的查询）。这样请求就绕过缓存直接查数据库，这也是经常提的缓存命中率问题。
解决办法;
最常见的则是采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免了对底层存储系统的查询压力。
另外也有一个更为简单粗暴的方法，如果一个查询返回的数据为空（不管是数据不存在，还是系统故障），我们仍然把这个空结果进行缓存，但它的过期时间会很短，最长不超过五分钟。通过这个直接设置的默认值存放到缓存，这样第二次到缓冲中获取就有值了，而不会继续访问数据库，这种办法最简单粗暴。
5TB的硬盘上放满了数据，请写一个算法将这些数据进行排重。如果这些数据是一些32bit大小的数据该如何解决？如果是64bit的呢？

对于空间的利用到达了一种极致，那就是Bitmap和布隆过滤器(Bloom Filter)。
Bitmap： 典型的就是哈希表
缺点是，Bitmap对于每个元素只能记录1bit信息，如果还想完成额外的功能，恐怕只能靠牺牲更多的空间、时间来完成了。

布隆过滤器（推荐）
就是引入了k(k>1)k(k>1)个相互独立的哈希函数，保证在给定的空间、误判率下，完成元素判重的过程。
它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。
Bloom-Filter算法的核心思想就是利用多个不同的Hash函数来解决“冲突”。
Hash存在一个冲突（碰撞）的问题，用同一个Hash得到的两个URL的值有可能相同。为了减少冲突，我们可以多引入几个Hash，如果通过其中的一个Hash值我们得出某元素不在集合中，那么该元素肯定不在集合中。只有在所有的Hash函数告诉我们该元素在集合中时，才能确定该元素存在于集合中。这便是Bloom-Filter的基本思想。
Bloom-Filter一般用于在大数据量的集合中判定某元素是否存在。

三、缓存预热
缓存预热这个应该是一个比较常见的概念，相信很多小伙伴都应该可以很容易的理解，缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！
解决思路：
1、直接写个缓存刷新页面，上线时手工操作下；
2、数据量不大，可以在项目启动的时候自动进行加载；
3、定时刷新缓存；

四、缓存更新
除了缓存服务器自带的缓存失效策略之外（Redis默认的有6中策略可供选择），我们还可以根据具体的业务需求进行自定义的缓存淘汰，常见的策略有两种：
（1）定时去清理过期的缓存；
（2）当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存。
两者各有优劣，第一种的缺点是维护大量缓存的key是比较麻烦的，第二种的缺点就是每次用户请求过来都要判断缓存失效，逻辑相对比较复杂！具体用哪种方案，大家可以根据自己的应用场景来权衡。
五、缓存降级
当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。
降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。
以参考日志级别设置预案：
（1）一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级；
（2）警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警；
（3）错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级；
（4）严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。

服务降级的目的，是为了防止Redis服务故障，导致数据库跟着一起发生雪崩问题。因此，对于不重要的缓存数据，可以采取服务降级策略，例如一个比较常见的做法就是，Redis出现问题，不去数据库查询，而是直接返回默认值给用户。

热点数据和冷数据是什么
热点数据，缓存才有价值
对于冷数据而言，大部分数据可能还没有再次访问到就已经被挤出内存，不仅占用内存，而且价值不大。频繁修改的数据，看情况考虑使用缓存
对于上面两个例子，寿星列表、导航信息都存在一个特点，就是信息修改频率不高，读取通常非常高的场景。
对于热点数据，比如我们的某IM产品，生日祝福模块，当天的寿星列表，缓存以后可能读取数十万次。再举个例子，某导航产品，我们将导航信息，缓存以后可能读取数百万次。
**数据更新前至少读取两次，**缓存才有意义。这个是最基本的策略，如果缓存还没有起作用就失效了，那就没有太大价值了。
那存不存在，修改频率很高，但是又不得不考虑缓存的场景呢？有！比如，这个读取接口对数据库的压力很大，但是又是热点数据，这个时候就需要考虑通过缓存手段，减少数据库的压力，比如我们的某助手产品的，点赞数，收藏数，分享数等是非常典型的热点数据，但是又不断变化，此时就需要将数据同步保存到Redis缓存，减少数据库压力。

Memcache与Redis的区别都有哪些？
1)、存储方式 Memecache把数据全部存在内存之中，断电后会挂掉，数据不能超过内存大小。 Redis有部份存在硬盘上，redis可以持久化其数据
2)、数据支持类型 memcached所有的值均是简单的字符串，redis作为其替代者，支持更为丰富的数据类型 ，提供list，set，zset，hash等数据结构的存储
3)、使用底层模型不同 它们之间底层实现方式 以及与客户端之间通信的应用协议不一样。 Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求。
4). value 值大小不同：Redis 最大可以达到 1gb；memcache 只有 1mb。
5）redis的速度比memcached快很多
6）Redis支持数据的备份，即master-slave模式的数据备份。

单线程的redis为什么这么快
(一)纯内存操作
(二)单线程操作，避免了频繁的上下文切换
(三)采用了非阻塞I/O多路复用机制


Redis 内部结构
dict 本质上是为了解决算法中的查找问题（Searching）是一个用于维护key和value映射关系的数据结构，与很多语言中的Map或dictionary类似。 本质上是为了解决算法中的查找问题（Searching）
sds sds就等同于char * 它可以存储任意二进制数据，不能像C语言字符串那样以字符’\0’来标识字符串的结 束，因此它必然有个长度字段。
skiplist （跳跃表） 跳表是一种实现起来很简单，单层多指针的链表，它查找效率很高，堪比优化过的二叉平衡树，且比平衡树的实现，
quicklist
ziplist 压缩表 ziplist是一个编码后的列表，是由一系列特殊编码的连续内存块组成的顺序型数据结构，
redis的过期策略以及内存淘汰机制
redis采用的是定期删除+惰性删除策略。
为什么不用定时删除策略?
定时删除,用一个定时器来负责监视key,过期则自动删除。虽然内存及时释放，但是十分消耗CPU资源。在大并发请求下，CPU要将时间应用在处理请求，而不是删除key,因此没有采用这一策略.
定期删除+惰性删除是如何工作的呢?
定期删除，redis默认每个100ms检查，是否有过期的key,有过期key则删除。需要说明的是，redis不是每个100ms将所有的key检查一次，而是随机抽取进行检查(如果每隔100ms,全部key进行检查，redis岂不是卡死)。因此，如果只采用定期删除策略，会导致很多key到时间没有删除。
于是，惰性删除派上用场。也就是说在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除。
采用定期删除+惰性删除就没其他问题了么?
不是的，如果定期删除没删除key。然后你也没即时去请求key，也就是说惰性删除也没生效。这样，redis的内存会越来越高。那么就应该采用内存淘汰机制。
在redis.conf中有一行配置

maxmemory-policy volatile-lru
1
该配置就是配内存淘汰策略的(什么，你没配过？好好反省一下自己)
volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰
volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰
volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰
allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰
allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰
no-enviction（驱逐）：禁止驱逐数据，新写入操作会报错
ps：如果没有设置 expire 的key, 不满足先决条件(prerequisites); 那么 volatile-lru, volatile-random 和 volatile-ttl 策略的行为, 和 noeviction(不删除) 基本上一致。

Redis 为什么是单线程的
官方FAQ表示，因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了（毕竟采用多线程会有很多麻烦！）Redis利用队列技术将并发访问变为串行访问
1）绝大部分请求是纯粹的内存操作（非常快速）2）采用单线程,避免了不必要的上下文切换和竞争条件
3）非阻塞IO优点：
1.速度快，因为数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)
2. 支持丰富数据类型，支持string，list，set，sorted set，hash
3.支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行
4. 丰富的特性：可用于缓存，消息，按key设置过期时间，过期后将会自动删除如何解决redis的并发竞争key问题

同时有多个子系统去set一个key。这个时候要注意什么呢？ 不推荐使用redis的事务机制。因为我们的生产环境，基本都是redis集群环境，做了数据分片操作。你一个事务中有涉及到多个key操作的时候，这多个key不一定都存储在同一个redis-server上。因此，redis的事务机制，十分鸡肋。
(1)如果对这个key操作，不要求顺序： 准备一个分布式锁，大家去抢锁，抢到锁就做set操作即可
(2)如果对这个key操作，要求顺序： 分布式锁+时间戳。 假设这会系统B先抢到锁，将key1设置为{valueB 3:05}。接下来系统A抢到锁，发现自己的valueA的时间戳早于缓存中的时间戳，那就不做set操作了。以此类推。
(3) 利用队列，将set方法变成串行访问也可以redis遇到高并发，如果保证读写key的一致性
对redis的操作都是具有原子性的,是线程安全的操作,你不用考虑并发问题,redis内部已经帮你处理好并发的问题了。

Redis 集群方案应该怎么做？都有哪些方案？
1.twemproxy，大概概念是，它类似于一个代理方式， 使用时在本需要连接 redis 的地方改为连接 twemproxy， 它会以一个代理的身份接收请求并使用一致性 hash 算法，将请求转接到具体 redis，将结果再返回 twemproxy。
缺点： twemproxy 自身单端口实例的压力，使用一致性 hash 后，对 redis 节点数量改变时候的计算值的改变，数据无法自动移动到新的节点。

2.codis，目前用的最多的集群方案，基本和 twemproxy 一致的效果，但它支持在 节点数量改变情况下，旧节点数据可恢复到新 hash 节点

3.redis cluster3.0 自带的集群，特点在于他的分布式算法不是一致性 hash，而是 hash 槽的概念，以及自身支持节点设置从节点。具体看官方文档介绍。

有没有尝试进行多机redis 的部署？如何保证数据一致的？
主从复制，读写分离
一类是主数据库（master）一类是从数据库（slave），主数据库可以进行读写操作，当发生写操作的时候自动将数据同步到从数据库，而从数据库一般是只读的，并接收主数据库同步过来的数据，一个主数据库可以有多个从数据库，而一个从数据库只能有一个主数据库。

对于大量的请求怎么样处理
redis是一个单线程程序，也就说同一时刻它只能处理一个客户端请求；
redis是通过IO多路复用（select，epoll, kqueue，依据不同的平台，采取不同的实现）来处理多个客户端请求的

Redis 常见性能问题和解决方案？
(1) Master 最好不要做任何持久化工作，如 RDB 内存快照和 AOF 日志文件
(2) 如果数据比较重要，某个 Slave 开启 AOF 备份数据，策略设置为每秒同步一次
(3) 为了主从复制的速度和连接的稳定性， Master 和 Slave 最好在同一个局域网内
(4) 尽量避免在压力很大的主库上增加从库
(5) 主从复制不要用图状结构，用单向链表结构更为稳定，即： Master <- Slave1 <- Slave2 <-
Slave3…

讲解下Redis线程模型
文件事件处理器包括分别是套接字、 I/O 多路复用程序、 文件事件分派器（dispatcher）、 以及事件处理器。使用 I/O 多路复用程序来同时监听多个套接字， 并根据套接字目前执行的任务来为套接字关联不同的事件处理器。当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时， 与操作相对应的文件事件就会产生， 这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。
I/O 多路复用程序负责监听多个套接字， 并向文件事件分派器传送那些产生了事件的套接字。
工作原理：
1)I/O 多路复用程序负责监听多个套接字， 并向文件事件分派器传送那些产生了事件的套接字。
尽管多个文件事件可能会并发地出现， 但 I/O 多路复用程序总是会将所有产生事件的套接字都入队到一个队列里面， 然后通过这个队列， 以有序（sequentially）、同步（synchronously）、每次一个套接字的方式向文件事件分派器传送套接字： 当上一个套接字产生的事件被处理完毕之后（该套接字为事件所关联的事件处理器执行完毕）， I/O 多路复用程序才会继续向文件事件分派器传送下一个套接字。如果一个套接字又可读又可写的话， 那么服务器将先读套接字， 后写套接字.



为什么Redis的操作是原子性的，怎么保证原子性的？
对于Redis而言，命令的原子性指的是：一个操作的不可以再分，操作要么执行，要么不执行。
Redis的操作之所以是原子性的，是因为Redis是单线程的。
Redis本身提供的所有API都是原子操作，Redis中的事务其实是要保证批量操作的原子性。
多个命令在并发中也是原子性的吗？
不一定， 将get和set改成单命令操作，incr 。使用Redis的事务，或者使用Redis+Lua==的方式实现.

Redis事务
Redis事务功能是通过MULTI、EXEC、DISCARD和WATCH 四个原语实现的
Redis会将一个事务中的所有命令序列化，然后按顺序执行。
1.redis 不支持回滚“Redis 在事务失败时不进行回滚，而是继续执行余下的命令”， 所以 Redis 的内部可以保持简单且快速。
2.如果在一个事务中的命令出现错误，那么所有的命令都不会执行；
3.如果在一个事务中出现运行错误，那么正确的命令会被执行。

1）MULTI命令用于开启一个事务，它总是返回OK。 MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。
2）EXEC：执行所有事务块内的命令。返回事务块内所有命令的返回值，按命令执行的先后顺序排列。 当操作被打断时，返回空值 nil 。
3）通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务， 并且客户端会从事务状态中退出。
4）WATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为。 可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行，监控一直持续到EXEC命令。

Redis实现分布式锁
Redis为单进程单线程模式，采用队列模式将并发访问变成串行访问，且多客户端对Redis的连接并不存在竞争关系Redis中可以使用SETNX命令实现分布式锁。
将 key 的值设为 value ，当且仅当 key 不存在。 若给定的 key 已经存在，则 SETNX 不做任何动作

解锁：使用 del key 命令就能释放锁
解决死锁：
1）通过Redis中expire()给锁设定最大持有时间，如果超过，则Redis来帮我们释放锁。
2） 使用 setnx key “当前系统时间+锁持有的时间”和getset key “当前系统时间+锁持有的时间”组合的命令就可以实现。
RabbitMq
rabbitmq交换机类型以及应用场景
·  最新版本的RabbitMQ有四种交换机类型，分别是Direct exchange、Fanout exchange、Topic exchange、Headers exchange。
·  Direct exchange（直连交换机）
处理路由键。需要将一个队列绑定到交换机上，要求该消息与一个特定的路由键完全匹配。这是一个完整的匹配。如果一个队列绑定到该交换机上要求路由键 “abc”，则只有被标记为“abc”的消息才被转发，不会转发abc.def，也不会转发dog.ghi，只会转发abc。
·  Fanout exchange（扇型交换机）
不处理路由键。你只需要简单的将队列绑定到交换机上。一个发送到交换机的消息都会被转发到与该交换机绑定的所有队列上。很像子网广播，每台子网内的主机都获得了一份复制的消息。Fanout交换机转发消息是最快的。

·  Topic exchange（主题交换机）
将路由键和某模式进行匹配。此时队列需要绑定要一个模式上。符号“#”匹配一个或多个词，符号“”匹配不多不少一个词。因此“abc.#”能够匹配到“abc.def.ghi”，但是“abc.” 只会匹配到“abc.def”。
·  Headers exchange（头交换机）
不处理路由键。而是根据发送的消息内容中的headers属性进行匹配。在绑定Queue与Exchange时指定一组键值对；当消息发送到RabbitMQ时会取到该消息的headers与Exchange绑定时指定的键值对进行匹配；如果完全匹配则消息会路由到该队列，否则不会路由到该队列。headers属性是一个键值对，可以是Hashtable，键值对的值可以是任何类型。而fanout，direct，topic 的路由键都需要要字符串形式的。

匹配规则x-match有下列两种类型：

x-match = all ：表示所有的键值对都匹配才能接受到消息

x-match = any ：表示只要有键值对匹配就能接受到消息
一、什么是RabbitMQ?
采用AMQP高级消息队列协议的一种消息队列技术，最大的特点就是消费并不需要确保提供方存在，实现了服务之间的高度解耦。
二、为什么要使用RabbitMQ?
①在分布式系统下具备异步，削峰，负载均衡等一系列高级功能;
②拥有持久化的机制，进程消息，队列中的信息也可以保存下来。
③实现消费者和生产者之间的解耦。
④对于高并发场景下，利用消息队列可以使得同步访问变为串行访问达到一定量的限流，利于数据库的操作。
⑤可以使用消息队列达到异步下单的效果，排队中，后台进行逻辑下单。
三、RabbitMQ的使用场景有哪些?
①跨系统的异步通信，所有需要异步交互的地方都可以使用消息队列。就像我们除了打电话(同步)以外，还需要发短信，发电子邮件(异步)的通讯方式。
②多个应用之间的耦合，由于消息是平台无关和语言无关的，而且语义上也不再是函数调用，因此更适合作为多个应用之间的松耦合的接口。基于消息队列的耦合，不需要发送方和接收方同时在线。在企业应用集成(EAI)中，文件传输，共享数据库，消息队列，远程过程调用都可以作为集成的方法。
③应用内的同步变异步，比如订单处理，就可以由前端应用将订单信息放到队列，后端应用从队列里依次获得消息处理，高峰时的大量订单可以积压在队列里慢慢处理掉。由于同步通常意味着阻塞，而大量线程的阻塞会降低计算机的性能。
④消息驱动的架构(EDA)，系统分解为消息队列，和消息制造者和消息消费者，一个处理流程可以根据需要拆成多个阶段(Stage)，阶段之间用队列连接起来，前一个阶段处理的结果放入队列，后一个阶段从队列中获取消息继续处理。
⑤应用需要更灵活的耦合方式，如发布订阅，比如可以指定路由规则。
⑥跨局域网，甚至跨城市的通讯(CDN行业)，比如北京机房与广州机房的应用程序的通信。
四、RabbitMQ有哪些重要的角色?
RabbitMQ中重要的角色有：生产者、消费者和代理：
①生产者：消息的创建者，负责创建和推送数据到消息服务器;
②消费者：消息的接收方，用于处理数据和确认消息;
③代理：就是RabbitMQ本身，用于扮演“快递”的角色，本身不生产消息，只是扮演“快递”的角色。
五、如何确保消息正确地发送至RabbitMQ?如何确保消息接收方消费了消息?
1、发送方确认模式
①将信道设置成confirm模式(发送方确认模式)，则所有在信道上发布的消息都会被指派一个唯一的ID。
②一旦消息被投递到目的队列后，或者消息被写入磁盘后(可持久化的消息)，信道会发送一个确认给生产者(包含消息唯一 ID)。
③如果RabbitMQ发生内部错误从而导致消息丢失，会发送一条 nack(notacknowledged，未确认)消息。
④发送方确认模式是异步的，生产者应用程序在等待确认的同时，可以继续发送消息。当确认消息到达生产者应用程序，生产者应用程序的回调方法就会被触发来处理确认消息。
2、接收方确认机制
①消费者接收每一条消息后都必须进行确认(消息接收和消息确认是两个不同操作)。只有消费者确认了消息，RabbitMQ 才能安全地把消息从队列中删除。
②这里并没有用到超时机制，RabbitMQ仅通过Consumer的连接中断来确认是否需要重新发送消息。也就是说，只要连接不中断，RabbitMQ给了Consumer足够长的时间来处理消息。保证数据的最终一致性。
3、下面罗列几种特殊情况
①如果消费者接收到消息，在确认之前断开了连接或取消订阅，RabbitMQ会认为消息没有被分发，然后重新分发给下一个订阅的消费者。(可能存在消息重复消费的隐患，需要去重)
②如果消费者接收到消息却没有确认消息，连接也未断开，则RabbitMQ认为该消费者繁忙，将不会给该消费者分发更多的消息。
六、RabbitMQ怎么避免消息丢失?
①消息持久化;
②ACK确认机制;
③设置集群镜像模式;
④消息补偿机制。
七、要保证消息持久化成功的条件有哪些?
①声明队列必须设置持久化durable设置为 true。
②消息推送投递模式必须设置持久化，deliveryMode设置为2(持久)。
③消息已经到达持久化交换器。
④消息已经到达持久化队列。
以上四个条件都满足才能保证消息持久化成功。
八、RabbitMQ持久化有什么缺点?
持久化的缺地就是降低了服务器的吞吐量，因为使用的是磁盘而非内存存储，从而降低了吞吐量。可尽量使用ssd硬盘来缓解吞吐量的问题。
九、RabbitMQ 有几种广播类型?
三种广播模式：
①fanout：所有bind到此exchange的queue都可以接收消息(纯广播，绑定到RabbitMQ的接受者都能收到消息);
②direct：通过routingKey和exchange决定的那个唯一的queue可以接收消息;
③topic：所有符合routingKey(此时可以是一个表达式)的routingKey所bind的queue可以接收消息;
十、RabbitMQ中vhost的作用是什么?
vhost可以理解为虚拟broker ，即mini-RabbitMQ server。其内部均含有独立的 queue、exchange和binding等，但最最重要的是，其拥有独立的权限系统，可以做到 vhost 范围的用户控制。当然，从RabbitMQ的全局角度，vhost可以作为不同权限隔离的手段(一个典型的例子就是不同的应用可以跑在不同的vhost中)。

RabbitMq如何做高可用？
普通集群模式:


即在多个服务器上部署多个MQ实例, 每台机器一个实例. 创建的每一个queue,只会存在一个MQ实例上. 但是每一个实例都会同步queue的元数据(即queue的标识信息). 当在进行消费的时候, 就算 连接到了其他的MQ实例上, 其也会根据内部的queue的元数据,从该queue所在实例上拉取数据过来.



这种方式只是一个简单的集群,并没有考虑高可用. 并且性能开销巨大.容易造成单实例的性能瓶颈. 并且如果真正有数据的那个queue的实例宕机了. 那么其他的实例就无法进行数据的拉取.

这种方式只是通过集群部署的方式提高了消息的吞吐量,但是并没有考虑到高可用.

镜像集群模式:

这种模式才是高可用模式. 与普通集群模式的主要区别在于. 无论queue的元数据还是queue中的消息都会同时存在与多个实例上.




要开启镜像集群模式,需要在后台新增镜像集群模式策略. 即要求数据同步到所有的节点.也可以指定同步到指定数量的节点.

这种方式的好处就在于, 任何一个服务宕机了,都不会影响整个集群数据的完整性, 因为其他服务中都有queue的完整数据, 当进行消息消费的时候,连接其他的服务器节点一样也能获取到数据.

缺点:

1: 性能开销大: 因为需要进行整个集群内部所有实例的数据同步

2:无法线性扩容: 因为每一个服务器中都包含整个集群服务节点中的所有数据, 这样如果一旦单个服务器节点的容量无法容纳了怎么办?.
RabbitMq死信队列原理，应用场景？
定义：
“死信”是RabbitMQ中的一种消息机制，当你在消费消息时，如果队列里的消息出现以下情况：
消息被否定确认，使用 channel.basicNack 或 channel.basicReject ，并且此时requeue 属性被设置为false。
消息在队列的存活时间超过设置的TTL时间。
消息队列的消息数量已经超过最大队列长度。
那么该消息将成为“死信”。
“死信”消息会被RabbitMQ进行特殊处理，如果配置了死信队列信息，那么该消息将会被丢进死信队列中，如果没有配置，则该消息将会被丢弃。

业务场景：
为了保证订单业务的消息数据不丢失，需要使用到RabbitMQ的死信队列机制，当消息消费发生异常时，将消息投入死信队列中。
一般用在较为重要的业务队列中，确保未被正确消费的消息不被丢弃，一般发生消费异常可能原因主要有由于消息信息本身存在错误导致处理异常，处理过程中参数校验异常，或者因网络波动导致的查询异常等等，当发生异常时，当然不能每次通过日志来获取原消息，然后让运维帮忙重新投递消息（没错，以前就是这么干的= =）。通过配置死信队列，可以让未正确处理的消息暂存到另一个队列中，待后续排查清楚问题后，编写相应的处理代码来处理死信消息，这样比手工恢复数据要好太多了。
RabbitMq如何保证消费顺序？
业务场景：
先看看顺序会错乱的俩场景：
RabbitMQ：一个 queue，多个 consumer。比如，生产者向 RabbitMQ 里发送了三条数据，顺序依次是 data1/data2/data3，压入的是 RabbitMQ 的一个内存队列。有三个消费者分别从 MQ 中消费这三条数据中的一条，结果消费者2先执行完操作，把 data2 存入数据库，然后是 data1/data3。这不明显乱了。


Kafka：比如说我们建了一个 topic，有三个 partition。生产者在写的时候，其实可以指定一个 key，比如说我们指定了某个订单 id 作为 key，那么这个订单相关的数据，一定会被分发到同一个 partition 中去，而且这个 partition 中的数据一定是有顺序的。
消费者从 partition 中取出来数据的时候，也一定是有顺序的。到这里，顺序还是 ok 的，没有错乱。接着，我们在消费者里可能会搞多个线程来并发处理消息。因为如果消费者是单线程消费处理，而处理比较耗时的话，比如处理一条消息耗时几十 ms，那么 1 秒钟只能处理几十条消息，这吞吐量太低了。而多个线程并发跑的话，顺序可能就乱掉了。





解决方案：
RabbitMQ
拆分多个 queue，每个 queue 一个 consumer，就是多一些 queue 而已，确实是麻烦点；或者就一个 queue 但是对应一个 consumer，然后这个 consumer 内部用内存队列做排队，然后分发给底层不同的 worker 来处理。

（我们遇到的大多数场景都不需要消息的有序的，如果对于消息顺序敏感，那么我们这里给出的方法是 消息体通过hash分派到队列里，每个队列对应一个消费者，多分拆队列。
为什么要这么设计？  同一组的任务会被分配到同一个队列里，每个队列只能有一个worker来消费，这样避免了同一个队列多个消费者消费时，乱序的可能！ t1, t2 两个任务， t1 虽然被c1先pop了，但是有可能c2先把 t2 任务给完成了。
一句话，主动去分配队列，单个消费者。
）

Kafka
一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个。
写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。




RabbitMq网络拓扑？
RabbitMq重复消费？
首先我们可以确认的是，触发消息重复执行的条件会是很苛刻的！ 也就说 在大多数场景下不会触发该条件！！！ 一般出在任务超时，或者没有及时返回状态，引起任务重新入队列，重新消费！  在rabbtimq里连接的断开也会触发消息重新入队列。  
消费任务类型最好要支持幂等性，这样的好处是 任务执行多少次都没关系，顶多消耗一些性能！ 如果不支持幂等，比如发送信息？ 那么需要构建一个map来记录任务的执行情况！ 不仅仅是成功和失败，还要有心跳！！！  这个map在消费端实现就可以了！！！    这里会出现一个问题，有两个消费者 c1, c2 ，一个任务有可能被c1消费，如果再来一次，被c2执行？ 那么如何得知任务的情况？ 任务派发！  任务做成hash，固定消费者！
坚决不要想方设法在mq扩展这个future。
一句话，要不保证消息幂等性，要不就用map记录任务状态.


Kafka
1 什么是kafka

Kafka是分布式发布-订阅消息系统，它最初是由LinkedIn公司开发的，之后成为Apache项目的一部分，Kafka是一个分布式，可划分的，冗余备份的持久性的日志服务，它主要用于处理流式数据。

2 为什么要使用 kafka，为什么要使用消息队列

缓冲和削峰：上游数据时有突发流量，下游可能扛不住，或者下游没有足够多的机器来保证冗余，kafka在中间可以起到一个缓冲的作用，把消息暂存在kafka中，下游服务就可以按照自己的节奏进行慢慢处理。

解耦和扩展性：项目开始的时候，并不能确定具体需求。消息队列可以作为一个接口层，解耦重要的业务流程。只需要遵守约定，针对数据编程即可获取扩展能力。

冗余：可以采用一对多的方式，一个生产者发布消息，可以被多个订阅topic的服务消费到，供多个毫无关联的业务使用。

健壮性：消息队列可以堆积请求，所以消费端业务即使短时间死掉，也不会影响主要业务的正常进行。

异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。

3.Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么

ISR:In-Sync Replicas 副本同步队列
AR:Assigned Replicas 所有副本
ISR是由leader维护，follower从leader同步数据有一些延迟（包括延迟时间replica.lag.time.max.ms和延迟条数replica.lag.max.messages两个维度, 当前最新的版本0.10.x中只支持replica.lag.time.max.ms这个维度），任意一个超过阈值都会把follower剔除出ISR, 存入OSR（Outof-Sync Replicas）列表，新加入的follower也会先存放在OSR中。AR=ISR+OSR。

4.kafka中的broker 是干什么的

broker 是消息的代理，Producers往Brokers里面的指定Topic中写消息，Consumers从Brokers里面拉取指定Topic的消息，然后进行业务处理，broker在中间起到一个代理保存消息的中转站。

5.kafka中的 zookeeper 起到什么作用，可以不用zookeeper么

zookeeper 是一个分布式的协调组件，早期版本的kafka用zk做meta信息存储，consumer的消费状态，group的管理以及 offset的值。考虑到zk本身的一些因素以及整个架构较大概率存在单点问题，新版本中逐渐弱化了zookeeper的作用。新的consumer使用了kafka内部的group coordination协议，也减少了对zookeeper的依赖，

但是broker依然依赖于ZK，zookeeper 在kafka中还用来选举controller 和 检测broker是否存活等等。

6.kafka follower如何与leader同步数据

Kafka的复制机制既不是完全的同步复制，也不是单纯的异步复制。完全同步复制要求All Alive Follower都复制完，这条消息才会被认为commit，这种复制方式极大的影响了吞吐率。而异步复制方式下，Follower异步的从Leader复制数据，数据只要被Leader写入log就被认为已经commit，这种情况下，如果leader挂掉，会丢失数据，kafka使用ISR的方式很好的均衡了确保数据不丢失以及吞吐率。Follower可以批量的从Leader复制数据，而且Leader充分利用磁盘顺序读以及send file(zero copy)机制，这样极大的提高复制性能，内部批量写磁盘，大幅减少了Follower与Leader的消息量差。
7.什么情况下一个 broker 会从 isr中踢出去

leader会维护一个与其基本保持同步的Replica列表，该列表称为ISR(in-sync Replica)，每个Partition都会有一个ISR，而且是由leader动态维护 ，如果一个follower比一个leader落后太多，或者超过一定时间未发起数据复制请求，则leader将其重ISR中移除 。

8.kafka 为什么那么快

Cache Filesystem Cache PageCache缓存

顺序写 由于现代的操作系统提供了预读和写技术，磁盘的顺序写大多数情况下比随机写内存还要快。

Zero-copy 零拷技术减少拷贝次数

Batching of Messages 批量量处理。合并小的请求，然后以流的方式进行交互，直顶网络上限。

Pull 拉模式 使用拉模式进行消息的获取消费，与消费端处理能力相符。

9.kafka producer如何优化打入速度

增加线程

提高 batch.size

增加更多 producer 实例

增加 partition 数

设置 acks=-1 时，如果延迟增大：可以增大 num.replica.fetchers（follower 同步数据的线程数）来调解；

跨数据中心的传输：增加 socket 缓冲区设置以及 OS tcp 缓冲区设置。

10.kafka producer 打数据，ack  为 0， 1， -1 的时候代表啥， 设置 -1 的时候，什么情况下，leader 会认为一条消息 commit了

1（默认）  数据发送到Kafka后，经过leader成功接收消息的的确认，就算是发送成功了。在这种情况下，如果leader宕机了，则会丢失数据。
0 生产者将数据发送出去就不管了，不去等待任何返回。这种情况下数据传输效率最高，但是数据可靠性确是最低的。
-1 producer需要等待ISR中的所有follower都确认接收到数据后才算一次发送完成，可靠性最高。当ISR中所有Replica都向Leader发送ACK时，leader才commit，这时候producer才能认为一个请求中的消息都commit了。
11.kafka  unclean 配置代表啥，会对 spark streaming 消费有什么影响

unclean.leader.election.enable 为true的话，意味着非ISR集合的broker 也可以参与选举，这样有可能就会丢数据，spark streaming在消费过程中拿到的 end offset 会突然变小，导致 spark streaming job挂掉。如果unclean.leader.election.enable参数设置为true，就有可能发生数据丢失和数据不一致的情况，Kafka的可靠性就会降低；而如果unclean.leader.election.enable参数设置为false，Kafka的可用性就会降低。

12.如果leader crash时，ISR为空怎么办

kafka在Broker端提供了一个配置参数：unclean.leader.election,这个参数有两个值：
true（默认）：允许不同步副本成为leader，由于不同步副本的消息较为滞后，此时成为leader，可能会出现消息不一致的情况。
false：不允许不同步副本成为leader，此时如果发生ISR列表为空，会一直等待旧leader恢复，降低了可用性。

13.kafka的message格式是什么样的

一个Kafka的Message由一个固定长度的header和一个变长的消息体body组成

header部分由一个字节的magic(文件格式)和四个字节的CRC32(用于判断body消息体是否正常)构成。

当magic的值为1的时候，会在magic和crc32之间多一个字节的数据：attributes(保存一些相关属性，

比如是否压缩、压缩格式等等);如果magic的值为0，那么不存在attributes属性

body是由N个字节构成的一个消息体，包含了具体的key/value消息

14.kafka中consumer group 是什么概念

同样是逻辑上的概念，是Kafka实现单播和广播两种消息模型的手段。同一个topic的数据，会广播给不同的group；同一个group中的worker，只有一个worker能拿到这个数据。换句话说，对于同一个topic，每个group都可以拿到同样的所有数据，但是数据进入group后只能被其中的一个worker消费。group内的worker可以使用多线程或多进程来实现，也可以将进程分散在多台机器上，worker的数量通常不超过partition的数量，且二者最好保持整数倍关系，因为Kafka在设计时假定了一个partition只能被一个worker消费（同一group内）。

15.Kafka中的消息是否会丢失和重复消费？

要确定Kafka的消息是否丢失或重复，从两个方面分析入手：消息发送和消息消费。

1、消息发送

         Kafka消息发送有两种方式：同步（sync）和异步（async），默认是同步方式，可通过producer.type属性进行配置。Kafka通过配置request.required.acks属性来确认消息的生产：

0---表示不进行消息接收是否成功的确认；
1---表示当Leader接收成功时确认；
-1---表示Leader和Follower都接收成功时确认；
综上所述，有6种消息生产的情况，下面分情况来分析消息丢失的场景：

（1）acks=0，不和Kafka集群进行消息接收确认，则当网络异常、缓冲区满了等情况时，消息可能丢失；

（2）acks=1、同步模式下，只有Leader确认接收成功后但挂掉了，副本没有同步，数据可能丢失；

2、消息消费

Kafka消息消费有两个consumer接口，Low-level API和High-level API：

Low-level API：消费者自己维护offset等值，可以实现对Kafka的完全控制；

High-level API：封装了对parition和offset的管理，使用简单；

如果使用高级接口High-level API，可能存在一个问题就是当消息消费者从集群中把消息取出来、并提交了新的消息offset值后，还没来得及消费就挂掉了，那么下次再消费时之前没消费成功的消息就“诡异”的消失了；

解决办法：

        针对消息丢失：同步模式下，确认机制设置为-1，即让消息写入Leader和Follower之后再确认消息发送成功；异步模式下，为防止缓冲区满，可以在配置文件设置不限制阻塞超时时间，当缓冲区满时让生产者一直处于阻塞状态；

        针对消息重复：将消息的唯一标识保存到外部介质中，每次消费时判断是否处理过即可。

消息重复消费及解决参考：https://www.javazhiyin.com/22910.html

16.为什么Kafka不支持读写分离？

在 Kafka 中，生产者写入消息、消费者读取消息的操作都是与 leader 副本进行交互的，从 而实现的是一种主写主读的生产消费模型。

Kafka 并不支持主写从读，因为主写从读有 2 个很明 显的缺点:

(1)数据一致性问题。数据从主节点转到从节点必然会有一个延时的时间窗口，这个时间 窗口会导致主从节点之间的数据不一致。某一时刻，在主节点和从节点中 A 数据的值都为 X， 之后将主节点中 A 的值修改为 Y，那么在这个变更通知到从节点之前，应用读取从节点中的 A 数据的值并不为最新的 Y，由此便产生了数据不一致的问题。

(2)延时问题。类似 Redis 这种组件，数据从写入主节点到同步至从节点中的过程需要经 历网络→主节点内存→网络→从节点内存这几个阶段，整个过程会耗费一定的时间。而在 Kafka 中，主从同步会比 Redis 更加耗时，它需要经历网络→主节点内存→主节点磁盘→网络→从节 点内存→从节点磁盘这几个阶段。对延时敏感的应用而言，主写从读的功能并不太适用。

17.Kafka中是怎么体现消息顺序性的？

kafka每个partition中的消息在写入时都是有序的，消费时，每个partition只能被每一个group中的一个消费者消费，保证了消费时也是有序的。
整个topic不保证有序。如果为了保证topic整个有序，那么将partition调整为1.

18.消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?
19.kafka如何实现延迟队列？

Kafka并没有使用JDK自带的Timer或者DelayQueue来实现延迟的功能，而是基于时间轮自定义了一个用于实现延迟功能的定时器（SystemTimer）。JDK的Timer和DelayQueue插入和删除操作的平均时间复杂度为O(nlog(n))，并不能满足Kafka的高性能要求，而基于时间轮可以将插入和删除操作的时间复杂度都降为O(1)。时间轮的应用并非Kafka独有，其应用场景还有很多，在Netty、Akka、Quartz、Zookeeper等组件中都存在时间轮的踪影。

底层使用数组实现，数组中的每个元素可以存放一个TimerTaskList对象。TimerTaskList是一个环形双向链表，在其中的链表项TimerTaskEntry中封装了真正的定时任务TimerTask.

Kafka中到底是怎么推进时间的呢？Kafka中的定时器借助了JDK中的DelayQueue来协助推进时间轮。具体做法是对于每个使用到的TimerTaskList都会加入到DelayQueue中。Kafka中的TimingWheel专门用来执行插入和删除TimerTaskEntry的操作，而DelayQueue专门负责时间推进的任务。再试想一下，DelayQueue中的第一个超时任务列表的expiration为200ms，第二个超时任务为840ms，这里获取DelayQueue的队头只需要O(1)的时间复杂度。如果采用每秒定时推进，那么获取到第一个超时的任务列表时执行的200次推进中有199次属于“空推进”，而获取到第二个超时任务时有需要执行639次“空推进”，这样会无故空耗机器的性能资源，这里采用DelayQueue来辅助以少量空间换时间，从而做到了“精准推进”。Kafka中的定时器真可谓是“知人善用”，用TimingWheel做最擅长的任务添加和删除操作，而用DelayQueue做最擅长的时间推进工作，相辅相成。

参考：https://blog.csdn.net/u013256816/article/details/80697456

20.Kafka中的事务是怎么实现的？

参考：https://blog.csdn.net/u013256816/article/details/89135417

21.Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？

https://blog.csdn.net/yanshu2012/article/details/54894629


Kafka与rabbitmq区别？
一、语言不同

RabbitMQ是由内在高并发的erlanng语言开发，用在实时的对可靠性要求比较高的消息传递上。

kafka是采用Scala语言开发，它主要用于处理活跃的流式数据,大数据量的数据处理上

二、结构不同

RabbitMQ采用AMQP（Advanced Message Queuing Protocol，高级消息队列协议）是一个进程间传递异步消息的网络协议



 RabbitMQ的broker由Exchange,Binding,queue组成

kafka采用mq结构：broker 有part 分区的概念

 

 三、Brokerr与Consume交互方式不同

RabbitMQ 采用push的方式

kafka采用pull的方式

四、在集群负载均衡方面，

rabbitMQ的负载均衡需要单独的loadbalancer进行支持。

kafka采用zookeeper对集群中的broker、consumer进行管理

五、使用场景

rabbitMQ支持对消息的可靠的传递，支持事务，不支持批量的操作；基于存储的可靠性的要求存储可以采用内存或者硬盘。

金融场景中经常使用

kafka具有高的吞吐量，内部采用消息的批量处理，zero-copy机制，数据的存储和获取是本地磁盘顺序批量操作，具有O(1)的复杂度（与分区上的存储大小无关），消息处理的效率很高。（大数据）

六、最大不同

Kafka是严格保证了消息队列的顺序，就是一个topic下面的一个分区内只能给一个消费者消费，对于一个分区来说，kafka是不支持并发，但是可以通过扩大分区实现并发

 

Rabbitmq 不承诺消息的顺序性，因此可以并发多线程处理。在队列中不必排队。如果对处理的顺序没有要求，就可以用Rabbitmq教容易的实现并发。

Zookeeper
Nginx

性能调优
tomcat调优
Nginx调优
数据库调优
